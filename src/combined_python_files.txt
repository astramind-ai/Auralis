

================================================================================
# File: combine_files.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/combine_files.py
================================================================================

import os
from pathlib import Path


def combine_python_files(root_dir: str, output_file: str):
    """
    Ricorsivamente trova tutti i file .py e li appende a un unico file
    con riferimento al path originale.

    Args:
        root_dir: Directory da cui partire per la ricerca
        output_file: File di output dove verranno combinati i risultati
    """
    with open(output_file, 'w', encoding='utf-8') as outfile:
        for root, _, files in os.walk(root_dir):
            for file in files:
                if file.endswith('.py'):
                    file_path = Path(root) / file
                    relative_path = file_path.relative_to(root_dir)

                    outfile.write(f"\n\n{'=' * 80}\n")
                    outfile.write(f"# File: {relative_path}\n")
                    outfile.write(f"# Path: {file_path}\n")
                    outfile.write('=' * 80 + "\n\n")

                    try:
                        with open(file_path, 'r', encoding='utf-8') as infile:
                            outfile.write(infile.read())
                    except Exception as e:
                        outfile.write(f"# Error reading file: {e}\n")


if __name__ == "__main__":
    # Esempio di utilizzo
    root_directory = "/home/marco/PycharmProjects/betterVoiceCraft/Auralis/src"  # Directory corrente
    output_file = "combined_python_files.txt"
    combine_python_files(root_directory, output_file)

================================================================================
# File: auralis/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/__init__.py
================================================================================

from .core.tts import TTS
from .common.definitions.requests import TTSRequest
from .common.definitions.output import TTSOutput
from .common.logging.logger import setup_logger, set_vllm_logging_level
from .common.definitions.enhancer import AudioPreprocessingConfig



================================================================================
# File: auralis/common/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/__init__.py
================================================================================



================================================================================
# File: auralis/common/utilities.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/utilities.py
================================================================================

from typing import Union, Callable, Dict, Any

import fsspec
import torch
import torchaudio
import io


def wav_to_mel_cloning(
        wav,
        mel_norms_file="../experiments/clips_mel_norms.pth",
        mel_norms=None,
        device=torch.device("cpu"),
        n_fft=4096,
        hop_length=1024,
        win_length=4096,
        power=2,
        normalized=False,
        sample_rate=22050,
        f_min=0,
        f_max=8000,
        n_mels=80,
):
    """Convert waveform to normalized mel-spectrogram for voice cloning.

    This function converts a raw audio waveform to a mel-spectrogram using the
    specified parameters, then normalizes it using pre-computed mel norms for
    consistent voice cloning results.

    Args:
        wav (torch.Tensor): Input waveform tensor.
        mel_norms_file (str, optional): Path to mel norms file. Defaults to
            "../experiments/clips_mel_norms.pth".
        mel_norms (torch.Tensor, optional): Pre-loaded mel norms. Defaults to None.
        device (torch.device, optional): Device to perform computation on.
            Defaults to CPU.
        n_fft (int, optional): FFT size. Defaults to 4096.
        hop_length (int, optional): Number of samples between STFT columns.
            Defaults to 1024.
        win_length (int, optional): Window size. Defaults to 4096.
        power (int, optional): Exponent for the magnitude spectrogram.
            Defaults to 2.
        normalized (bool, optional): Whether to normalize by magnitude after STFT.
            Defaults to False.
        sample_rate (int, optional): Audio sample rate. Defaults to 22050.
        f_min (int, optional): Minimum frequency. Defaults to 0.
        f_max (int, optional): Maximum frequency. Defaults to 8000.
        n_mels (int, optional): Number of mel filterbanks. Defaults to 80.

    Returns:
        torch.Tensor: Normalized mel-spectrogram.
    """
    mel_stft = torchaudio.transforms.MelSpectrogram(
        n_fft=n_fft,
        hop_length=hop_length,
        win_length=win_length,
        power=power,
        normalized=normalized,
        sample_rate=sample_rate,
        f_min=f_min,
        f_max=f_max,
        n_mels=n_mels,
        norm="slaney",
    ).to(device)
    wav = wav.to(device)
    mel = mel_stft(wav)
    mel = torch.log(torch.clamp(mel, min=1e-5))
    if mel_norms is None:
        mel_norms = torch.load(mel_norms_file, map_location=device)
    mel = mel / mel_norms.unsqueeze(0).unsqueeze(-1)
    return mel


def load_audio(audiopath, sampling_rate):
    """Load and preprocess audio file.

    This function loads an audio file, converts it to mono if needed,
    resamples to the target sampling rate, and ensures valid amplitude range.

    Args:
        audiopath (Union[str, Path]): Path to audio file.
        sampling_rate (int): Target sampling rate.

    Returns:
        torch.Tensor: Preprocessed audio tensor of shape [1, samples].
    """
    audio, lsr = torchaudio.load(audiopath)

    # Stereo to mono if needed
    if audio.size(0) != 1:
        audio = torch.mean(audio, dim=0, keepdim=True)

    if lsr != sampling_rate:
        audio = torchaudio.functional.resample(audio, lsr, sampling_rate)

    # Clip audio invalid values
    audio.clip_(-1, 1)
    return audio

def load_fsspec(
    path: str,
    map_location: Union[str, Callable, torch.device, Dict[Union[str, torch.device], Union[str, torch.device]]] = None,
    **kwargs,
) -> Any:
    """Load PyTorch checkpoint from any fsspec-supported location.

    This function extends torch.load to support loading from various file systems
    and cloud storage providers (e.g., S3, GCS) using fsspec.

    Args:
        path (str): Any path or URL supported by fsspec (e.g., 's3://', 'gs://').
        map_location (Union[str, Callable, torch.device, Dict], optional): Device
            mapping specification for torch.load. Defaults to None.
        **kwargs: Additional arguments passed to torch.load.

    Returns:
        Any: Object stored in the checkpoint.

    Example:
        >>> state_dict = load_fsspec('s3://my-bucket/model.pth', map_location='cuda')
    """
    with fsspec.open(path, "rb") as f:
            return torch.load(f, map_location=map_location, **kwargs)


================================================================================
# File: auralis/common/scheduling/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/scheduling/__init__.py
================================================================================



================================================================================
# File: auralis/common/scheduling/two_phase_scheduler.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/scheduling/two_phase_scheduler.py
================================================================================

import uuid
from typing import Any, Dict, AsyncGenerator, Callable, Awaitable, Optional
import asyncio
import time
from contextlib import asynccontextmanager
from auralis.common.definitions.scheduler import QueuedRequest, TaskState
from auralis.common.logging.logger import setup_logger


class TwoPhaseScheduler:
    """Asynchronous task scheduler with parallel processing support.

    This scheduler manages the lifecycle of a request, including both phases
    of processing and error handling. It includes request timeout handling and
    orderly output generation.

    Features:
        - Controlled concurrency for parallel processing
        - Request timeout management
        - Ordered output collection from parallel generators
        - Error handling and cleanup
        - Resource management with automatic cleanup

    Attributes:
        concurrency (int): Maximum number of parallel tasks.
        request_timeout (float): Maximum time allowed for a complete request.
        generator_timeout (float): Maximum time allowed between generator yields.
    """

    def __init__(
            self,
            concurrency: int = 10,
            request_timeout: float = None,
            generator_timeout: float = None
    ):
        """Initialize the scheduler.

        Args:
            concurrency (int, optional): Maximum parallel tasks.
                Defaults to 10.
            request_timeout (float, optional): Request timeout in seconds.
                Defaults to None (no timeout).
            generator_timeout (float, optional): Generator timeout in seconds.
                Defaults to None (no timeout).
        """
        # Core configuration
        self.concurrency = concurrency
        self.request_timeout = request_timeout
        self.generator_timeout = generator_timeout
        self.logger = setup_logger(__file__)

        # State management
        self.is_running = False
        self.request_queue = None
        self.active_requests = {}
        self.queue_processor_tasks = []
        self.cancel_warning_issued = False

        # Concurrency controls
        self.semaphore = None
        self.active_generator_count = 0
        self.generator_count_lock = asyncio.Lock()
        self.cleanup_lock = asyncio.Lock()

    async def start(self):
        """Start the scheduler.

        Initializes queues, semaphores, and worker tasks. This method is
        idempotent and safe to call multiple times.
        """
        if self.is_running:
            return

        self.request_queue = asyncio.Queue()
        self.semaphore = asyncio.Semaphore(self.concurrency)
        self.is_running = True
        self.queue_processor_tasks = [
            asyncio.create_task(self._process_queue())
            for _ in range(self.concurrency)
        ]

    async def _process_queue(self):
        """Process requests from the queue continuously.

        This is a worker task that runs continuously while the scheduler is active,
        processing requests as they arrive in the queue.
        """
        while self.is_running:
            try:
                request = await self.request_queue.get()
                if request.state == TaskState.QUEUED:
                    async with self._request_lifecycle(request.id):
                        self.active_requests[request.id] = request
                        await self._process_request(request)
            except asyncio.CancelledError:
                if not self.cancel_warning_issued:
                    self.logger.warning("Queue processing task cancelled")
                    self.cancel_warning_issued = True
                break
            except Exception as e:
                self.logger.error(f"Queue processing error: {e}")
                await asyncio.sleep(1)

    @asynccontextmanager
    async def _request_lifecycle(self, request_id: str):
        """Manage the lifecycle of a request.

        Context manager that ensures proper cleanup of request resources,
        even if processing fails.

        Args:
            request_id (str): ID of the request to manage.
        """
        try:
            yield
        finally:
            async with self.cleanup_lock:
                self.active_requests.pop(request_id, None)

    async def _process_request(self, request: QueuedRequest):
        """Process a request through all phases.

        Handles the complete lifecycle of a request, including all
        phases of processing and error handling.

        Args:
            request (QueuedRequest): Request to process.
        """
        try:
            self.logger.info(f"Starting request {request.id}")

            # All phase processing is now moved out of this class, the scheduler is just a container for the lifecycle
            request.state = TaskState.PROCESSING_SECOND

            if not request.error:
                request.state = TaskState.COMPLETED
                self.logger.info(f"Request {request.id} completed")

        except Exception as e:
            request.error = e
            request.state = TaskState.FAILED
            self.logger.error(f"Request {request.id} failed: {e}")
        finally:
            request.completion_event.set()

    async def run(
            self,
            inputs: Any,
            processing_fn: Callable[[Any], Awaitable[Any]],
            request_id: str = None,
    ) -> AsyncGenerator[Any, None]:
        """Run a two-phase processing task.

        This is the main entry point for task execution. It creates a new request,
        queues it for processing, and yields results in sequence order.

        Args:
            inputs (Any): Input data for processing.
            processing_fn (Callable[[Any], Awaitable[Any]]): Main processing function
            request_id (str, optional): Custom request ID. Defaults to None.

        Yields:
            Any: Processing results in sequence order.

        Example:
            >>> async for result in scheduler.run(
            ...     inputs=text,
            ...     processing_fn=process_text
            ... ):
            ...     process_result(result)
        """
        if not self.is_running:
            await self.start()

        request = QueuedRequest(
            id=request_id,
            input=inputs,
            processing_fn=processing_fn
        )

        await self.request_queue.put(request)

        try:
            async for item in self._yield_ordered_outputs(request):
                yield item

            await asyncio.wait_for(
                request.completion_event.wait(),
                timeout=self.request_timeout
            )
            if request.error:
                raise request.error

        finally:
            async with self.cleanup_lock:
                self.active_requests.pop(request.id, None)

    async def _yield_ordered_outputs(self, request: QueuedRequest) -> AsyncGenerator[Any, None]:
        """Yield outputs from all generators in sequence order.

        Collects outputs from multiple generators and yields them in the correct
        sequence order, handling timeouts and errors.

        Args:
            request (QueuedRequest): Request to yield outputs from.

        Yields:
            Any: Output items in sequence order.

        Raises:
            TimeoutError: If no progress is made within request_timeout.
            Exception: If any generator fails.
        """
        current_index = 0
        last_progress = time.time()
        while not self._is_processing_complete(request):
            if self._check_timeout(last_progress):
                raise TimeoutError("No progress in output generation")

            if request.error:
                raise request.error

            if current_index in request.sequence_buffers:
                buffer = request.sequence_buffers[current_index]
                if buffer:
                    item, event = buffer[0]
                    try:
                        await asyncio.wait_for(event.wait(), timeout=self.generator_timeout)
                        yield item
                        buffer.pop(0)
                        last_progress = time.time()
                    except asyncio.TimeoutError:
                        raise TimeoutError(f"Timeout waiting for item in sequence {current_index}")
                    current_index += 1

            await asyncio.sleep(0.01)

    def _is_processing_complete(self, request: QueuedRequest) -> bool:
        """Check if request processing is complete.

        Args:
            request (QueuedRequest): Request to check.

        Returns:
            bool: True if all processing is complete, False otherwise.
        """
        return (request.state in (TaskState.COMPLETED, TaskState.FAILED) and
                request.completed_generators >= request.generators_count and
                all(len(buffer) == 0 for buffer in request.sequence_buffers.values()))

    def _check_timeout(self, last_progress: float) -> bool:
        """Check if request has timed out.

        Args:
            last_progress (float): Timestamp of last progress.

        Returns:
            bool: True if request has timed out, False otherwise.
        """
        return self.request_timeout and time.time() - last_progress > self.request_timeout


    async def shutdown(self):
        self.is_running = False

        for task in self.queue_processor_tasks:
            if task and not task.done():
                task.cancel()

        await asyncio.gather(*self.queue_processor_tasks, return_exceptions=True)

        if self.active_requests:
            await asyncio.gather(
                *(request.completion_event.wait() for request in self.active_requests.values()),
                return_exceptions=True
            )

================================================================================
# File: auralis/common/definitions/scheduler.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/definitions/scheduler.py
================================================================================

import time
from dataclasses import dataclass, field
from enum import Enum
from collections import defaultdict
from typing import Any, Dict, Optional, List, Callable, TypeVar

import asyncio

T = TypeVar('T')
R = TypeVar('R')


class TaskState(Enum):
    QUEUED = "queued"
    PROCESSING_FIRST = "processing_first"
    PROCESSING_SECOND = "processing_second"
    COMPLETED = "completed"
    FAILED = "failed"


@dataclass
class QueuedRequest:
    id: str
    input: Any
    state: TaskState = TaskState.QUEUED
    error: Optional[Exception] = None
    first_phase_result: Any = None
    generators_count: int = 0
    completed_generators: int = 0
    processing_fn: Callable = None
    sequence_buffers: Dict[int, List[Any]] = field(default_factory=lambda: defaultdict(list))
    start_time: float = field(default_factory=time.time)
    completion_event: asyncio.Event = field(default_factory=asyncio.Event)


================================================================================
# File: auralis/common/definitions/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/definitions/__init__.py
================================================================================



================================================================================
# File: auralis/common/definitions/enhancer.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/definitions/enhancer.py
================================================================================

import uuid
from dataclasses import dataclass, field
from typing import Union, AsyncGenerator, Optional, List, Literal, get_args
from functools import lru_cache
import numpy as np
import librosa
import torch
import torchaudio
import pyloudnorm

@dataclass
class AudioPreprocessingConfig:
    sample_rate: int = 22050
    normalize: bool = True
    trim_silence: bool = True
    remove_noise: bool = True
    enhance_speech: bool = True

    # VAD parameters
    vad_threshold: float = 0.02
    vad_frame_length: int = 1024*4

    # Noise reduction
    noise_reduce_margin: float = 1.0
    noise_reduce_frames: int = 25

    # Enhancement
    enhance_amount: float = 1.0

    # Normalization target
    target_lufs: float = -18.0


class EnhancedAudioProcessor:
    def __init__(self, config: AudioPreprocessingConfig):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    @staticmethod
    @torch.no_grad()
    def get_mel_spectrogram(audio: np.ndarray, sr: int) -> torch.Tensor:
        """Compute mel spectrogram efficiently using torch."""
        audio_tensor = torch.FloatTensor(audio).unsqueeze(0)
        mel_spec = torchaudio.transforms.MelSpectrogram(
            sample_rate=sr,
            n_fft=2048,
            hop_length=512,
            n_mels=80
        )(audio_tensor)
        return torch.log(torch.clamp(mel_spec, min=1e-5))

    def vad_split(self, audio: np.ndarray) -> np.ndarray:
        """Enhanced Voice Activity Detection using energy and spectral features."""
        # Compute short-time energy
        frame_length = self.config.vad_frame_length
        frames = librosa.util.frame(audio, frame_length=frame_length, hop_length=frame_length // 2)
        energy = np.sum(frames ** 2, axis=0)
        energy = energy / np.max(energy)

        # Compute spectral features
        mel_spec = self.get_mel_spectrogram(audio, self.config.sample_rate)
        spectral_sum = torch.sum(mel_spec, dim=1).numpy().squeeze()
        spectral_sum = spectral_sum / np.max(spectral_sum)

        # Resize signals to match
        if len(energy) > len(spectral_sum):
            # Interpolate spectral_sum to match energy length
            spectral_sum = np.interp(
                np.linspace(0, 1, len(energy)),
                np.linspace(0, 1, len(spectral_sum)),
                spectral_sum
            )
        else:
            # Interpolate energy to match spectral_sum length
            energy = np.interp(
                np.linspace(0, 1, len(spectral_sum)),
                np.linspace(0, 1, len(energy)),
                energy
            )

        # Combine features
        vad_signal = (energy + spectral_sum) / 2
        vad_mask = np.absolute(vad_signal) > self.config.vad_threshold

        # Apply mask (resizing to audio length)
        mask_upsampled = np.interp(
            np.linspace(0, 1, len(audio)),
            np.linspace(0, 1, len(vad_mask)),
            vad_mask.astype(float)
        )

        return audio * mask_upsampled

    def spectral_gating(self, audio: np.ndarray) -> np.ndarray:
        """Enhanced spectral noise reduction."""
        # Compute STFT
        D = librosa.stft(audio)
        mag, phase = librosa.magphase(D)

        # Estimate noise profile from lowest energy frames
        noise_profile = np.mean(np.sort(mag, axis=1)[:, :self.config.noise_reduce_frames], axis=1)
        noise_profile = noise_profile[:, None]

        # Create mask
        mask = (mag - noise_profile * self.config.noise_reduce_margin).clip(min=0)
        mask = mask / (mask + noise_profile)

        # Apply mask
        return librosa.istft(mask * D)

    def enhance_clarity(self, audio: np.ndarray) -> np.ndarray:
        """Enhance speech clarity using spectral shaping."""
        # Convert to frequency domain
        D = librosa.stft(np.nan_to_num(audio, nan=0.0, posinf=0.0, neginf=0.0))
        mag, phase = librosa.magphase(D)

        # Apply mild spectral shaping to enhance clarity
        freq_bins = np.fft.fftfreq(D.shape[0], 1 / self.config.sample_rate)
        clarity_boost = np.exp(-np.abs(freq_bins - 2000) / 1000) * self.config.enhance_amount
        clarity_boost = clarity_boost[:, None]

        mag_enhanced = mag * (1 + clarity_boost)

        return librosa.istft(mag_enhanced * phase)

    def normalize_loudness(self, audio: np.ndarray) -> np.ndarray:
        """Improved loudness normalization targeting LUFS."""
        # Compute current loudness
        meter = pyloudnorm.Meter(self.config.sample_rate)
        current_loudness = meter.integrated_loudness(audio)

        # Compute gain needed
        gain_db = self.config.target_lufs - current_loudness
        gain_linear = 10 ** (gain_db / 20)

        # Apply gain with soft clipping
        audio_normalized = audio * gain_linear
        return np.tanh(audio_normalized)  # Soft clipping

    def process(self, audio: np.ndarray) -> np.ndarray:
        """Apply all processing steps efficiently."""
        if self.config.trim_silence:
            audio = self.vad_split(audio)

        if self.config.remove_noise:
            audio = self.spectral_gating(audio)

        if self.config.enhance_speech:
            audio = self.enhance_clarity(audio)

        if self.config.normalize:
            audio = self.normalize_loudness(audio)

        return audio

================================================================================
# File: auralis/common/definitions/openai.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/definitions/openai.py
================================================================================

import base64
from dataclasses import fields

from openai import OpenAI
from pydantic import BaseModel, Field, field_validator, model_validator
from typing import List, Optional, Dict, Any, Literal, Union
from auralis.common.definitions.requests import TTSRequest


class ChatCompletionMessage(BaseModel):
    role: Literal["system", "user", "assistant"]
    content: str

tts_defaults = {field.name: field.default for field in fields(TTSRequest)}

class VoiceChatCompletionRequest(BaseModel):
    # Chat completion fields
    model: str
    messages: List[ChatCompletionMessage]
    speaker_files: List[str] = Field(..., description="List of base64-encoded audio files")
    modalities: List[Literal["text", "audio"]] = Field(
        default=["text", "audio"],
        description="Output modalities to return"
    )
    openai_api_url: Optional[str] = Field(
        default=None,
        description="Custom OpenAI API endpoint to make the LLM reqeust to"
    )
    vocalize_at_every_n_words: int = Field(
        default=100,
        ge=1,
        description="Number of words after which to generate audio"
    )
    stream: bool = Field(default=True)

    # TTSRequest parameters usando i defaults dalla dataclass
    enhance_speech: bool = Field(default=tts_defaults['enhance_speech'])
    language: str = Field(default=tts_defaults['language'])
    max_ref_length: int = Field(default=tts_defaults['max_ref_length'])
    gpt_cond_len: int = Field(default=tts_defaults['gpt_cond_len'])
    gpt_cond_chunk_len: int = Field(default=tts_defaults['gpt_cond_chunk_len'])
    temperature: float = Field(default=tts_defaults['temperature'])
    top_p: float = Field(default=tts_defaults['top_p'])
    top_k: int = Field(default=tts_defaults['top_k'])
    repetition_penalty: float = Field(default=tts_defaults['repetition_penalty'])
    length_penalty: float = Field(default=tts_defaults['length_penalty'])
    do_sample: bool = Field(default=tts_defaults['do_sample'])

    @field_validator('openai_api_url')
    def validate_oai_url(cls, v):
        if v is None:
            raise ValueError("You should always give a url for the text generation")
        return v

    @field_validator('stream')
    def validate_stream(cls, v):
        if not v:
            raise ValueError('Streaming should be enabled! For non-streaming conversion use the audio endpoint')
        return v

    @field_validator('speaker_files')
    def validate_speaker_files(cls, v):
        if not v:
            raise ValueError("At least one speaker file is required")
        for file in v:
            try:
                base64.b64decode(file)
            except Exception:
                raise ValueError(f"Invalid base64 encoding in speaker file")
        return v

    @field_validator('modalities')
    def validate_modalities(cls, v):
        valid_modalities = ["text", "audio"]
        if not all(m in valid_modalities for m in v):
            raise ValueError(f"Invalid modalities. Must be one or more of {valid_modalities}")
        return v

    def to_tts_request(self, text: str = "") -> TTSRequest:
        """Convert to TTSRequest with decoded speaker files"""
        speaker_data_list = [base64.b64decode(f) for f in self.speaker_files]

        return TTSRequest(
            text=text,
            stream=False,
            speaker_files=speaker_data_list,
            enhance_speech=self.enhance_speech,
            language=self.language,
            max_ref_length=self.max_ref_length,
            gpt_cond_len=self.gpt_cond_len,
            gpt_cond_chunk_len=self.gpt_cond_chunk_len,
            temperature=self.temperature,
            top_p=self.top_p,
            top_k=self.top_k,
            repetition_penalty=self.repetition_penalty,
            length_penalty=self.length_penalty,
            do_sample=self.do_sample
        )

    def to_openai_request(self) -> Dict[str, Any]:
        """Convert to OpenAI API compatible request format"""
        oai_dict = {
            k: v for k, v in self.model_dump().items()
            if k not in ["speaker_files", "openai_api_url", "vocalize_at_every_n_words", 'modalities'] and
               not k in tts_defaults.keys()
        }
        oai_dict.update({"stream": True})
        return oai_dict


class AudioSpeechGenerationRequest(BaseModel):
        # Chat completion fields
        input: str = Field(..., description="The textual input to convert")
        model: str = Field(..., description="The model to use for conversion")
        voice: List[str] = Field(..., description="List of base64-encoded audio files")
        response_format: Literal["mp3", "opus", "aac", "flac", "wav", "pcm"] = Field(
            default='mp3', description="List of base64-encoded audio files"
        )
        speed: float = Field(default=1.0, description="List of base64-encoded audio files"),

        # TTSRequest parameters
        enhance_speech: bool = Field(default=tts_defaults['enhance_speech'])
        language: str = Field(default=tts_defaults['language'])
        max_ref_length: int = Field(default=tts_defaults['max_ref_length'])
        gpt_cond_len: int = Field(default=tts_defaults['gpt_cond_len'])
        gpt_cond_chunk_len: int = Field(default=tts_defaults['gpt_cond_chunk_len'])
        temperature: float = Field(default=tts_defaults['temperature'])
        top_p: float = Field(default=tts_defaults['top_p'])
        top_k: int = Field(default=tts_defaults['top_k'])
        repetition_penalty: float = Field(default=tts_defaults['repetition_penalty'])
        length_penalty: float = Field(default=tts_defaults['length_penalty'])
        do_sample: bool = Field(default=tts_defaults['do_sample'])

        @field_validator('voice')
        def validate_speaker_files(cls, v):
            if not v:
                raise ValueError("At least one voice file is required")
            for file in v:
                try:
                    base64.b64decode(file)
                except Exception:
                    raise ValueError(f"Invalid base64 encoding in voice file")
            return v

        def to_tts_request(self) -> TTSRequest:
            """Convert to TTSRequest with decoded speaker files"""
            speaker_data_list = [base64.b64decode(f) for f in self.voice]

            return TTSRequest(
                text=self.input,
                stream=False,
                speaker_files=speaker_data_list,
                enhance_speech=self.enhance_speech,
                language=self.language,
                max_ref_length=self.max_ref_length,
                gpt_cond_len=self.gpt_cond_len,
                gpt_cond_chunk_len=self.gpt_cond_chunk_len,
                temperature=self.temperature,
                top_p=self.top_p,
                top_k=self.top_k,
                repetition_penalty=self.repetition_penalty,
                length_penalty=self.length_penalty,
                do_sample=self.do_sample
            )


================================================================================
# File: auralis/common/definitions/output.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/definitions/output.py
================================================================================

import io
from dataclasses import dataclass
from pathlib import Path
from typing import Union, Optional, Tuple, List
import sounddevice as sd

from IPython.display import Audio, display


import numpy as np
import torch
import torchaudio
from torio.io import CodecConfig


@dataclass
class TTSOutput:
    """Container for TTS inference output with integrated audio utilities"""
    array: Union[np.ndarray, bytes]
    sample_rate: int = 24000
    bit_depth: int = 32
    bit_rate: int = 192 # kbps
    compression: int = 10 #
    channel: int = 1

    start_time: Optional[float] = None
    end_time: Optional[float] = None
    token_length: Optional[int] = None


    def __post_init__(self):
        if isinstance(self.array, bytes):
            self.array = np.frombuffer(self.array, dtype=np.int16)
            #normalize in the range
            self.array = self.array.astype(np.float32) / 32768.0
            fade_length = 100
            fade_in = np.linspace(0, 1, fade_length)
            self.array[:fade_length] *= fade_in

    def change_speed(self, speed_factor: float) -> 'TTSOutput':
        """
        Change audio speed while preserving quality and minimizing distortion.
        Uses phase vocoder for better quality at extreme speed changes.

        Args:
            speed_factor (float): Speed modification factor:
                                 > 1.0: speeds up (e.g. 1.2 = 20% faster)
                                 < 1.0: slows down (e.g. 0.8 = 20% slower)
                                 = 1.0: no change

        Returns:
            TTSOutput: New instance with speed-modified audio

        Example:
            # Speed up 20%
            faster = audio.change_speed(1.2)

            # Slow down 20%
            slower = audio.change_speed(0.8)

        Raises:
            ValueError: If speed_factor is <= 0
        """
        import librosa

        # Validate input
        if speed_factor <= 0:
            raise ValueError("Speed factor must be positive")

        if speed_factor == 1.0:
            return self

        # Ensure float32
        wav = self.array.astype(np.float32) if self.array.dtype != np.float32 else self.array

        # Parameters for STFT
        n_fft = 2048
        hop_length = 512

        # Compute STFT
        D = librosa.stft(wav, n_fft=n_fft, hop_length=hop_length)

        # Time-stretch using phase vocoder
        modified_stft = librosa.phase_vocoder(
            D,
            rate=speed_factor,
            hop_length=hop_length
        )

        # Inverse STFT
        modified = librosa.istft(
            modified_stft,
            hop_length=hop_length,
            length=len(wav)
        )

        # Normalize to prevent clipping
        modified = librosa.util.normalize(modified, norm=np.inf)

        return TTSOutput(
            array=modified,
            sample_rate=self.sample_rate
        )

    @staticmethod
    def combine_outputs(outputs: List['TTSOutput']) -> 'TTSOutput':
        """Combine multiple TTSOutput instances into a single instance.

        Args:
            outputs: List of TTSOutput instances

        Returns:
            New TTSOutput instance with concatenated audio
        """
        # Concatenate audio
        combined_audio = np.concatenate([out.array for out in outputs])

        # Use sample rate of first output
        return TTSOutput(
            array=combined_audio,
            sample_rate=outputs[0].sample_rate
        )

    def to_tensor(self) -> Union[torch.Tensor, np.ndarray]:
        """Convert numpy array to torch tensor"""
        if isinstance(self.array, np.ndarray):
            return torch.from_numpy(self.array)
        return self.array

    def to_bytes(self, format: str = 'wav', sample_width: int = 2) -> bytes:
        """Convert audio to bytes format.

        Args:
            format: Output format ('mp3', 'opus', 'aac', 'flac', 'wav', 'pcm')
            sample_width: Bit depth (1, 2, or 4 bytes per sample)

        Returns:
            Audio data as bytes
        """
        # Convert to tensor if needed
        wav_tensor = self.to_tensor().to(torch.float32)

        # Ensure correct shape (1, N) for torchaudio
        if wav_tensor.dim() == 1:
            wav_tensor = wav_tensor.unsqueeze(0)

        # Normalize to [-1, 1]
        wav_tensor = torch.clamp(wav_tensor, -1.0, 1.0)

        buffer = io.BytesIO()

        if format in ['wav', 'flac']:
            torchaudio.save(
                buffer,
                wav_tensor,
                self.sample_rate,
                format=format,
                encoding="PCM_S" if sample_width == 2 else "PCM_F",
                bits_per_sample=sample_width * 8,
                compression = CodecConfig(compression_level=min(8, self.compression)) if format == 'flac' else None
            )
        elif format == 'mp3':
            torchaudio.save(
                buffer,
                wav_tensor,
                self.sample_rate,
                format="mp3",
                compression = CodecConfig(bit_rate=self.bit_rate)
            )
        elif format == 'opus':
            torchaudio.save(
                buffer,
                wav_tensor,
                self.sample_rate,
                format="opus",
                compression = CodecConfig(compression_level=self.compression)
            )
        elif format == 'aac':
            torchaudio.save(
                buffer,
                wav_tensor,
                self.sample_rate,
                format="adts",
                compression = CodecConfig(bit_rate=self.bit_rate)
            )
        elif format == 'pcm':
            # Scale to appropriate range based on sample width
            if sample_width == 2:  # 16-bit
                wav_tensor = (wav_tensor * 32767).to(torch.int16)
            elif sample_width == 4:  # 32-bit
                wav_tensor = (wav_tensor * 2147483647).to(torch.int32)
            else:  # 8-bit
                wav_tensor = (wav_tensor * 127).to(torch.int8)
            return wav_tensor.cpu().numpy().tobytes()
        else:
            raise ValueError(f"Unsupported format: {format}. Supported formats are: mp3, opus, aac, flac, wav, pcm")

        return buffer.getvalue()

    def save(self,
             filename: Union[str, Path],
             sample_rate: Optional[int] = None,
             format: Optional[str] = None) -> None:
        """Save audio to file.

        Args:
            filename: Output filename
            sample_rate: Optional new sample rate for resampling
            format: Optional format override (default: inferred from extension)
        """
        wav_tensor = self.to_tensor()
        if wav_tensor.dim() == 1:
            wav_tensor = wav_tensor.unsqueeze(0)

        # Resample if needed
        if sample_rate and sample_rate != self.sample_rate:
            wav_tensor = torchaudio.functional.resample(
                wav_tensor,
                orig_freq=self.sample_rate,
                new_freq=sample_rate
            )
        else:
            sample_rate = self.sample_rate
        if wav_tensor.dtype != torch.float32:
            wav_tensor = wav_tensor.to(torch.float32)
        torchaudio.save(
            filename,
            wav_tensor,
            sample_rate,
            format=format,
            bits_per_sample=self.bit_depth,
            channels_first=self.channel
        )

    def resample(self, new_sample_rate: int) -> 'TTSOutput':
        """Create new TTSOutput with resampled audio.

        Args:
            new_sample_rate: Target sample rate

        Returns:
            New TTSOutput instance with resampled audio
        """
        wav_tensor = self.to_tensor()
        if wav_tensor.dim() == 1:
            wav_tensor = wav_tensor.unsqueeze(0)

        resampled = torchaudio.functional.resample(
            wav_tensor,
            orig_freq=self.sample_rate,
            new_freq=new_sample_rate
        )

        return TTSOutput(
            array=resampled.squeeze().numpy(),
            sample_rate=new_sample_rate
        )

    def get_info(self) -> Tuple[int, int, float]:
        """Get audio information.

        Returns:
            Tuple of (number of samples, sample rate, duration in seconds)
        """
        n_samples = len(self.array)
        duration = n_samples / self.sample_rate
        return n_samples, self.sample_rate, duration

    @classmethod
    def from_tensor(cls, tensor: torch.Tensor, sample_rate: int = 24000) -> 'TTSOutput':
        """Create TTSOutput from torch tensor.

        Args:
            tensor: Audio tensor
            sample_rate: Sample rate of the audio

        Returns:
            New TTSOutput instance
        """
        return cls(
            array=tensor.squeeze().cpu().numpy(),
            sample_rate=sample_rate
        )

    @classmethod
    def from_file(cls, filename: Union[str, Path]) -> 'TTSOutput':
        """Create TTSOutput from audio file.

        Args:
            filename: Path to audio file

        Returns:
            New TTSOutput instance
        """
        wav_tensor, sample_rate = torchaudio.load(filename)
        return cls.from_tensor(wav_tensor, sample_rate)

    def play(self) -> None:
        """Play the audio through the default sound device.
        For use in regular Python scripts/applications."""
        # Ensure the audio is in the correct format
        if isinstance(self.array, torch.Tensor):
            audio_data = self.array.cpu().numpy()
        else:
            audio_data = self.array

        # Ensure float32 and normalize
        if audio_data.dtype != np.float32:
            audio_data = audio_data.astype(np.float32)
        audio_data = np.clip(audio_data, -1.0, 1.0)

        # Play the audio
        sd.play(audio_data, self.sample_rate, blocksize=2048)
        sd.wait()  # Wait until the audio is finished playing

    def display(self) -> Optional[Audio]:
        """Display audio player in Jupyter notebook.
        Returns Audio widget if in notebook, None otherwise."""
        try:
            # Convert to bytes
            audio_bytes = self.to_bytes(format='wav')

            # Create and display audio widget
            audio_widget = Audio(audio_bytes, rate=self.sample_rate, autoplay=False)
            display(audio_widget)
            return audio_widget
        except Exception as e:
            print(f"Could not display audio widget: {str(e)}")
            print("Try using .play() method instead")
            return None

    def preview(self) -> None:
        """Smart play method that chooses appropriate playback method."""
        try:
            # Try notebook display first
            if self.display() is None:
                # Fall back to sounddevice if not in notebook
                self.play()
        except Exception as e:
            print(f"Error playing audio: {str(e)}")

================================================================================
# File: auralis/common/definitions/requests.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/definitions/requests.py
================================================================================

import io
import uuid

from dataclasses import dataclass
from pathlib import Path
from typing import Union, AsyncGenerator, Optional, List, Literal, get_args, Callable

import langid
import librosa
import soundfile as sf

import functools
import hashlib
import json
from functools import lru_cache
from dataclasses import asdict, field

from cachetools import LRUCache


def hash_params(*args, **kwargs):
    """Create a hash from function parameters for caching.

    Args:
        *args: Variable positional arguments to hash.
        **kwargs: Variable keyword arguments to hash.

    Returns:
        str: MD5 hash of the stringified parameters.
    """
    params_str = json.dumps([str(arg) for arg in args], sort_keys=True)
    return hashlib.md5(params_str.encode()).hexdigest()


def cached_processing(maxsize=128):
    """Decorator for caching audio processing results.

    Implements an LRU cache for audio processing functions to avoid
    reprocessing the same audio files with the same configuration.

    Args:
        maxsize (int, optional): Maximum size of the LRU cache. Defaults to 128.

    Returns:
        Callable: Decorated function with caching behavior.
    """
    def decorator(func):
        # Create cache storage
        cache = LRUCache(maxsize=maxsize)
        @functools.wraps(func)
        def wrapper(self, audio_path: str, audio_config: AudioPreprocessingConfig, *args, **kwargs):
            # Create hash from the two parameters we care about
            params_dict = {
                'audio_path': audio_path,
                'config': asdict(audio_config)
            }
            cache_key = hash_params(params_dict)

            # Check cache
            if result := cache.get(cache_key):
                return result

            # If not in cache, process and store
            result = func(self, audio_path, audio_config, *args, **kwargs)
            cache.__setitem__(cache_key, result)
            return result

        return wrapper

    return decorator

from auralis.common.definitions.enhancer import EnhancedAudioProcessor, AudioPreprocessingConfig

SupportedLanguages = Literal[
        "en",
        "es",
        "fr",
        "de",
        "it",
        "pt",
        "pl",
        "tr",
        "ru",
        "nl",
        "cs",
        "ar",
        "zh-cn",
        "hu",
        "ko",
        "ja",
        "hi",
        "auto",
        ""
    ]

@lru_cache(maxsize=1024)
def get_language(text: str):
    """Detect the language of input text.

    Uses langid for language detection and handles special cases like
    Chinese (zh-cn).

    Args:
        text (str): Text to detect language for.

    Returns:
        str: Detected language code.
    """
    detected_language =  langid.classify(text)[0].strip()
    if detected_language == "zh":
        # we use zh-cn
        detected_language = "zh-cn"
    return detected_language

def validate_language(language: str) -> SupportedLanguages:
    """Validate that a language code is supported.

    Args:
        language (str): Language code to validate.

    Returns:
        SupportedLanguages: Validated language code.

    Raises:
        ValueError: If the language is not supported.
    """
    supported = get_args(SupportedLanguages)
    if language not in supported:
        raise ValueError(
            f"Language {language} not supported. Must be one of {supported}"
        )
    return language # type: ignore

@dataclass
class TTSRequest:
    """Container for TTS inference request data.
    
    This class encapsulates all parameters needed for text-to-speech synthesis,
    including text input, speaker reference files, and generation parameters.
    It also handles audio preprocessing and language detection.

    Attributes:
        text (Union[AsyncGenerator[str, None], str, List[str]]): Input text to synthesize.
        speaker_files (Union[Union[str,List[str]], Union[bytes,List[bytes]]]): Reference audio for voice cloning.
        context_partial_function (Optional[Callable]): Optional function for context preparation.
        start_time (Optional[float]): Request start time.
        enhance_speech (bool): Whether to apply speech enhancement.
        audio_config (AudioPreprocessingConfig): Audio preprocessing configuration.
        language (SupportedLanguages): Language code for synthesis.
        request_id (str): Unique request identifier.
        load_sample_rate (int): Sample rate for loading audio files.
        sound_norm_refs (bool): Whether to normalize reference audio.
        max_ref_length (int): Maximum reference audio length in seconds.
        gpt_cond_len (int): Length of GPT conditioning.
        gpt_cond_chunk_len (int): Length of each conditioning chunk.
        stream (bool): Whether to stream the output.
        temperature (float): Sampling temperature.
        top_p (float): Nucleus sampling parameter.
        top_k (int): Top-k sampling parameter.
        repetition_penalty (float): Penalty for token repetition.
        length_penalty (float): Penalty for sequence length.
        do_sample (bool): Whether to use sampling for generation.
    """
    # Request metadata
    text: Union[AsyncGenerator[str, None], str, List[str]]

    speaker_files: Union[Union[str,List[str]], Union[bytes,List[bytes]]]
    context_partial_function: Optional[Callable] = None

    start_time: Optional[float] = None
    enhance_speech: bool = False
    audio_config: AudioPreprocessingConfig = field(default_factory=AudioPreprocessingConfig)
    language: SupportedLanguages = "auto"
    request_id: str = field(default_factory=lambda: uuid.uuid4().hex)
    load_sample_rate: int = 22050
    sound_norm_refs: bool = False

    # Voice conditioning parameters
    max_ref_length: int = 60
    gpt_cond_len: int = 30
    gpt_cond_chunk_len: int = 4

    # Generation parameters
    stream: bool = False
    temperature: float = 0.75
    top_p: float = 0.85
    top_k: int = 50
    repetition_penalty: float = 5.0
    length_penalty: float = 1.0
    do_sample: bool = True

    def __post_init__(self):
        """Initialize request after dataclass creation.
        
        Performs language detection if needed and sets up audio preprocessing.
        """
        if self.language == 'auto' and len(self.text) > 0:
            self.language = get_language(self.text)

        validate_language(self.language)
        self.processor = EnhancedAudioProcessor(self.audio_config)
        if isinstance(self.speaker_files, list) and self.enhance_speech:
            self.speaker_files = [self.preprocess_audio(f, self.audio_config) for f in self.speaker_files]

    def infer_language(self):
        """Infer the language of the input text if not specified.
        
        Updates the language attribute based on text content if set to 'auto'.
        """
        if self.language == 'auto':
            self.language = get_language(self.text)

    @cached_processing()
    def preprocess_audio(self, audio_source: Union[str, bytes], audio_config: AudioPreprocessingConfig) -> str:
        """Preprocess audio files for voice cloning.

        Applies audio enhancement and preprocessing according to the configuration.
        Results are cached to avoid reprocessing the same files.

        Args:
            audio_source (Union[str, bytes]): Path to audio file or audio data.
            audio_config (AudioPreprocessingConfig): Preprocessing configuration.

        Returns:
            str: Path to the processed audio file.

        Note:
            Processed files are stored in /tmp/auralis with unique identifiers.
        """
        try:
            temp_dir = Path("/tmp/auralis")
            temp_dir.mkdir(exist_ok=True)
            if isinstance(audio_source, str):
                audio_source = Path(audio_source)
                audio, sr = librosa.load(audio_source, sr=self.audio_config.sample_rate)
            else:
                audio, sr = librosa.load(io.BytesIO(audio_source), sr=self.audio_config.sample_rate)
            processed = self.processor.process(audio)

            output_path = temp_dir / (f"{hash(audio_source) if isinstance(audio_source, bytes) else audio_source.stem}"
                                      f"{uuid.uuid4().hex}"
                                      f"{'.wav' if isinstance(audio_source, bytes) else audio_source.suffix}")
            sf.write(output_path, processed, sr)
            return str(output_path)

        except Exception as e:
            print(f"Error processing audio: {e}. Using original file.")
            return audio_source

    def copy(self):
        """Create a deep copy of the request.

        Returns:
            TTSRequest: New instance with the same attributes.
        """
        copy_fields = {
            'text': self.text,
            'speaker_files': self.speaker_files,
            'enhance_speech': self.enhance_speech,
            'audio_config': self.audio_config,
            'language': self.language,
            'request_id': self.request_id,
            'load_sample_rate': self.load_sample_rate,
            'sound_norm_refs': self.sound_norm_refs,
            'max_ref_length': self.max_ref_length,
            'gpt_cond_len': self.gpt_cond_len,
            'gpt_cond_chunk_len': self.gpt_cond_chunk_len,
            'stream': self.stream,
            'temperature': self.temperature,
            'top_p': self.top_p,
            'top_k': self.top_k,
            'repetition_penalty': self.repetition_penalty,
            'length_penalty': self.length_penalty,
            'do_sample': self.do_sample
        }

        return TTSRequest(**copy_fields)


================================================================================
# File: auralis/common/logging/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/logging/__init__.py
================================================================================



================================================================================
# File: auralis/common/logging/logger.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/logging/logger.py
================================================================================

import logging
import sys
from pathlib import Path
from datetime import datetime
import colorama
from colorama import Fore, Back, Style
from typing import Optional, Union
import re
import traceback
import copy
import os

# Initialize colorama
colorama.init()

VLLM_LOGGER_LEVEL = logging.INFO

class VLLMLogOverrider:
    """Override VLLM loggers to use custom formatting.
    
    This class intercepts and reformats logs from VLLM to provide consistent
    formatting and better integration with the application's logging system.
    It handles special cases like performance metrics and filters out unwanted
    pipeline warnings.

    Attributes:
        target_logger (logging.Logger): Logger to redirect VLLM logs to.
        perf_pattern (re.Pattern): Pattern to identify performance metric logs.
        pipeline_warning_pattern (re.Pattern): Pattern to identify pipeline warnings.
    """

    def __init__(self, target_logger: logging.Logger):
        """Initialize VLLM log overrider.

        Args:
            target_logger (logging.Logger): Logger to redirect VLLM logs to.
        """
        self.target_logger = target_logger
        self.perf_pattern = re.compile(
            r"Avg prompt throughput:.+tokens/s,.+GPU KV cache usage:.+CPU KV cache usage:.+"
        )
        self.pipeline_warning_pattern = re.compile(r"Your model uses the legacy input pipeline instead of the new")
        self._override_vllm_loggers()

    def _override_vllm_loggers(self):
        """Override all VLLM loggers to use custom formatting.
        
        This method finds all loggers with names starting with 'vllm' and
        replaces their handlers with our custom handler.
        """
        global VLLM_LOGGER_LEVEL
        for name in logging.root.manager.loggerDict:
            if name.startswith('vllm'):
                vllm_logger = logging.getLogger(name)
                current_level = VLLM_LOGGER_LEVEL
                vllm_logger.handlers.clear()
                vllm_logger.propagate = False
                handler = self._create_redirecting_handler()
                vllm_logger.addHandler(handler)
                vllm_logger.setLevel(current_level)

    def _create_redirecting_handler(self):
        """Create a custom logging handler for VLLM logs.
        
        Returns:
            logging.Handler: Handler that reformats and redirects VLLM logs.
        """

        class RedirectHandler(logging.Handler):
            def __init__(self, target_logger, perf_pattern, pipe_warn):
                super().__init__()
                self.target_logger = target_logger
                self.pipe_warn = pipe_warn
                self.perf_pattern = perf_pattern

            def emit(self, record):
                msg = str(record.msg)
                if record.args:
                    msg = msg % record.args

                # Modify performance metrics format
                if self.perf_pattern.search(msg):
                    self.target_logger.log(record.levelno, f"Decoder performance: {msg}")
                elif self.pipe_warn.search(msg):
                    # Skip pipeline warning logs
                    pass
                else:
                    # Pass through all other logs normally
                    self.target_logger.log(record.levelno, msg)

        return RedirectHandler(self.target_logger, self.perf_pattern, self.pipeline_warning_pattern)


class ColoredFormatter(logging.Formatter):
    """Colored formatter for structured log output.
    
    This formatter adds color-coding, icons, timestamps, and file location
    information to log messages. It supports different color schemes for
    different log levels and includes special formatting for exceptions.

    !!! tip "Color Scheme"
        Each log level has its own color scheme and icon:

        - DEBUG: Cyan (dim) 🔍
        - INFO: Green ℹ️
        - WARNING: Yellow (bright) ⚠️
        - ERROR: Red (bright) ❌
        - CRITICAL: White on Red background 💀

    !!! example "Sample Output"
        ```
        10:30:45.123 | app.py:42 | ℹ️ INFO     | Starting application
        10:30:45.234 | model.py:156 | ⚠️ WARNING  | GPU memory running low
        ```

    See Also:
        - [`setup_logger`][auralis.common.logging.logger.setup_logger]: Main logger setup function
        - [`VLLMLogOverrider`][auralis.common.logging.logger.VLLMLogOverrider]: VLLM log handler

    Attributes:
        COLORS (dict): Color schemes for different log levels, including:
            - color: Foreground color
            - style: Text style (dim, normal, bright)
            - icon: Emoji icon for the log level
            - bg: Background color (for critical logs)
    """

    COLORS = {
        'DEBUG': {
            'color': Fore.CYAN,
            'style': Style.DIM,
            'icon': '🔍'
        },
        'INFO': {
            'color': Fore.GREEN,
            'style': Style.NORMAL,
            'icon': 'ℹ️'
        },
        'WARNING': {
            'color': Fore.YELLOW,
            'style': Style.BRIGHT,
            'icon': '⚠️'
        },
        'ERROR': {
            'color': Fore.RED,
            'style': Style.BRIGHT,
            'icon': '❌'
        },
        'CRITICAL': {
            'color': Fore.WHITE,
            'style': Style.BRIGHT,
            'bg': Back.RED,
            'icon': '💀'
        }
    }

    def format(self, record: logging.LogRecord) -> str:
        """Format a log record with color and structure.

        This method formats log records with:
        - Timestamp in HH:MM:SS.mmm format
        - File location (filename:line)
        - Color-coded level name with icon
        - Color-coded message
        - Formatted exception traceback if present

        Args:
            record (logging.LogRecord): Log record to format.

        Returns:
            str: Formatted log message with color and structure.
        """
        colored_record = copy.copy(record)

        # Get color scheme
        scheme = self.COLORS.get(record.levelname, {
            'color': Fore.WHITE,
            'style': Style.NORMAL,
            'icon': '•'
        })

        # Format timestamp
        timestamp = datetime.fromtimestamp(record.created).strftime('%H:%M:%S.%f')[:-3]

        # Get file location
        file_location = f"{os.path.basename(record.pathname)}:{record.lineno}"

        # Build components
        components = []

        # log formatting
        components.extend([
            f"{Fore.BLUE}{timestamp}{Style.RESET_ALL}",
            f"{Fore.WHITE}{Style.DIM}{file_location}{Style.RESET_ALL}",
            f"{scheme['color']}{scheme['style']}{scheme['icon']} {record.levelname:8}{Style.RESET_ALL}",
            f"{scheme['color']}{record.msg}{Style.RESET_ALL}"
        ])

        # Add exception info
        if record.exc_info:
            components.append(
                f"\n{Fore.RED}{Style.BRIGHT}"
                f"{''.join(traceback.format_exception(*record.exc_info))}"
                f"{Style.RESET_ALL}"
            )

        return " | ".join(components)


def setup_logger(
        name: Optional[Union[str, Path]] = None,
        level: int = logging.INFO
) -> logging.Logger:
    """Set up a colored logger with VLLM override.

    This function creates or retrieves a logger with colored output and
    automatic VLLM log interception. If a file path is provided as the name,
    it will use the filename (without extension) as the logger name.

    !!! note "VLLM Integration"
        When used with VLLM components, this logger automatically:
        
        - Intercepts and reformats VLLM logs
        - Filters redundant pipeline warnings
        - Enhances performance metric visibility

    !!! example "Basic Usage"
        ```python
        # Setup with module name
        logger = setup_logger(__name__)
        logger.info("Starting process")

        # Setup with custom name and level
        debug_logger = setup_logger("debug_logs", logging.DEBUG)
        debug_logger.debug("Detailed information")
        ```

    Args:
        name (Optional[Union[str, Path]], optional): Logger name or __file__ for
            module name. Defaults to None.
        level (int, optional): Logging level. Defaults to logging.INFO.

    Returns:
        logging.Logger: Configured logger instance.

    See Also:
        - [`ColoredFormatter`][auralis.common.logging.logger.ColoredFormatter]: Formatter class
        - [`VLLMLogOverrider`][auralis.common.logging.logger.VLLMLogOverrider]: VLLM integration
    """
    # Get logger name from file path
    if isinstance(name, (str, Path)) and Path(name).suffix == '.py':
        name = Path(name).stem

    # Get or create logger
    logger = logging.getLogger(name)
    logger.setLevel(level)

    # Only add handler if none exists
    if not logger.handlers:
        # Create console handler
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setFormatter(ColoredFormatter())
        logger.addHandler(console_handler)

        # Override VLLM loggers to use our logger
        VLLMLogOverrider(logger)

    return logger


def set_vllm_logging_level(level: logging):
    """Set logging level for all VLLM loggers.

    This function finds all loggers with names starting with 'vllm' and
    sets their logging level. This is useful for controlling the verbosity
    of VLLM's output.

    Args:
        level (logging): Logging level to set (e.g., logging.INFO, logging.DEBUG).

    Example:
        >>> set_vllm_logging_level(logging.WARNING)  # Reduce VLLM verbosity
    """
    for name in logging.root.manager.loggerDict:
        if name.startswith('vllm'):
            vllm_logger = logging.getLogger(name)
            vllm_logger.setLevel(level)


================================================================================
# File: auralis/common/metrics/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/metrics/__init__.py
================================================================================



================================================================================
# File: auralis/common/metrics/performance.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/common/metrics/performance.py
================================================================================

import time
from dataclasses import dataclass, field
from functools import wraps
from typing import TypeVar, AsyncGenerator, Callable, Any
from auralis.common.logging.logger import setup_logger


T = TypeVar('T')


@dataclass
class TTSMetricsTracker:
    """Performance metrics tracker for TTS generation.
    
    This class tracks and calculates various performance metrics for TTS generation,
    including throughput (requests and tokens per second) and latency. It maintains
    a sliding window of metrics and provides periodic logging.

    Attributes:
        window_start (float): Start time of current metrics window.
        last_log_time (float): Time of last metrics log.
        log_interval (float): Seconds between metric logs.
        window_tokens (int): Total tokens processed in current window.
        window_audio_seconds (float): Total audio seconds generated in window.
        window_requests (int): Total requests processed in window.
    """

    logger = setup_logger(__file__)

    window_start: float = field(default_factory=time.time)
    last_log_time: float = field(default_factory=time.time)
    log_interval: float = 5.0  # sec between logs

    window_tokens: int = 0
    window_audio_seconds: float = 0
    window_requests: int = 0

    @property
    def requests_per_second(self) -> float:
        """Calculate requests processed per second.

        Returns:
            float: Average requests per second in current window.
        """
        elapsed = time.time() - self.window_start
        return self.window_requests / elapsed if elapsed > 0 else 0

    @property
    def tokens_per_second(self) -> float:
        """Calculate tokens processed per second.

        Returns:
            float: Average tokens per second in current window.
        """
        elapsed = time.time() - self.window_start
        return self.window_tokens / elapsed if elapsed > 0 else 0

    @property
    def ms_per_second_of_audio(self) -> float:
        """Calculate processing time per second of generated audio.

        Returns:
            float: Milliseconds required to generate one second of audio.
        """
        elapsed = (time.time() - self.window_start) * 1000  # in ms
        return elapsed / self.window_audio_seconds if self.window_audio_seconds > 0 else 0

    def reset_window(self) -> None:
        """Reset all metrics for a new window.
        
        This method resets all counters and timestamps to start a fresh
        metrics collection window.
        """
        current_time = time.time()
        self.last_log_time = current_time
        # reset window
        self.window_start = current_time
        self.window_tokens = 0
        self.window_audio_seconds = 0
        self.window_requests = 0

    def update_metrics(self, tokens: int, audio_seconds: float) -> bool:
        """Update metrics with new generation results.

        Args:
            tokens (int): Number of tokens processed.
            audio_seconds (float): Seconds of audio generated.

        Returns:
            bool: Whether metrics should be logged based on log interval.
        """
        self.window_tokens += tokens
        self.window_audio_seconds += audio_seconds
        self.window_requests += 1

        current_time = time.time()
        should_log = current_time - self.last_log_time >= self.log_interval

        return should_log


metrics = TTSMetricsTracker()


def track_generation(func: Callable[..., AsyncGenerator[T, None]]) -> Callable[..., AsyncGenerator[T, None]]:
    """Decorator to track TTS generation performance metrics.

    This decorator wraps TTS generation functions to automatically track
    performance metrics for each generated audio chunk. It updates the global
    metrics tracker and logs performance statistics at regular intervals.

    Args:
        func (Callable[..., AsyncGenerator[T, None]]): Async generator function
            that yields TTS outputs.

    Returns:
        Callable[..., AsyncGenerator[T, None]]: Wrapped function that tracks metrics.

    Example:
        >>> @track_generation
        ... async def generate_speech(text: str) -> AsyncGenerator[TTSOutput, None]:
        ...     # Generation code here
        ...     yield output
    """

    @wraps(func)
    async def wrapper(*args, **kwargs) -> AsyncGenerator[T, None]:
        """Wrapped generation function that tracks metrics.

        Args:
            *args: Positional arguments passed to the generation function.
            **kwargs: Keyword arguments passed to the generation function.

        Yields:
            T: TTS output chunks with tracked metrics.
        """
        async for output in func(*args, **kwargs):
            if output.start_time:
                audio_seconds = output.array.shape[0] / output.sample_rate

                if metrics.update_metrics(output.token_length, audio_seconds):
                    metrics.logger.info(
                        f"Generation metrics | "
                        f"Throughput: {metrics.requests_per_second:.2f} req/s | "
                        f"{metrics.tokens_per_second:.1f} tokens/s | "
                        f"Latency: {metrics.ms_per_second_of_audio:.0f}ms per second of audio generated"
                    )
                    metrics.reset_window()
            yield output

    return wrapper

================================================================================
# File: auralis/entrypoints/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/entrypoints/__init__.py
================================================================================



================================================================================
# File: auralis/entrypoints/oai_server.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/entrypoints/oai_server.py
================================================================================

import argparse

import base64
import json
import logging
import uuid
from typing import  Optional

import aiohttp
import uvicorn
from fastapi import FastAPI, Header, HTTPException, Response
from fastapi.responses import JSONResponse
from starlette.responses import StreamingResponse

from auralis.core.tts import TTS
from auralis.common.definitions.openai import VoiceChatCompletionRequest, AudioSpeechGenerationRequest

app = FastAPI()

tts_engine: TTS

logger_str_to_logging={
    "info": logging.INFO,
    "warn": logging.WARNING,
    "err": logging.ERROR
}

def start_tts_engine(args, logging_level):
    global tts_engine
    tts_engine = (TTS(
        scheduler_max_concurrency=args.max_concurrency,
        vllm_logging_level=logging_level)
    .from_pretrained(
        args.model, gpt_model=args.gpt_model
    ))

@app.post("/v1/audio/speech")
async def generate_audio(request: AudioSpeechGenerationRequest):

    try:
        # Create TTSRequest with default params and auralis overrides
        tts_request = request.to_tts_request()

        output = await tts_engine.generate_speech_async(tts_request)
        output = output.change_speed(request.speed)
        audio_bytes = output.to_bytes(request.response_format)

        return Response(content=audio_bytes, media_type=f"audio/{request.response_format}")

    except Exception as e:
        return JSONResponse(status_code=500, content={"error": f"Error generating audio: {str(e)}"})


@app.post("/v1/chat/completions")
async def chat_completions(request: VoiceChatCompletionRequest, authorization: Optional[str] = Header(None)):
    if not authorization or not authorization.startswith("Bearer "):
        return JSONResponse(
            status_code=400,
            content={"error": "Authorization header with Bearer token is required"}
        )
    try:
        # Rest of the parameters
        openai_api_key = authorization[len("Bearer "):]
        modalities = request.modalities
        num_of_token_to_vocalize = request.vocalize_at_every_n_words

        # Initialize TTS request with auralis parameters
        tts_request = request.to_tts_request(text='')

        # Prepare OpenAI request
        openai_request_data = request.to_openai_request()

        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {openai_api_key}"
        }

        request_id = uuid.uuid4().hex

        # Validate modalities
        valid_modalities = ['text', 'audio']
        if not all(m in valid_modalities for m in modalities):
            return JSONResponse(
                status_code=400,
                content={"error": f"Invalid modalities. Must be one or more of {valid_modalities}"}
            )

        async def stream_generator():
            accumulated_content = ""

            try:
                async with aiohttp.ClientSession() as session:
                    async with session.post(request.openai_api_url, json=openai_request_data, headers=headers) as resp:
                        if resp.status != 200:
                            error_response = await resp.text()
                            raise HTTPException(status_code=resp.status, detail=error_response)

                        async for line in resp.content:
                            if not line:
                                continue

                            line = line.decode("utf-8").strip()
                            if not line.startswith("data:"):
                                continue

                            data_str = line[5:].strip()
                            if data_str == "[DONE]":
                                break

                            try:
                                data = json.loads(data_str)
                                content = data.get("choices", [{}])[0].get("delta", {}).get("content", "")

                                if content:
                                    accumulated_content += content
                                    # Only yield text if text modality is requested
                                    if 'text' in modalities:
                                        yield f"data: {json.dumps(data)}\n\n"

                                    if len(accumulated_content.split()) >= num_of_token_to_vocalize:
                                        # Only generate and yield audio if audio modality is requested
                                        if 'audio' in modalities:
                                            tts_request.text = accumulated_content
                                            tts_request.infer_language()
                                            audio_output = await tts_engine.generate_speech_async(tts_request)
                                            audio_base64 = base64.b64encode(audio_output.to_bytes()).decode("utf-8")
                                            yield f"data: {json.dumps({'id': request_id, 'object': 'audio.chunk', 'data': audio_base64})}\n\n"

                                        accumulated_content = ""
                                elif 'text' in modalities:
                                    # Other non-content text events only if text modality is requested
                                    yield f"data: {json.dumps(data)}\n\n"

                            except json.JSONDecodeError:
                                continue

                # Process any remaining content for audio if needed
                if accumulated_content and 'audio' in modalities:
                    tts_request.text = accumulated_content
                    tts_request.infer_language()
                    audio_output = await tts_engine.generate_speech_async(tts_request)
                    audio_base64 = base64.b64encode(audio_output.to_bytes()).decode("utf-8")
                    yield f"data: {json.dumps({'id': request_id, 'object': 'audio.chunk', 'data': audio_base64})}\n\n"

                # Send completion messages only if text modality is requested
                if 'text' in modalities:
                    yield f"data: {json.dumps({'id': request_id, 'object': 'chat.completion.chunk', 'choices': [{'delta': {}, 'index': 0, 'finish_reason': 'stop'}]})}\n\n"
                yield "data: [DONE]\n\n"

            except Exception as e:
                yield f"data: {json.dumps({'error': str(e)})}\n\n"
            finally:
                pass

        return StreamingResponse(stream_generator(), media_type="text/event-stream")

    except Exception as e:
        return JSONResponse(status_code=500, content={"error": f"Error in chat completions: {str(e)}"})

def main():
    parser = argparse.ArgumentParser(description="Auralis TTS FastAPI Server")
    parser.add_argument("--host", type=str, default="127.0.0.1", help="Host to run the server on")
    parser.add_argument("--port", type=int, default=8000, help="Port to run the server on")
    parser.add_argument("--model",  type=str, default='AstraMindAI/xttsv2', help="The base model to run")
    parser.add_argument("--gpt_model", type=str, default='AstraMindAI/xtts2-gpt', help="The gpt model to load alongside the base model, if present")
    parser.add_argument("--max_concurrency", type=int, default=8, help="The concurrency value that is used in the TTS Engine, it is directly connected to the memory consumption")
    parser.add_argument("--vllm_logging_level", type=str, default='warn', help="The vllm logging level, could be one of [info | warn | err]")

    args = parser.parse_args()

    # Initialize the TTS engine
    logging_level = logger_str_to_logging.get(args.vllm_logging_level, None)
    if not logging_level:
        raise ValueError("The logging level for vllm was not correct, please choose between ['info' | 'warn' | 'err']")

    start_tts_engine(args, logging_level)

    uvicorn.run(
        "auralis.entrypoints.oai_server:app",
        host=args.host,
        port=args.port,
    )

if __name__ == "__main__":
    main()

================================================================================
# File: auralis/models/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/__init__.py
================================================================================

from .xttsv2 import XTTSv2Engine



================================================================================
# File: auralis/models/base.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/base.py
================================================================================

from abc import ABC, abstractmethod
from pathlib import Path
from typing import AsyncGenerator, List, Union, Tuple, Optional

import torch
import torchaudio
from dataclasses import dataclass

from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo, nvmlDeviceGetCount
from vllm import RequestOutput

from auralis.common.definitions.output import TTSOutput
from auralis.common.definitions.requests import TTSRequest

Token = Union[int, List[int]]

AudioTokenGenerator = AsyncGenerator[RequestOutput, None]
AudioOutputGenerator = AsyncGenerator[TTSOutput, None]

SpeakerEmbeddings = torch.Tensor
GPTLikeDecoderConditioning = torch.Tensor
RequestsIds = List

TokenGeneratorsAndPossiblyConditioning = Union[
    Tuple[
        List[AudioTokenGenerator],
        RequestsIds,
        SpeakerEmbeddings,
        Union[List[GPTLikeDecoderConditioning], GPTLikeDecoderConditioning]
    ],
    Tuple[
        List[AudioTokenGenerator],
        RequestsIds,
        SpeakerEmbeddings
    ],
    Tuple[
        List[AudioTokenGenerator],
        RequestsIds,
        GPTLikeDecoderConditioning
    ],
    List[AudioTokenGenerator],
    RequestsIds
    ]

@dataclass
class ConditioningConfig:
    """Conditioning configuration for the model.
    
    Attributes:
        speaker_embeddings (bool): Whether the model uses speaker embeddings for voice cloning.
        gpt_like_decoder_conditioning (bool): Whether the model uses GPT-like decoder conditioning.
    """
    speaker_embeddings: bool = False
    gpt_like_decoder_conditioning: bool = False


class BaseAsyncTTSEngine(ABC, torch.nn.Module):
    """Base interface for asynchronous text-to-speech engines.
    
    This abstract class defines the interface for TTS engines that follow a two-phase generation process:
    1. Token generation: Converting text to intermediate tokens
    2. Audio generation: Converting tokens to speech waveforms
    
    The class supports both speaker conditioning and GPT-like decoder conditioning for enhanced control
    over the generated speech. It inherits from torch.nn.Module for neural network functionality.
    """

    @abstractmethod
    async def get_generation_context(
            self,
            request: TTSRequest,
    ) -> TokenGeneratorsAndPossiblyConditioning:
        """Get token generators and conditioning for audio generation.

        This method prepares the generation context by processing the input text and any
        conditioning signals (speaker embeddings, GPT conditioning) specified in the request.

        Args:
            request (TTSRequest): The TTS request containing input text and optional speaker files.

        Returns:
            TokenGeneratorsAndPossiblyConditioning: A tuple containing token generators and optional
                conditioning tensors (speaker embeddings and/or GPT conditioning).

        Raises:
            NotImplementedError: Must be implemented by subclasses.
        """
        raise NotImplementedError

    @abstractmethod
    async def process_tokens_to_speech(
            self,
            generator: AudioTokenGenerator,
            speaker_embeddings: SpeakerEmbeddings,
            multimodal_data: GPTLikeDecoderConditioning = None,
            request: TTSRequest = None,
    ) -> AudioOutputGenerator:
        """Generate speech from tokens with optional conditioning.

        This method converts the generated tokens into speech waveforms, applying any
        specified conditioning signals to control the voice characteristics.

        Args:
            generator (AudioTokenGenerator): Token generator from the first phase.
            speaker_embeddings (SpeakerEmbeddings): Speaker embeddings for voice cloning.
            multimodal_data (GPTLikeDecoderConditioning, optional): GPT conditioning data.
            request (TTSRequest, optional): Original TTS request for reference.

        Returns:
            AudioOutputGenerator: An async generator yielding TTSOutput objects containing
                audio chunks.

        Raises:
            NotImplementedError: Must be implemented by subclasses.
        """
        raise NotImplementedError

    @property
    def conditioning_config(self) -> ConditioningConfig:
        """Get the model's conditioning configuration.

        Returns:
            ConditioningConfig: Configuration specifying which conditioning types are supported.

        Raises:
            NotImplementedError: Must be implemented by subclasses.
        """
        raise NotImplementedError

    @property
    def device(self):
        """Get the current device of the model.

        Returns:
            torch.device: The device (CPU/GPU) where the model parameters reside.
        """
        return next(self.parameters()).device

    @property
    def dtype(self):
        """Get the current data type of the model parameters.

        Returns:
            torch.dtype: The data type of the model parameters.
        """
        return next(self.parameters()).dtype

    @abstractmethod
    def get_memory_usage_curve(self):
        """Get memory usage curve for different concurrency levels.

        This method tests VLLM memory usage at different concurrency levels to help
        optimize resource allocation.

        Raises:
            NotImplementedError: Must be implemented by subclasses.
        """
        raise NotImplementedError

    @staticmethod
    def get_memory_percentage(memory: int) -> Optional[float]:
        """Calculate the percentage of GPU memory that would be used.

        Args:
            memory (int): The amount of memory in bytes to check.

        Returns:
            Optional[float]: The fraction of total GPU memory that would be used,
                or None if no suitable GPU is found.
        """
        for i in range(torch.cuda.device_count()):
            free_memory, total_memory = torch.cuda.mem_get_info(i)
            used_memory = total_memory - free_memory
            estimated_mem_occupation = (memory + used_memory) / total_memory
            if estimated_mem_occupation > 0 and estimated_mem_occupation < 1:
                return estimated_mem_occupation
        return None

    @classmethod
    def from_pretrained(
            cls,
            *args,
            **kwargs
    )-> 'BaseAsyncTTSEngine':
        """Load a pretrained model.

        Args:
            *args: Variable length argument list.
            **kwargs: Arbitrary keyword arguments.

        Returns:
            BaseAsyncTTSEngine: An instance of the model loaded with pretrained weights.

        Raises:
            NotImplementedError: Must be implemented by subclasses.
        """
        raise NotImplementedError

    @staticmethod
    def load_audio(audio_path: Union[str, Path], sampling_rate: int = 22050) -> torch.Tensor:
        """Load and preprocess an audio file.

        This method loads an audio file, converts it to mono if needed, resamples to the
        target sampling rate, and ensures valid amplitude range.

        Args:
            audio_path (Union[str, Path]): Path to the audio file.
            sampling_rate (int, optional): Target sampling rate. Defaults to 22050.

        Returns:
            torch.Tensor: Preprocessed audio tensor with shape (1, samples).
        """
        audio, lsr = torchaudio.load(audio_path)

        # Stereo to mono if needed
        if audio.size(0) != 1:
            audio = torch.mean(audio, dim=0, keepdim=True)

        if lsr != sampling_rate:
            audio = torchaudio.functional.resample(audio, lsr, sampling_rate)

        # Clip audio invalid values
        audio.clip_(-1, 1)
        return audio

================================================================================
# File: auralis/models/registry.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/registry.py
================================================================================

MODEL_REGISTRY = {}

def register_model(name, model):
    MODEL_REGISTRY[name] = model

================================================================================
# File: auralis/models/xttsv2/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/xttsv2/__init__.py
================================================================================

from .XTTSv2 import XTTSv2Engine
from .components.vllm_mm_gpt import XttsGPT
from ..registry import register_model

from vllm import ModelRegistry

register_model("xtts", XTTSv2Engine)
ModelRegistry.register_model("XttsGPT", XttsGPT)


================================================================================
# File: auralis/models/xttsv2/XTTSv2.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/xttsv2/XTTSv2.py
================================================================================

import asyncio
import functools
import time
import uuid
from contextlib import asynccontextmanager

from pathlib import Path
from typing import Optional, List, Tuple, Union, AsyncGenerator
from concurrent.futures import ThreadPoolExecutor

import librosa
import numpy as np
import torch
import torchaudio
from torch import nn

from vllm import AsyncLLMEngine, AsyncEngineArgs, TokensPrompt, RequestOutput
from vllm.multimodal import MultiModalDataDict
from vllm.sampling_params import RequestOutputKind
from vllm.utils import Counter

from ..base import BaseAsyncTTSEngine, ConditioningConfig, TokenGeneratorsAndPossiblyConditioning
from ...common.logging.logger import setup_logger
from ...common.definitions.output import TTSOutput
from ...common.definitions.requests import TTSRequest
from ...common.utilities import wav_to_mel_cloning, load_audio

from .components.vllm_mm_gpt import LearnedPositionEmbeddings
from .config.tokenizer import XTTSTokenizerFast
from .config.xttsv2_config import XTTSConfig
from .config.xttsv2_gpt_config import XTTSGPTConfig

from .components.vllm.hidden_state_collector import HiddenStatesCollector
from .components.vllm.hijack import ExtendedSamplingParams, LogitsRepetitionPenalizer
from .components.tts.layers.xtts.hifigan_decoder import HifiDecoder
from .components.tts.layers.xtts.latent_encoder import ConditioningEncoder
from .components.tts.layers.xtts.perceiver_encoder import PerceiverResampler
from ...core.async_lru_cache import AsyncLRUCache


class XTTSv2Engine(BaseAsyncTTSEngine):
    """Asynchronous XTTS model implementation using VLLM's AsyncEngine.
    
    This class implements a high-performance text-to-speech engine based on the XTTS v2 architecture.
    It uses VLLM for efficient token generation and supports both speaker conditioning and
    GPT-like decoder conditioning for enhanced voice control. The implementation is optimized
    for inference speed through parallel processing and efficient memory management.

    Attributes:
        model_type (str): The model type identifier, set to "xtts".
    """

    model_type: "xtts"

    def __init__(self,
                 hifi_config: XTTSConfig,
                 gpt_config: XTTSGPTConfig,
                 pipeline_parallel_size: int = 1,
                 tensor_parallel_size: int = 1,
                 **kwargs):
        """Initialize the XTTS v2 engine.

        Args:
            hifi_config (XTTSConfig): Configuration for the HiFi-GAN decoder.
            gpt_config (XTTSGPTConfig): Configuration for the GPT model.
            pipeline_parallel_size (int, optional): Number of pipeline parallel partitions. Defaults to 1.
            tensor_parallel_size (int, optional): Number of tensor parallel partitions. Defaults to 1.
            **kwargs: Additional arguments including:
                - gpt_model: Path to the GPT model
                - max_concurrency: Maximum number of concurrent requests
        """
        super().__init__()

        self.max_gb_for_vllm_model = None

        self.logger = setup_logger(__file__)
        self.logger.info("Initializing XTTSv2Engine...")

        self.gpt_model = kwargs.pop('gpt_model')
        self.hifi_config = hifi_config
        self.gpt_config = gpt_config
        self.mel_bos_token_id = gpt_config.start_audio_token
        self.mel_eos_token_id = gpt_config.stop_audio_token
        self.tp = tensor_parallel_size
        self.pp = pipeline_parallel_size
        self.tokenizer = XTTSTokenizerFast.from_pretrained(self.gpt_model)
        self.request_counter = Counter()

        self.max_concurrency = kwargs.pop('max_concurrency', 10)
        semaphore_concurrency = max(1,self.max_concurrency // 6) * self.tp

        # Register buffer before creating modules
        self.register_buffer("mel_stats", torch.ones(80))

        # Initialize all nn.Module components
        self.conditioning_encoder = ConditioningEncoder(
            gpt_config.audio_config.mel_channels,
            gpt_config.hidden_size,
            num_attn_heads=gpt_config.num_attention_heads
        )

        self.text_embedding = nn.Embedding(
            gpt_config.number_text_tokens,
            gpt_config.hidden_size
        )

        self.text_pos_embedding = (
            LearnedPositionEmbeddings(
                gpt_config.max_text_tokens + 2,
                gpt_config.hidden_size,
                supports_pp=False
            )
            if gpt_config.max_audio_tokens != -1
            else functools.partial(gpt_config.null_position_embeddings, dim=gpt_config.hidden_size)
        )

        self.conditioning_perceiver = PerceiverResampler(
            dim=gpt_config.hidden_size,
            depth=2,
            dim_context=gpt_config.hidden_size,
            num_latents=32,
            dim_head=64,
            heads=8,
            ff_mult=4,
            use_flash_attn=False,
        )

        # Initialize HiFi-GAN decoder
        self.hifigan_decoder = HifiDecoder(
            input_sample_rate=self.hifi_config.input_sample_rate,
            output_sample_rate=self.hifi_config.output_sample_rate,
            output_hop_length=self.hifi_config.output_hop_length,
            ar_mel_length_compression=self.hifi_config.gpt_code_stride_len,
            decoder_input_dim=self.hifi_config.decoder_input_dim,
            d_vector_dim=self.hifi_config.d_vector_dim,
            cond_d_vector_in_each_upsampling_layer=self.hifi_config.cond_d_vector_in_each_upsampling_layer,
        )

        self.final_norm = nn.LayerNorm(gpt_config.hidden_size, eps=1e-5, bias=True)

        # Kept for model loading purposes
        self.text_head = nn.Linear(gpt_config.hidden_size, gpt_config.number_text_tokens, bias=True)

        self.get_memory_usage_curve()

        # Initialize VLLM engine at the end, settings its concurrency
        self.init_vllm_engine(self.max_concurrency)

        # Semaphore for concurrency control of the encoding process
        self.eval()

    def get_memory_usage_curve(self):
        """Calculate the memory usage curve based on concurrency level.
        
        Uses empirically determined polynomial coefficients to estimate memory requirements
        for different concurrency levels. This helps in optimizing resource allocation
        for the VLLM engine.
        """
        # empirically found values
        x = np.array([2, 5, 10, 16])
        y = np.array([1.25, 1.35, 1.45, 1.625])

        # polynomial fit
        coefficients = np.polyfit(x, y, 2)

        # create a polynomial object
        self.max_gb_for_vllm_model = (coefficients[0] * self.max_concurrency ** 2 +
                                      coefficients[1] * self.max_concurrency +
                                      coefficients[2])

    @property
    def conditioning_config(self) -> ConditioningConfig:
        return ConditioningConfig(
            speaker_embeddings=True, # noqa
            gpt_like_decoder_conditioning=True # noqa
        )

    def half(self):
        self.logger.warning("Cannot call .half() on XTTSv2Engine. it will be ignored.")
        # We cannot permit downcasting since it will throw an error while padding
        return

    def to(self, *args, **kwargs):
        # Block downcasting
        dtype = kwargs.get('dtype', None)
        if dtype == torch.float16 or dtype == torch.bfloat16:
            self.logger.warning("Cannot cast to half precision. Ignoring the request.")
            kwargs['dtype'] = torch.float32
        elif len(args) > 0 and (args[0] == torch.float16 or args[0] == torch.bfloat16):
            self.logger.warning("Cannot cast to half precision. Ignoring the request.")
            args = list(args)
            args[0] = torch.float32
            args = tuple(args)
        return super().to(*args, **kwargs)

    def init_vllm_engine(self, concurrency):
        """Initialize the VLLM engine with specified concurrency.

        Args:
            concurrency (int): Maximum number of concurrent requests to handle.

        Raises:
            RuntimeError: If unable to determine memory usage for model initialization.
        """
        """Initialize models with AsyncVLLMEngine."""
        max_seq_num = concurrency
        mem_utils = self.get_memory_percentage(self.max_gb_for_vllm_model * 1024 ** 3) #
        if not mem_utils:
            raise RuntimeError("Could not find the memory usage for the VLLM model initialization.")
        engine_args = AsyncEngineArgs(
            model=self.gpt_model,
            tensor_parallel_size=self.tp,
            pipeline_parallel_size=self.pp,
            dtype="auto",
            max_model_len=self.gpt_config.max_text_tokens +
                          self.gpt_config.max_audio_tokens +
                          32 + 5 + 3, # this is from the xttsv2 code, 32 is the conditioning sql
            gpu_memory_utilization=mem_utils,
            trust_remote_code=True,
            enforce_eager=True,
            limit_mm_per_prompt={"audio": 1}, # even if more audio are present, they'll be condendesed into one
            max_num_seqs=max_seq_num,
            disable_log_stats=True, # temporary fix for the log stats, there is a known bug in vllm that will be fixed in the next relaese
            max_num_batched_tokens=(self.gpt_config.max_text_tokens +
                                    self.gpt_config.max_audio_tokens +
                                    32 + 5 + 3) * max_seq_num,
            #We round to the nearest multiple of 32 and multiply by max_seq_num to get the max batched number (arbitrary) of tokens
        )
        self.logger.info(f"Initializing VLLM engine with args: {engine_args}")
        self.llm_engine = AsyncLLMEngine.from_engine_args(engine_args)

    @classmethod
    def from_pretrained(
            cls,
            pretrained_model_name_or_path: str,
            torch_dtype: torch.dtype = torch.float32,
            device_map: Optional[str] = "auto",
            tensor_parallel_size: int = 1,
            pipeline_parallel_size: int = 1,
            **kwargs,
    ) -> nn.Module:
        """Load a pretrained XTTS model from local path or Hugging Face Hub.

        Args:
            pretrained_model_name_or_path (str): Path to pretrained model or HF model identifier.
            torch_dtype (torch.dtype, optional): Model data type. Defaults to torch.float32.
            device_map (Optional[str], optional): Device mapping strategy. Defaults to "auto".
            tensor_parallel_size (int, optional): Number of tensor parallel partitions. Defaults to 1.
            pipeline_parallel_size (int, optional): Number of pipeline parallel partitions. Defaults to 1.
            **kwargs: Additional arguments passed to the model constructor.

        Returns:
            nn.Module: Loaded XTTS model instance.
        """
        from huggingface_hub import hf_hub_download
        import json
        import os

        # Download and load configs
        if not os.path.exists(pretrained_model_name_or_path):
            config_file = hf_hub_download(
                repo_id=pretrained_model_name_or_path,
                filename="config.json"
            )
            with open(config_file, 'r') as f:
                config = json.load(f)

        else:
            # Load from local path
            with open(os.path.join(pretrained_model_name_or_path, "config.json"), 'r') as f:
                config = json.load(f)

        # Initialize configs
        gpt_config = XTTSGPTConfig(**config['gpt_config'])
        hifi_config = XTTSConfig(**config)

        # Initialize model
        model = cls(
            hifi_config=hifi_config,
            gpt_config=gpt_config,
            tensor_parallel_size=tensor_parallel_size,
            pipeline_parallel_size=pipeline_parallel_size,
            **kwargs
        )

        # Load model weights
        if not os.path.exists(pretrained_model_name_or_path):
            hifigan_weights = hf_hub_download(
                repo_id=pretrained_model_name_or_path,
                filename="xtts-v2.safetensors"
            )
        else:
            hifigan_weights = os.path.join(pretrained_model_name_or_path, "xtts-v2.safetensors")

        import safetensors.torch

        # Load HiFi-GAN weights
        hifigan_state = safetensors.torch.load_file(hifigan_weights)
        model.load_state_dict(hifigan_state)

        # Set model properties
        model.config = config

        # Cast model to specified dtype
        model = model.to(torch_dtype)
        model = model.to('cuda')

        return model

    async def _get_speaker_embedding(self, audio, sr):
        """Extract speaker embedding from audio.

        Args:
            audio: Input audio tensor.
            sr: Sampling rate of the audio.

        Returns:
            torch.Tensor: Speaker embedding tensor.
        """
        audio_16k = torchaudio.functional.resample(audio, sr, 16000)
        return (
                self.hifigan_decoder.speaker_encoder.forward(audio_16k.to(self.device), l2_norm=True)
                .unsqueeze(-1)
                .to(self.device)
            )

    async def _merge_conditioning(self,
                                  text_conditioning: List[torch.Tensor],
                                  audio_conditioning: torch.Tensor) -> List[torch.Tensor]:
        """Merge text and audio conditioning signals.

        Args:
            text_conditioning (List[torch.Tensor]): List of text conditioning tensors.
            audio_conditioning (torch.Tensor): Audio conditioning tensor.

        Returns:
            List[torch.Tensor]: List of merged conditioning tensors.
        """
        cond_latents = []
        for text_embedding in text_conditioning:
            # Concatenate along sequence dimension
            cond_latents.append((torch.cat([audio_conditioning, text_embedding], dim=1).squeeze(0)
                                 .to(self.llm_engine.engine.model_config.dtype)))
        return cond_latents

    def get_gpt_cond_latents(self, audio, sr, length: int = 30, chunk_length: int = 6):
        """Generate GPT conditioning latents from audio.

        Args:
            audio: Input audio tensor.
            sr: Sampling rate of the audio.
            length (int, optional): Maximum reference length in seconds. Defaults to 30.
            chunk_length (int, optional): Length of each conditioning chunk. Defaults to 6.

        Returns:
            torch.Tensor: GPT conditioning latents.
        """
        if sr != 22050:
            audio = torchaudio.functional.resample(audio, sr, 22050)
        if length > 0:
            audio = audio[:, : 22050 * length]
        if self.gpt_config.use_perceiver_resampler:
            style_embs = []
            for i in range(0, audio.shape[1], 22050 * chunk_length):
                audio_chunk = audio[:, i: i + 22050 * chunk_length]

                # if the chunk is too short ignore it
                if audio_chunk.size(-1) < 22050 * 0.33:
                    continue

                mel_chunk = wav_to_mel_cloning(
                    audio_chunk,
                    mel_norms=self.mel_stats.cpu(),
                    n_fft=2048,
                    hop_length=256,
                    win_length=1024,
                    power=2,
                    normalized=False,
                    sample_rate=22050,
                    f_min=0,
                    f_max=8000,
                    n_mels=80,
                )
                style_emb = self.get_style_emb(mel_chunk.to(self.device), None)
                style_embs.append(style_emb)

            # mean style embedding
            cond_latent = torch.stack(style_embs).mean(dim=0)
        else:
            mel = wav_to_mel_cloning(
                audio,
                mel_norms=self.mel_stats.cpu(),
                n_fft=4096,
                hop_length=1024,
                win_length=4096,
                power=2,
                normalized=False,
                sample_rate=22050,
                f_min=0,
                f_max=8000,
                n_mels=80,
            )
            cond_latent = self.get_style_emb(mel.to(self.device))
        return cond_latent.transpose(1, 2)

    async def get_conditioning_latents(
            self,
            audio_reference,
            max_ref_length=30,
            gpt_cond_len=6,
            gpt_cond_chunk_len=6,
            librosa_trim_db=None,
            sound_norm_refs=False,
            load_sr=22050,
    ):
        """Generate conditioning latents from reference audio.

        Args:
            audio_reference: Reference audio file path or tensor.
            max_ref_length (int, optional): Maximum reference length in seconds. Defaults to 30.
            gpt_cond_len (int, optional): Length of GPT conditioning. Defaults to 6.
            gpt_cond_chunk_len (int, optional): Length of each conditioning chunk. Defaults to 6.
            librosa_trim_db (float, optional): Trim silence below this dB threshold.
            sound_norm_refs (bool, optional): Whether to normalize reference audio. Defaults to False.
            load_sr (int, optional): Sampling rate for loading audio. Defaults to 22050.

        Returns:
            Tuple: GPT conditioning latents and speaker embeddings.
        """
        # Deal with multiple references
        assert (isinstance(audio_reference, bytes) or
                isinstance(audio_reference, str) or
                isinstance(audio_reference, list)), f"audio_reference must be a string, byte or a list but it is {type(audio_reference)}"

        if not isinstance(audio_reference, list):
            audio_paths = [audio_reference]
        else:
            audio_paths = audio_reference

        speaker_embeddings = []
        audios = []
        for file_path in audio_paths:
            audio = load_audio(file_path, load_sr)
            audio = audio[:, : load_sr * max_ref_length].to(self.device).to(self.dtype)
            if sound_norm_refs:
                audio = (audio / torch.abs(audio).max()) * 0.75
            if librosa_trim_db is not None:
                audio = librosa.effects.trim(audio, top_db=librosa_trim_db)[0]

            # Compute latents for the decoder
            speaker_embedding = await self._get_speaker_embedding(audio, load_sr)
            speaker_embeddings.append(speaker_embedding)

            audios.append(audio)

        # Merge all the audios and compute the latents for the GPT
        full_audio = torch.cat(audios, dim=-1)
        gpt_cond_latents = await asyncio.to_thread(self.get_gpt_cond_latents,
            full_audio, load_sr, length=gpt_cond_len, chunk_length=gpt_cond_chunk_len
        )  # [1, 1024, T]

        speaker_embedding = torch.stack(speaker_embeddings)
        speaker_embedding = speaker_embedding.mean(dim=0)

        return gpt_cond_latents, speaker_embedding

    @asynccontextmanager
    async def cuda_memory_manager(self):
        """Context manager for CUDA memory management.
        
        Ensures proper allocation and deallocation of CUDA memory during processing.
        """
        try:
            yield
        finally:
            torch.cuda.synchronize()
            await asyncio.sleep(0.1)
            torch.cuda.empty_cache()

    def get_style_emb(self, cond_input: torch.Tensor, return_latent: Optional[bool] = False) -> torch.Tensor:
        """Extract style embedding from conditioning input.

        Args:
            cond_input (torch.Tensor): Conditioning input tensor.
            return_latent (Optional[bool], optional): Whether to return latent representation. Defaults to False.

        Returns:
            torch.Tensor: Style embedding tensor.
        """
        if not return_latent:
            if cond_input.ndim == 4:
                cond_input = cond_input.squeeze(1)
            conds = self.conditioning_encoder(cond_input)

            if hasattr(self, 'conditioning_perceiver'):
                conds = self.conditioning_perceiver(
                    conds.permute(0, 2, 1)
                ).transpose(1, 2) # (b,d,32)
        else:
            conds = cond_input.unsqueeze(1)
        return conds

    async def prepare_text_tokens_async(self, text: str, language: str, split_text=False) \
            -> Tuple[List[Union[int, List[int]]], List[torch.Tensor]]:
        """Prepare text tokens and embeddings asynchronously.

        Args:
            text (str): Input text to tokenize.
            language (str): Language code.
            split_text (bool, optional): Whether to split text into chunks. Defaults to False.

        Returns:
            Tuple: Token IDs and text embeddings.
        """
        self.logger.debug(f"Preparing text tokens for text: {text}")
        async def elaborate_tokens(text_tokens: List[int]) -> torch.Tensor:
            text_tokens.insert(0, self.tokenizer.bos_token_id)
            text_tokens.append(self.tokenizer.eos_token_id)
            return torch.tensor(text_tokens).unsqueeze(0).to(self.text_embedding.weight.device)

        async def embed_tokens(text_tokens: Union[torch.Tensor, List[torch.Tensor]]) -> List[torch.Tensor]:
            embeds = []
            if isinstance(text_tokens, list):
                for list_element in text_tokens:
                    embeds.append(self.text_embedding(list_element) + self.text_pos_embedding(list_element))
            else:
                embeds.append(self.text_embedding(text_tokens) + self.text_pos_embedding(text_tokens))
            return embeds

        fake_tokens_for_audio_generation = []
        if split_text:
            text_tokens = self.tokenizer.batch_encode_with_split(text, lang=[language])
            for idx, text_token in enumerate(text_tokens):
                text_tokens[idx] = await elaborate_tokens(text_token)
                fake_tokens_for_audio_generation.append([1] * len(text_token))
        else:
            text_tokens = self.tokenizer(text, lang=[language])['input_ids'][0]
            text_tokens = await elaborate_tokens(text_tokens)
            fake_tokens_for_audio_generation = [1] * len(text_tokens)
        return fake_tokens_for_audio_generation, await embed_tokens(text_tokens)



    async def prepare_inputs_async(self, text: str, language: str, speaker_file: List[Union[str, Path]],
                                   max_ref_length: int, gpt_cond_len: int, gpt_cond_chunk_len: int, split_text: bool) \
            -> Tuple[List[List[int]], List[torch.Tensor], torch.Tensor]:
        """Prepare all inputs for speech generation asynchronously.

        Args:
            text (str): Input text.
            language (str): Language code.
            speaker_file (List[Union[str, Path]]): List of speaker reference files.
            max_ref_length (int): Maximum reference length in seconds.
            gpt_cond_len (int): Length of GPT conditioning.
            gpt_cond_chunk_len (int): Length of each conditioning chunk.
            split_text (bool): Whether to split text into chunks.

        Returns:
            Tuple: Token IDs, text embeddings, and speaker embeddings.
        """
        # Tokenize text based on the language
        text_tokens, text_embeddings = await self.prepare_text_tokens_async(text, language, split_text)

        # Load the speaker file and convert it to a tensor
        gpt_cond_latent, speaker_embeddings = await self.get_audio_conditioning(
            speaker_file,
            max_ref_length,
            gpt_cond_len,
            gpt_cond_chunk_len
        )

        cond_latents = await self._merge_conditioning(text_embeddings, gpt_cond_latent)

        return text_tokens, cond_latents, speaker_embeddings

    @AsyncLRUCache()
    async def get_audio_conditioning(
            self,
            audio_reference: [str, Path],
            max_ref_length=30,
            gpt_cond_len=6,
            gpt_cond_chunk_len=6,
            librosa_trim_db=None,
            sound_norm_refs=False,
            load_sr=22050,
    ):
        """Generate audio conditioning from reference files.

        Args:
            audio_reference ([str, Path]): Reference audio file paths.
            max_ref_length (int, optional): Maximum reference length in seconds. Defaults to 30.
            gpt_cond_len (int, optional): Length of GPT conditioning. Defaults to 6.
            gpt_cond_chunk_len (int, optional): Length of each conditioning chunk. Defaults to 6.
            librosa_trim_db (float, optional): Trim silence below this dB threshold.
            sound_norm_refs (bool, optional): Whether to normalize reference audio. Defaults to False.
            load_sr (int, optional): Sampling rate for loading audio. Defaults to 22050.

        Returns:
            Tuple: GPT conditioning latents and speaker embeddings.
        """
        """Async version of get_conditioning_latents with concurrency control."""
        # Run the original get_conditioning_latents in executor
        result = await self.get_conditioning_latents(
            audio_reference,
            max_ref_length,
            gpt_cond_len,
            gpt_cond_chunk_len,
            librosa_trim_db,
            sound_norm_refs,
            load_sr
        )
        return result

    async def get_model_logits(
            self,
            token_ids: List[int],
            conditioning: MultiModalDataDict,
            request_id: str,
    ) -> torch.Tensor:
        """Get model logits for token generation.

        Args:
            token_ids (List[int]): Input token IDs.
            conditioning (MultiModalDataDict): Conditioning data.
            request_id (str): Unique request identifier.

        Returns:
            torch.Tensor: Model logits.
        """
        """
        Get model logits for a request with retry logic for empty hidden states.

        Args:
            token_ids: Input token IDs
            conditioning: Conditioning data
            request_id: Unique request ID
        """
        request_id = f"{request_id}_logits"


        # Reset token_ids on each attempt
        token_ids = ([self.mel_bos_token_id] + list(token_ids) + [self.mel_eos_token_id] * 4)
        # we need 5 eos tokens

        engine_inputs = TokensPrompt(prompt_token_ids=token_ids)
        conditioning['audio']['sequence_length'] = len(token_ids)

        engine_inputs["multi_modal_data"] = conditioning

        hidden_states_collector = HiddenStatesCollector()
        # Bind the collector to this request
        bound_collector = hidden_states_collector.bind_to_request(request_id)

        # Set up sampling parameters with the bound collector
        sampling_params = ExtendedSamplingParams(
            detokenize=False,
            request_id=request_id,
            max_tokens=1,
            hidden_state_collector=bound_collector,
            output_kind=RequestOutputKind.FINAL_ONLY
        )

        # Generate with unique request ID
        generator = self.llm_engine.generate(
            prompt=engine_inputs,
            sampling_params=sampling_params,
            request_id=request_id
        )

        async for output in generator:  # consume the generator
            if output.finished:
                pass

        # Get the collected hidden states
        hidden_states = await hidden_states_collector.get_hidden_states(request_id)

        if hidden_states is None:
            raise RuntimeError(
                f"No hidden states collected for request {request_id}. "
                f"This should never happen! Please report this issue on GitHub."
            )
        start_of_audio_hs = conditioning["audio"]["embeds"].shape[0] # type: ignore
        # Successfully got hidden states
        return self.final_norm(hidden_states[start_of_audio_hs:-5, ...].unsqueeze(0).to(self.device).to(self.dtype))


    @torch.inference_mode()
    async def get_generation_context(self,
                                     request: TTSRequest,
                                     ) -> TokenGeneratorsAndPossiblyConditioning:
        """Get generation context for speech synthesis.

        Args:
            request (TTSRequest): TTS request object.
            gpt_cond_latent (Optional[torch.Tensor], optional): Pre-computed GPT conditioning latents.
            speaker_embeddings (Optional[torch.Tensor], optional): Pre-computed speaker embeddings.

        Returns:
            TokenGeneratorsAndPossiblyConditioning: Token generators and conditioning tensors.
        """
        # Prepare input with conditioning
        tokens_list, gpt_embed_inputs, speaker_embeddings = await self.prepare_inputs_async(
            request.text,
            request.language,
            request.speaker_files,
            request.max_ref_length,
            request.gpt_cond_len,
            request.gpt_cond_chunk_len,
            split_text=True  # Split text to avoid OOM on big texts
        )


        # Start all requests in parallel
        generators = []
        requests_id = []
        for seq_index, sequence in enumerate(tokens_list):
            sampling_params = ExtendedSamplingParams(
                temperature=request.temperature,
                top_p=request.top_p,
                detokenize=False,
                request_id=uuid.uuid4(),
                top_k=request.top_k,
                logits_processors=[LogitsRepetitionPenalizer(request.repetition_penalty)],
                repetition_penalty=1.0,  # Since we're handling repetition penalty manually
                max_tokens=self.gpt_config.gpt_max_audio_tokens,
                ignore_eos=True,  # Ignore the tokenizer eos token since it is for textual generation
                stop_token_ids=[self.mel_eos_token_id],
                output_kind=RequestOutputKind.FINAL_ONLY
            )

            engine_inputs = TokensPrompt(prompt_token_ids=sequence)
            if gpt_embed_inputs is not None:
                engine_inputs["multi_modal_data"] = {
                    "audio": {
                        "embeds": gpt_embed_inputs[seq_index],
                        "is_logits_only_mode": False,
                        "sequence_length": len(sequence)
                    }
                }
            request_id =f"{request.request_id}_{seq_index}"
            # Get audio token generator from VLLM
            token_generator = self.llm_engine.generate(
                prompt=engine_inputs,
                sampling_params=sampling_params,
                request_id=request_id,
            )
            generators.append(token_generator)
            requests_id.append(request_id)

        return generators, requests_id, speaker_embeddings, gpt_embed_inputs

    @torch.inference_mode()
    async def process_tokens_to_speech(
            self,
            generator: AsyncGenerator[RequestOutput, None],
            speaker_embeddings: Optional[torch.Tensor] = None,
            multimodal_data: Optional[torch.Tensor] = None,
            request: TTSRequest = None,
    ) -> AsyncGenerator[TTSOutput, None]:
        """Convert generated tokens to speech waveforms.

        Args:
            generator (AsyncGenerator[RequestOutput, None]): Token generator.
            speaker_embeddings (Optional[torch.Tensor], optional): Speaker embeddings.
            multimodal_data (Optional[torch.Tensor], optional): Additional multimodal data.
            request (TTSRequest, optional): Original TTS request.

        Yields:
            TTSOutput: Generated speech chunks.
        """
        assert speaker_embeddings is not None, "Speaker embeddings must be provided for speech generation with XTTSv2."
        assert multimodal_data is not None, "Multimodal data must be provided for speech generation with XTTSv2."


        async for output in generator:
            # TODO, to lower ttfb we can do output delta here at least until the generation do't accomulate enough tokens
            if output.finished:
                # get the hidden states
                hidden_states = await self.get_model_logits(
                    list(output.outputs[0].token_ids),
                    {
                        "audio": {
                            'embeds': multimodal_data,  # Use multimodal data for conditioning
                            "is_logits_only_mode": True,
                            "sequence_length": False # to be inserted later
                        },
                    },
                    output.request_id
                )


                async with self.decoder_semaphore:
                    async with self.cuda_memory_manager():
                        wav = (await asyncio.to_thread(self.hifigan_decoder,
                                hidden_states,
                                g=speaker_embeddings
                            )).cpu().detach().numpy().squeeze()
                         # noqa

                        # yield the audio output
                        yield TTSOutput(array= wav,
                                        start_time = request.start_time,
                                        token_length = len(output.outputs[0].token_ids)
                                        )



    async def shutdown(self):
        self.llm_engine.shutdown_background_loop()



================================================================================
# File: auralis/models/xttsv2/config/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/xttsv2/config/__init__.py
================================================================================



================================================================================
# File: auralis/models/xttsv2/config/xttsv2_config.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/xttsv2/config/xttsv2_config.py
================================================================================

from dataclasses import asdict, dataclass
from typing import Dict, Optional, List
from transformers.configuration_utils import PretrainedConfig
from transformers.utils import logging

logger = logging.get_logger(__name__)


@dataclass
class GPTAudioConfig:
    """Configuration for GPT audio processing parameters.
    
    This class defines the basic audio processing parameters used by the GPT
    component of the XTTS model.

    Attributes:
        mel_channels (int): Number of mel-spectrogram channels. Defaults to 80.
        sample_rate (int): Input audio sampling rate in Hz. Defaults to 22050.
        output_sample_rate (int): Output audio sampling rate in Hz. Defaults to 24000.
    """
    mel_channels: int = 80
    sample_rate: int = 22050
    output_sample_rate: int = 24000

@dataclass
class XTTSAudioConfig:
    """Configuration for XTTS audio processing parameters.
    
    This class defines the complete set of audio processing parameters used
    throughout the XTTS model, including mel-spectrogram generation and
    normalization.

    Attributes:
        sample_rate (int): Input audio sampling rate in Hz. Defaults to 22050.
        output_sample_rate (int): Output audio sampling rate in Hz. Defaults to 24000.
        mel_channels (int): Number of mel-spectrogram channels. Defaults to 80.
        hop_length (int): Number of samples between STFT columns. Defaults to 256.
        win_length (int): Window size for STFT. Defaults to 1024.
        n_fft (int): FFT size. Defaults to 1024.
        fmin (int): Minimum frequency for mel scale. Defaults to 0.
        fmax (int): Maximum frequency for mel scale. Defaults to 8000.
        power (float): Power of the magnitude spectrogram. Defaults to 1.0.
        mel_norms_file (Optional[str]): Path to mel-spectrogram normalization file.
    """
    sample_rate: int = 22050
    output_sample_rate: int = 24000
    mel_channels: int = 80
    hop_length: int = 256
    win_length: int = 1024
    n_fft: int = 1024
    fmin: int = 0
    fmax: int = 8000
    power: float = 1.0
    mel_norms_file: Optional[str] = None


class XTTSGPTConfig(PretrainedConfig):
    """Configuration for the GPT component of XTTS.
    
    This class defines the architecture and behavior of the GPT model used in XTTS.
    It inherits from HuggingFace's PretrainedConfig for compatibility with the
    transformers library.

    The GPT model is responsible for generating audio tokens from text tokens,
    with support for various conditioning signals and architectural features.

    Attributes:
        hidden_size (int): Size of hidden layers. Defaults to 1024.
        n_inner (int): Size of feed-forward inner layer. Defaults to 4096.
        num_hidden_layers (int): Number of transformer layers. Defaults to 30.
        num_attention_heads (int): Number of attention heads. Defaults to 16.
        vocab_size (int): Size of text vocabulary. Defaults to 6681.
        number_text_tokens (int): Explicit text token vocabulary size.
        start_text_token (Optional[int]): Token ID for text start.
        stop_text_token (Optional[int]): Token ID for text end.
        num_audio_tokens (int): Size of audio token vocabulary. Defaults to 1026.
        start_audio_token (int): Token ID for audio start. Defaults to 1024.
        stop_audio_token (int): Token ID for audio end. Defaults to 1025.
        max_audio_tokens (int): Maximum audio sequence length. Defaults to 605.
        max_text_tokens (int): Maximum text sequence length. Defaults to 402.
        max_prompt_tokens (int): Maximum prompt sequence length. Defaults to 70.
        use_masking_gt_prompt_approach (bool): Whether to use masking. Defaults to True.
        use_perceiver_resampler (bool): Whether to use perceiver. Defaults to True.
        kv_cache (bool): Whether to use KV cache. Defaults to True.
        enable_redaction (bool): Whether to enable redaction. Defaults to False.
        audio_config (Optional[Dict]): Audio processing configuration.
    """
    model_type = "xtts_gpt"

    def __init__(
            self,
            # Model architecture
            hidden_size: int = 1024,  # gpt_n_model_channels in original
            n_inner: int = 4096,
            num_hidden_layers: int = 30,  # gpt_layers in original
            num_attention_heads: int = 16,  # gpt_n_heads in original

            # Tokenizer settings
            vocab_size: int = 6681,  # gpt_number_text_tokens in original
            number_text_tokens: int = 6681,  # Explicit text token vocabulary size
            start_text_token: Optional[int] = None,
            stop_text_token: Optional[int] = None,

            # Audio token settings
            num_audio_tokens: int = 1026,  # gpt_num_audio_tokens in original
            start_audio_token: int = 1024,  # gpt_start_audio_token in original
            stop_audio_token: int = 1025,  # gpt_stop_audio_token in original

            # Sequence length settings
            max_audio_tokens: int = 605,  # gpt_max_audio_tokens in original
            max_text_tokens: int = 402,  # gpt_max_text_tokens in original
            max_prompt_tokens: int = 70,  # gpt_max_prompt_tokens in original
            gpt_max_audio_tokens: int = 605,  # Used for generation

            # Model behavior settings
            use_masking_gt_prompt_approach: bool = True,  # gpt_use_masking_gt_prompt_approach in original
            use_perceiver_resampler: bool = True,  # gpt_use_perceiver_resampler in original
            kv_cache: bool = True,
            enable_redaction: bool = False,

            # GPT batch settings
            gpt_batch_size: int = 1,

            # Audio processing
            audio_config: Optional[Dict] = None,

            # Architecture specifics
            layer_norm_epsilon: float = 1e-5,
            initializer_range: float = 0.02,
            add_cross_attention: bool = False,
            scale_attn_by_inverse_layer_idx: bool = False,
            reorder_and_upcast_attn: bool = False,

            # Size settings for the decoder
            decoder_input_dim: int = 1024,
            architectures=["XttsGPT"],
            auto_map={
                "AutoConfig": "AstraMindAI/xtts2-gpt--gpt_config.XTTSGPTConfig",
                "AutoModelForCausalLM": "AstraMindAI/xtts2-gpt--xtts2_gpt_modeling.XttsGPT",
            },
            activation_function: str = "gelu",
            attn_pdrop: float = 0.1,
            **kwargs
    ):
        super().__init__(**kwargs)
        self.architectures = architectures
        self.auto_map = auto_map
        self.audio_config = GPTAudioConfig(
            **audio_config if audio_config is not None else {}
        )
        self.activation_function = activation_function
        self.attn_pdrop = attn_pdrop
        self.hidden_size = hidden_size
        self.n_inner = n_inner
        self.num_hidden_layers = num_hidden_layers
        self.num_attention_heads = num_attention_heads

        self.vocab_size = vocab_size
        self.number_text_tokens = number_text_tokens
        self.start_text_token = start_text_token
        self.stop_text_token = stop_text_token

        self.num_audio_tokens = num_audio_tokens
        self.start_audio_token = start_audio_token
        self.stop_audio_token = stop_audio_token

        self.max_audio_tokens = max_audio_tokens
        self.max_text_tokens = max_text_tokens
        self.max_prompt_tokens = max_prompt_tokens
        self.gpt_max_audio_tokens = gpt_max_audio_tokens

        self.use_masking_gt_prompt_approach = use_masking_gt_prompt_approach
        self.use_perceiver_resampler = use_perceiver_resampler
        self.kv_cache = kv_cache
        self.enable_redaction = enable_redaction

        self.gpt_batch_size = gpt_batch_size

        self.layer_norm_epsilon = layer_norm_epsilon
        self.initializer_range = initializer_range
        self.add_cross_attention = add_cross_attention
        self.scale_attn_by_inverse_layer_idx = scale_attn_by_inverse_layer_idx
        self.reorder_and_upcast_attn = reorder_and_upcast_attn

        self.decoder_input_dim = decoder_input_dim

    def to_dict(self) -> Dict:
        """Convert configuration to dictionary format.

        Returns:
            Dict: Configuration dictionary including audio config.
        """
        output = super().to_dict()
        output["audio_config"] = asdict(self.audio_config)
        return output

    @classmethod
    def from_dict(cls, config_dict: Dict, *args, **kwargs) -> "XTTSGPTConfig":
        """Create configuration from dictionary.

        Args:
            config_dict (Dict): Configuration dictionary.
            *args: Additional positional arguments.
            **kwargs: Additional keyword arguments.

        Returns:
            XTTSGPTConfig: Configuration instance.
        """
        return cls(**config_dict)


class XTTSConfig(PretrainedConfig):
    """Configuration for the complete XTTS model.
    
    This class defines the configuration for all XTTS components except the GPT
    model (which has its own config). It includes settings for audio processing,
    model architecture, training, tokenization, and language support.

    Attributes:
        audio_config (XTTSAudioConfig): Audio processing configuration.
        input_sample_rate (int): Input audio sampling rate. Defaults to 22050.
        output_sample_rate (int): Output audio sampling rate. Defaults to 24000.
        output_hop_length (int): Output hop length. Defaults to 256.
        decoder_input_dim (int): Decoder input dimension. Defaults to 1024.
        d_vector_dim (int): Speaker embedding dimension. Defaults to 512.
        cond_d_vector_in_each_upsampling_layer (bool): Whether to condition each
            upsampling layer. Defaults to True.
        gpt_code_stride_len (int): GPT code stride length. Defaults to 1024.
        duration_const (int): Duration constant. Defaults to 102400.
        tokenizer_file (str): Path to tokenizer file.
        num_chars (int): Number of characters. Defaults to 255.
        languages (List[str]): Supported language codes.
        gpt (XTTSGPTConfig): GPT model configuration.
    """
    model_type = "xtts"

    def __init__(
            self,
            # Audio settings
            audio_config: Optional[Dict] = None,
            input_sample_rate: int = 22050,
            output_sample_rate: int = 24000,
            output_hop_length: int = 256,

            # Model architecture
            decoder_input_dim: int = 1024,
            d_vector_dim: int = 512,
            cond_d_vector_in_each_upsampling_layer: bool = True,

            # Training settings
            gpt_code_stride_len: int = 1024,
            duration_const: int = 102400,

            # Tokenizer settings
            tokenizer_file: str = "",
            num_chars: int = 255,

            # Language support
            languages: Optional[List[str]] = None,

            # GPT configuration
            gpt_config: Optional[Dict] = None,
            architectures=["Xtts"],
            auto_map = {
                       "AutoConfig": "AstraMindAI/xtts2--xtts2_config.XTTSConfig",
                       "AutoModelForCausalLM": "AstraMindAI/xtts2--xtts2_modeling.Xtts",
                   },
            **kwargs
    ):
        super().__init__(**kwargs)
        self.architectures = architectures
        self.auto_map = auto_map
        # Initialize audio config
        self.audio_config = XTTSAudioConfig(
            **audio_config if audio_config is not None else {}
        )

        self.input_sample_rate = input_sample_rate
        self.output_sample_rate = output_sample_rate
        self.output_hop_length = output_hop_length

        self.decoder_input_dim = decoder_input_dim
        self.d_vector_dim = d_vector_dim
        self.cond_d_vector_in_each_upsampling_layer = cond_d_vector_in_each_upsampling_layer

        self.gpt_code_stride_len = gpt_code_stride_len
        self.duration_const = duration_const

        self.tokenizer_file = tokenizer_file
        self.num_chars = num_chars

        # Initialize GPT config
        self.gpt = XTTSGPTConfig(**gpt_config if gpt_config is not None else {})

        if languages is None:
            self.languages = [
                "en", "es", "fr", "de", "it", "pt", "pl", "tr", "ru",
                "nl", "cs", "ar", "zh-cn", "hu", "ko", "ja", "hi"
            ]
        else:
            self.languages = languages

    def to_dict(self) -> Dict:
        """Convert configuration to dictionary format.

        Returns:
            Dict: Configuration dictionary including audio and GPT configs.
        """
        output = super().to_dict()
        output["audio_config"] = asdict(self.audio_config)
        output["gpt_config"] = self.gpt.to_dict()
        return output

    @classmethod
    def from_dict(cls, config_dict: Dict, *args, **kwargs) -> "XTTSConfig":
        """Create configuration from dictionary.

        Args:
            config_dict (Dict): Configuration dictionary.
            *args: Additional positional arguments.
            **kwargs: Additional keyword arguments.

        Returns:
            XTTSConfig: Configuration instance.
        """
        if "gpt_config" in config_dict:
            gpt_config = config_dict["gpt_config"]
            config_dict = {k: v for k, v in config_dict.items() if k != "gpt_config"}
            return cls(gpt_config=gpt_config, **config_dict)
        return cls(**config_dict)

================================================================================
# File: auralis/models/xttsv2/config/xttsv2_gpt_config.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/xttsv2/config/xttsv2_gpt_config.py
================================================================================

from dataclasses import asdict, dataclass
from typing import Dict, Optional, List
from transformers.configuration_utils import PretrainedConfig
from transformers.utils import logging

logger = logging.get_logger(__name__)


@dataclass
class GPTAudioConfig:
    """Configuration for GPT audio processing parameters.
    
    This class defines the basic audio processing parameters used by the GPT
    component of the XTTS model, focusing on mel-spectrogram generation and
    sample rate conversion.

    Attributes:
        mel_channels (int): Number of mel-spectrogram channels. Defaults to 80.
        sample_rate (int): Input audio sampling rate in Hz. Defaults to 22050.
        output_sample_rate (int): Output audio sampling rate in Hz. Defaults to 24000.
    """
    mel_channels: int = 80
    sample_rate: int = 22050
    output_sample_rate: int = 24000

@dataclass
class XTTSAudioConfig:
    """Configuration for XTTS audio processing parameters.
    
    This class defines the complete set of audio processing parameters used
    throughout the XTTS model, including mel-spectrogram generation, STFT
    parameters, and normalization settings.

    Attributes:
        sample_rate (int): Input audio sampling rate in Hz. Defaults to 22050.
        output_sample_rate (int): Output audio sampling rate in Hz. Defaults to 24000.
        mel_channels (int): Number of mel-spectrogram channels. Defaults to 80.
        hop_length (int): Number of samples between STFT columns. Defaults to 256.
        win_length (int): Window size for STFT. Defaults to 1024.
        n_fft (int): FFT size. Defaults to 1024.
        fmin (int): Minimum frequency for mel scale in Hz. Defaults to 0.
        fmax (int): Maximum frequency for mel scale in Hz. Defaults to 8000.
        power (float): Power of the magnitude spectrogram. Defaults to 1.0.
        mel_norms_file (Optional[str]): Path to mel-spectrogram normalization file.
    """
    sample_rate: int = 22050
    output_sample_rate: int = 24000
    mel_channels: int = 80
    hop_length: int = 256
    win_length: int = 1024
    n_fft: int = 1024
    fmin: int = 0
    fmax: int = 8000
    power: float = 1.0
    mel_norms_file: Optional[str] = None


class XTTSGPTConfig(PretrainedConfig):
    """Configuration for the GPT component of XTTS.
    
    This class defines the architecture and behavior of the GPT model used in XTTS.
    It inherits from HuggingFace's PretrainedConfig for compatibility with the
    transformers library. The GPT model is responsible for generating audio tokens
    from text tokens, with support for various conditioning signals and
    architectural features.

    !!! note "Model Architecture"
        The model uses a transformer-based architecture with configurable layers,
        heads, and dimensions. Key components include:
        
        - Transformer layers with self-attention
        - Feed-forward networks
        - Optional perceiver resampler
        - KV-cache for efficient inference

    !!! example "Usage"
        ```python
        config = XTTSGPTConfig(
            hidden_size=1024,
            num_hidden_layers=30,
            num_attention_heads=16
        )
        model = XttsGPT(config)
        ```

    See Also:
        - [`XTTSConfig`][auralis.models.xttsv2.config.xttsv2_config.XTTSConfig]: Main XTTS configuration
        - [`XttsGPT`][auralis.models.xttsv2.XTTSv2]: GPT model implementation

    Attributes:
        Model Architecture:
            hidden_size (int): Size of hidden layers. Defaults to 1024.
            n_inner (int): Size of feed-forward inner layer. Defaults to 4096.
            num_hidden_layers (int): Number of transformer layers. Defaults to 30.
            num_attention_heads (int): Number of attention heads. Defaults to 16.

        Tokenizer Settings:
            vocab_size (int): Size of text vocabulary. Defaults to 6681.
            number_text_tokens (int): Explicit text token vocabulary size.
            start_text_token (Optional[int]): Token ID for text start.
            stop_text_token (Optional[int]): Token ID for text end.

        Audio Token Settings:
            num_audio_tokens (int): Size of audio token vocabulary. Defaults to 1026.
            start_audio_token (int): Token ID for audio start. Defaults to 1024.
            stop_audio_token (int): Token ID for audio end. Defaults to 1025.

        Sequence Length Settings:
            max_audio_tokens (int): Maximum audio sequence length. Defaults to 605.
            max_text_tokens (int): Maximum text sequence length. Defaults to 402.
            max_prompt_tokens (int): Maximum prompt sequence length. Defaults to 70.

        Model Behavior Settings:
            use_masking_gt_prompt_approach (bool): Whether to use masking. Defaults to True.
            use_perceiver_resampler (bool): Whether to use perceiver. Defaults to True.
            kv_cache (bool): Whether to use KV cache. Defaults to True.
            enable_redaction (bool): Whether to enable redaction. Defaults to False.

        Architecture Specifics:
            layer_norm_epsilon (float): Layer normalization epsilon. Defaults to 1e-5.
            initializer_range (float): Weight initialization range. Defaults to 0.02.
            add_cross_attention (bool): Whether to add cross attention. Defaults to False.
            scale_attn_by_inverse_layer_idx (bool): Whether to scale attention.
            reorder_and_upcast_attn (bool): Whether to reorder attention.

        Other Settings:
            decoder_input_dim (int): Decoder input dimension. Defaults to 1024.
            activation_function (str): Activation function. Defaults to "gelu".
            attn_pdrop (float): Attention dropout probability. Defaults to 0.1.
    """
    model_type = "xtts_gpt"

    def __init__(
            self,
            # Model architecture
            hidden_size: int = 1024,  # gpt_n_model_channels in original
            n_inner: int = 4096,
            num_hidden_layers: int = 30,  # gpt_layers in original
            num_attention_heads: int = 16,  # gpt_n_heads in original

            # Tokenizer settings
            vocab_size: int = 6681,  # gpt_number_text_tokens in original
            number_text_tokens: int = 6681,  # Explicit text token vocabulary size
            start_text_token: Optional[int] = None,
            stop_text_token: Optional[int] = None,

            # Audio token settings
            num_audio_tokens: int = 1026,  # gpt_num_audio_tokens in original
            start_audio_token: int = 1024,  # gpt_start_audio_token in original
            stop_audio_token: int = 1025,  # gpt_stop_audio_token in original

            # Sequence length settings
            max_audio_tokens: int = 605,  # gpt_max_audio_tokens in original
            max_text_tokens: int = 402,  # gpt_max_text_tokens in original
            max_prompt_tokens: int = 70,  # gpt_max_prompt_tokens in original
            gpt_max_audio_tokens: int = 605,  # Used for generation

            # Model behavior settings
            use_masking_gt_prompt_approach: bool = True,  # gpt_use_masking_gt_prompt_approach in original
            use_perceiver_resampler: bool = True,  # gpt_use_perceiver_resampler in original
            kv_cache: bool = True,
            enable_redaction: bool = False,

            # GPT batch settings
            gpt_batch_size: int = 1,

            # Audio processing
            audio_config: Optional[Dict] = None,

            # Architecture specifics
            layer_norm_epsilon: float = 1e-5,
            initializer_range: float = 0.02,
            add_cross_attention: bool = False,
            scale_attn_by_inverse_layer_idx: bool = False,
            reorder_and_upcast_attn: bool = False,

            # Size settings for the decoder
            decoder_input_dim: int = 1024,
            architectures=["XttsGPT"],
            auto_map={
                "AutoConfig": "AstraMindAI/xtts2-gpt--gpt_config.XTTSGPTConfig",
                "AutoModelForCausalLM": "AstraMindAI/xtts2-gpt--xtts2_gpt_modeling.XttsGPT",
            },
            activation_function: str = "gelu",
            attn_pdrop: float = 0.1,
            **kwargs
    ):
        super().__init__(**kwargs)
        self.architectures = architectures
        self.auto_map = auto_map
        self.audio_config = GPTAudioConfig(
            **audio_config if audio_config is not None else {}
        )
        self.activation_function = activation_function
        self.attn_pdrop = attn_pdrop
        self.hidden_size = hidden_size
        self.n_inner = n_inner
        self.num_hidden_layers = num_hidden_layers
        self.num_attention_heads = num_attention_heads

        self.vocab_size = vocab_size
        self.number_text_tokens = number_text_tokens
        self.start_text_token = start_text_token
        self.stop_text_token = stop_text_token

        self.num_audio_tokens = num_audio_tokens
        self.start_audio_token = start_audio_token
        self.stop_audio_token = stop_audio_token

        self.max_audio_tokens = max_audio_tokens
        self.max_text_tokens = max_text_tokens
        self.max_prompt_tokens = max_prompt_tokens
        self.gpt_max_audio_tokens = gpt_max_audio_tokens

        self.use_masking_gt_prompt_approach = use_masking_gt_prompt_approach
        self.use_perceiver_resampler = use_perceiver_resampler
        self.kv_cache = kv_cache
        self.enable_redaction = enable_redaction

        self.gpt_batch_size = gpt_batch_size

        self.layer_norm_epsilon = layer_norm_epsilon
        self.initializer_range = initializer_range
        self.add_cross_attention = add_cross_attention
        self.scale_attn_by_inverse_layer_idx = scale_attn_by_inverse_layer_idx
        self.reorder_and_upcast_attn = reorder_and_upcast_attn

        self.decoder_input_dim = decoder_input_dim

    def to_dict(self) -> Dict:
        """Convert configuration to dictionary format.

        This method converts the configuration object to a dictionary format,
        including the audio configuration settings.

        Returns:
            Dict: Configuration dictionary including audio config.
        """
        output = super().to_dict()
        output["audio_config"] = asdict(self.audio_config)
        return output

    @classmethod
    def from_dict(cls, config_dict: Dict, *args, **kwargs) -> "XTTSGPTConfig":
        """Create configuration from dictionary.

        This class method creates a new configuration instance from a dictionary
        of parameters.

        Args:
            config_dict (Dict): Configuration dictionary.
            *args: Additional positional arguments.
            **kwargs: Additional keyword arguments.

        Returns:
            XTTSGPTConfig: Configuration instance.
        """
        return cls(**config_dict)




================================================================================
# File: auralis/models/xttsv2/config/tokenizer.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/xttsv2/config/tokenizer.py
================================================================================

import re
from typing import List, Optional, Union, Dict, Any
from functools import cached_property

import pypinyin
import torch
from hangul_romanize import Transliter
from hangul_romanize.rule import academic
from num2words import num2words
from spacy.lang.ar import Arabic
from spacy.lang.en import English
from spacy.lang.es import Spanish
from spacy.lang.ja import Japanese
from spacy.lang.zh import Chinese
from transformers import PreTrainedTokenizerFast, BatchEncoding
from transformers.tokenization_utils_base import TruncationStrategy, PaddingStrategy
from tokenizers import Tokenizer
from tokenizers.pre_tokenizers import WhitespaceSplit
from tokenizers.processors import TemplateProcessing

from auralis.models.xttsv2.components.tts.layers.xtts.zh_num2words import TextNorm as zh_num2words

import cutlet

def get_spacy_lang(lang):
    """Get spaCy language model for text processing.
    
    This function returns the appropriate spaCy language model based on the
    input language code. For languages without specific models, it defaults
    to English which provides basic tokenization capabilities.

    Args:
        lang (str): Language code (e.g., 'zh', 'ja', 'ar', 'es').

    Returns:
        spacy.Language: Initialized spaCy language model.
    """
    if lang == "zh":
        return Chinese()
    elif lang == "ja":
        return Japanese()
    elif lang == "ar":
        return Arabic()
    elif lang == "es":
        return Spanish()
    else:
        # For most languages, English does the job
        return English()


def find_best_split_point(text: str, target_pos: int, window_size: int = 30) -> int:
    """Find the optimal point to split text near a target position.
    
    This function analyzes text around the target position to find the most
    natural break point, considering punctuation marks, whitespace, and other
    language markers. It uses a scoring system that prioritizes different types
    of breaks and their distance from the target position.

    Args:
        text (str): Input text to analyze.
        target_pos (int): Target position around which to find split point.
        window_size (int, optional): Size of text window to analyze. Defaults to 30.

    Returns:
        int: Position of the best split point.

    Notes:
        The function uses a prioritized list of markers:
        1. Strong breaks (periods, exclamation marks, question marks)
        2. Medium breaks (commas, closing brackets)
        3. Weak breaks (spaces, special characters)
        
        Each marker type has a priority score, and the final position is chosen
        based on both the marker priority and proximity to target position.
    """
    # Define split markers by priority
    markers = [
        # Strong breaks (longest pause)
        (r'[.!?؟။။။]+[\s]*', 1.0),  # Periods, exclamation, question (multi-script)
        (r'[\n\r]+\s*[\n\r]+', 1.0),  # Multiple newlines
        (r'[:|;；：；][\s]*', 0.9),  # Colons, semicolons (multi-script)

        # Medium breaks
        (r'[,，،、][\s]*', 0.8),  # Commas (multi-script)
        (r'[)}\]）】』»›》\s]+', 0.7),  # Closing brackets/parentheses
        (r'[-—−]+[\s]*', 0.7),  # Dashes

        # Weak breaks
        (r'\s+[&+=/\s]+\s+', 0.6),  # Special characters with spaces
        (r'[\s]+', 0.5),  # Any whitespace as last resort
    ]

    # Calculate window boundaries
    start = max(0, target_pos - window_size)
    end = min(len(text), target_pos + window_size)
    window = text[start:end]

    best_pos = target_pos
    best_score = 0

    for pattern, priority in markers:
        matches = list(re.finditer(pattern, window))
        for match in matches:
            # Calculate position score based on distance from target
            pos = start + match.end()
            distance = abs(pos - target_pos)
            distance_score = 1 - (distance / (window_size * 2))

            # Combine priority and position scores
            score = priority * distance_score

            if score > best_score:
                best_score = score
                best_pos = pos

    return best_pos


def split_sentence(text: str, lang: str, text_split_length: int = 250) -> List[str]:
    """Split text into natural sentences optimized for TTS.
    
    This function performs intelligent text splitting that considers language
    structure, sentence boundaries, and optimal split points. It uses spaCy
    for initial sentence detection and handles special cases like long
    sentences that exceed the target length.

    !!! note "Language Support"
        The function supports multiple languages through spaCy models:
        
        - English (default fallback)
        - Chinese (zh)
        - Japanese (ja)
        - Arabic (ar)
        - Spanish (es)

    !!! tip "Split Point Selection"
        Split points are chosen based on priority:

        1. Strong breaks (1.0):
           - Periods, exclamation marks, question marks
           - Multiple newlines
           - Colons, semicolons
        
        2. Medium breaks (0.8-0.7):
           - Commas
           - Closing brackets/parentheses
           - Dashes
        
        3. Weak breaks (0.6-0.5):
           - Special characters with spaces
           - Any whitespace

    !!! example "Usage Example"
        ```python
        text = "This is a long sentence. It needs to be split. For TTS processing."
        splits = split_sentence(text, "en", text_split_length=50)
        # Result: ["This is a long sentence.", "It needs to be split.", "For TTS processing."]
        ```

    Args:
        text (str): Input text to split.
        lang (str): Language code for text processing.
        text_split_length (int, optional): Target length for text splits.
            Defaults to 250.

    Returns:
        List[str]: List of text splits optimized for TTS processing.

    See Also:
        - [`find_best_split_point`][auralis.models.xttsv2.config.tokenizer.find_best_split_point]: Split point finder
        - [`get_spacy_lang`][auralis.models.xttsv2.config.tokenizer.get_spacy_lang]: Language model selector
    """
    text = text.strip()
    if len(text) <= text_split_length:
        return [text]

    nlp = get_spacy_lang(lang)
    if "sentencizer" not in nlp.pipe_names:
        nlp.add_pipe("sentencizer")

    # Get base sentences using spaCy
    doc = nlp(text)
    sentences = list(doc.sents)

    splits = []
    current_split = []
    current_length = 0

    for sent in sentences:
        sentence_text = str(sent).strip()
        sentence_length = len(sentence_text)

        # If sentence fits in current split
        if current_length + sentence_length <= text_split_length:
            current_split.append(sentence_text)
            current_length += sentence_length + 1

        # Handle long sentences
        elif sentence_length > text_split_length:
            # Add current split if exists
            if current_split:
                splits.append(" ".join(current_split))
                current_split = []
                current_length = 0

            # Split long sentence at optimal points
            remaining = sentence_text
            while len(remaining) > text_split_length:
                split_pos = find_best_split_point(
                    remaining,
                    text_split_length,
                    window_size=30
                )

                # Add split and continue with remainder
                splits.append(remaining[:split_pos].strip())
                remaining = remaining[split_pos:].strip()

            # Handle remaining text
            if remaining:
                current_split = [remaining]
                current_length = len(remaining)

        # Start new split
        else:
            splits.append(" ".join(current_split))
            current_split = [sentence_text]
            current_length = sentence_length

    # Add final split if needed
    if current_split:
        splits.append(" ".join(current_split))

    cleaned_sentences = [s[:-1]+' ' if s.endswith('.') else s for s in splits if s] # prevents annoying sounds in italian
    # Clean up splits
    return cleaned_sentences

_whitespace_re = re.compile(r"\s+")

# List of (regular expression, replacement) pairs for abbreviations:
_abbreviations = {
    "en": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            ("mrs", "misess"),
            ("mr", "mister"),
            ("dr", "doctor"),
            ("st", "saint"),
            ("co", "company"),
            ("jr", "junior"),
            ("maj", "major"),
            ("gen", "general"),
            ("drs", "doctors"),
            ("rev", "reverend"),
            ("lt", "lieutenant"),
            ("hon", "honorable"),
            ("sgt", "sergeant"),
            ("capt", "captain"),
            ("esq", "esquire"),
            ("ltd", "limited"),
            ("col", "colonel"),
            ("ft", "fort"),
        ]
    ],
    "es": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            ("sra", "señora"),
            ("sr", "señor"),
            ("dr", "doctor"),
            ("dra", "doctora"),
            ("st", "santo"),
            ("co", "compañía"),
            ("jr", "junior"),
            ("ltd", "limitada"),
        ]
    ],
    "fr": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            ("mme", "madame"),
            ("mr", "monsieur"),
            ("dr", "docteur"),
            ("st", "saint"),
            ("co", "compagnie"),
            ("jr", "junior"),
            ("ltd", "limitée"),
        ]
    ],
    "de": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            ("fr", "frau"),
            ("dr", "doktor"),
            ("st", "sankt"),
            ("co", "firma"),
            ("jr", "junior"),
        ]
    ],
    "pt": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            ("sra", "senhora"),
            ("sr", "senhor"),
            ("dr", "doutor"),
            ("dra", "doutora"),
            ("st", "santo"),
            ("co", "companhia"),
            ("jr", "júnior"),
            ("ltd", "limitada"),
        ]
    ],
    "it": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            # ("sig.ra", "signora"),
            ("sig", "signore"),
            ("dr", "dottore"),
            ("st", "santo"),
            ("co", "compagnia"),
            ("jr", "junior"),
            ("ltd", "limitata"),
        ]
    ],
    "pl": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            ("p", "pani"),
            ("m", "pan"),
            ("dr", "doktor"),
            ("sw", "święty"),
            ("jr", "junior"),
        ]
    ],
    "ar": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            # There are not many common abbreviations in Arabic as in English.
        ]
    ],
    "zh": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            # Chinese doesn't typically use abbreviations in the same way as Latin-based scripts.
        ]
    ],
    "cs": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            ("dr", "doktor"),  # doctor
            ("ing", "inženýr"),  # engineer
            ("p", "pan"),  # Could also map to pani for woman but no easy way to do it
            # Other abbreviations would be specialized and not as common.
        ]
    ],
    "ru": [
        (re.compile("\\b%s\\b" % x[0], re.IGNORECASE), x[1])
        for x in [
            ("г-жа", "госпожа"),  # Mrs.
            ("г-н", "господин"),  # Mr.
            ("д-р", "доктор"),  # doctor
            # Other abbreviations are less common or specialized.
        ]
    ],
    "nl": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            ("dhr", "de heer"),  # Mr.
            ("mevr", "mevrouw"),  # Mrs.
            ("dr", "dokter"),  # doctor
            ("jhr", "jonkheer"),  # young lord or nobleman
            # Dutch uses more abbreviations, but these are the most common ones.
        ]
    ],
    "tr": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            ("b", "bay"),  # Mr.
            ("byk", "büyük"),  # büyük
            ("dr", "doktor"),  # doctor
            # Add other Turkish abbreviations here if needed.
        ]
    ],
    "hu": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            ("dr", "doktor"),  # doctor
            ("b", "bácsi"),  # Mr.
            ("nőv", "nővér"),  # nurse
            # Add other Hungarian abbreviations here if needed.
        ]
    ],
    "ko": [
        (re.compile("\\b%s\\." % x[0], re.IGNORECASE), x[1])
        for x in [
            # Korean doesn't typically use abbreviations in the same way as Latin-based scripts.
        ]
    ],
}

def expand_abbreviations_multilingual(text, lang="en"):
    if lang in _abbreviations:
        for regex, replacement in _abbreviations[lang]:
            text = re.sub(regex, replacement, text)
    return text

_symbols_multilingual = {
    "en": [
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " and "),
            ("@", " at "),
            ("%", " percent "),
            ("#", " hash "),
            ("$", " dollar "),
            ("£", " pound "),
            ("°", " degree "),
        ]
    ],
    "es": [
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " y "),
            ("@", " arroba "),
            ("%", " por ciento "),
            ("#", " numeral "),
            ("$", " dolar "),
            ("£", " libra "),
            ("°", " grados "),
        ]
    ],
    "fr": [
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " et "),
            ("@", " arobase "),
            ("%", " pour cent "),
            ("#", " dièse "),
            ("$", " dollar "),
            ("£", " livre "),
            ("°", " degrés "),
        ]
    ],
    "de": [
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " und "),
            ("@", " at "),
            ("%", " prozent "),
            ("#", " raute "),
            ("$", " dollar "),
            ("£", " pfund "),
            ("°", " grad "),
        ]
    ],
    "pt": [
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " e "),
            ("@", " arroba "),
            ("%", " por cento "),
            ("#", " cardinal "),
            ("$", " dólar "),
            ("£", " libra "),
            ("°", " graus "),
        ]
    ],
    "it": [
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " e "),
            ("@", " chiocciola "),
            ("%", " per cento "),
            ("#", " cancelletto "),
            ("$", " dollaro "),
            ("£", " sterlina "),
            ("°", " gradi "),
        ]
    ],
    "pl": [
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " i "),
            ("@", " małpa "),
            ("%", " procent "),
            ("#", " krzyżyk "),
            ("$", " dolar "),
            ("£", " funt "),
            ("°", " stopnie "),
        ]
    ],
    "ar": [
        # Arabic
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " و "),
            ("@", " على "),
            ("%", " في المئة "),
            ("#", " رقم "),
            ("$", " دولار "),
            ("£", " جنيه "),
            ("°", " درجة "),
        ]
    ],
    "zh": [
        # Chinese
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " 和 "),
            ("@", " 在 "),
            ("%", " 百分之 "),
            ("#", " 号 "),
            ("$", " 美元 "),
            ("£", " 英镑 "),
            ("°", " 度 "),
        ]
    ],
    "cs": [
        # Czech
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " a "),
            ("@", " na "),
            ("%", " procento "),
            ("#", " křížek "),
            ("$", " dolar "),
            ("£", " libra "),
            ("°", " stupně "),
        ]
    ],
    "ru": [
        # Russian
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " и "),
            ("@", " собака "),
            ("%", " процентов "),
            ("#", " номер "),
            ("$", " доллар "),
            ("£", " фунт "),
            ("°", " градус "),
        ]
    ],
    "nl": [
        # Dutch
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " en "),
            ("@", " bij "),
            ("%", " procent "),
            ("#", " hekje "),
            ("$", " dollar "),
            ("£", " pond "),
            ("°", " graden "),
        ]
    ],
    "tr": [
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " ve "),
            ("@", " at "),
            ("%", " yüzde "),
            ("#", " diyez "),
            ("$", " dolar "),
            ("£", " sterlin "),
            ("°", " derece "),
        ]
    ],
    "hu": [
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " és "),
            ("@", " kukac "),
            ("%", " százalék "),
            ("#", " kettőskereszt "),
            ("$", " dollár "),
            ("£", " font "),
            ("°", " fok "),
        ]
    ],
    "ko": [
        # Korean
        (re.compile(r"%s" % re.escape(x[0]), re.IGNORECASE), x[1])
        for x in [
            ("&", " 그리고 "),
            ("@", " 에 "),
            ("%", " 퍼센트 "),
            ("#", " 번호 "),
            ("$", " 달러 "),
            ("£", " 파운드 "),
            ("°", " 도 "),
        ]
    ],
}

def expand_symbols_multilingual(text, lang="en"):
    if lang in _symbols_multilingual:
        for regex, replacement in _symbols_multilingual[lang]:
            text = re.sub(regex, replacement, text)
            text = text.replace("  ", " ")  # Ensure there are no double spaces
    return text.strip()

_ordinal_re = {
    "en": re.compile(r"([0-9]+)(st|nd|rd|th)"),
    "es": re.compile(r"([0-9]+)(º|ª|er|o|a|os|as)"),
    "fr": re.compile(r"([0-9]+)(º|ª|er|re|e|ème)"),
    "de": re.compile(r"([0-9]+)(st|nd|rd|th|º|ª|\.(?=\s|$))"),
    "pt": re.compile(r"([0-9]+)(º|ª|o|a|os|as)"),
    "it": re.compile(r"([0-9]+)(º|°|ª|o|a|i|e)"),
    "pl": re.compile(r"([0-9]+)(º|ª|st|nd|rd|th)"),
    "ar": re.compile(r"([0-9]+)(ون|ين|ث|ر|ى)"),
    "cs": re.compile(r"([0-9]+)\.(?=\s|$)"),  # In Czech, a dot is often used after the number to indicate ordinals.
    "ru": re.compile(r"([0-9]+)(-й|-я|-е|-ое|-ье|-го)"),
    "nl": re.compile(r"([0-9]+)(de|ste|e)"),
    "tr": re.compile(r"([0-9]+)(\.|inci|nci|uncu|üncü|\.)"),
    "hu": re.compile(r"([0-9]+)(\.|adik|edik|odik|edik|ödik|ödike|ik)"),
    "ko": re.compile(r"([0-9]+)(번째|번|차|째)"),
}
_number_re = re.compile(r"[0-9]+")
# noinspection Annotator
_currency_re = {
    "USD": re.compile(r"((\$[0-9\.\,]*[0-9]+)|([0-9\.\,]*[0-9]+\$))"),
    "GBP": re.compile(r"((£[0-9\.\,]*[0-9]+)|([0-9\.\,]*[0-9]+£))"),
    "EUR": re.compile(r"(([0-9\.\,]*[0-9]+€)|((€[0-9\.\,]*[0-9]+)))"),
}

_comma_number_re = re.compile(r"\b\d{1,3}(,\d{3})*(\.\d+)?\b")
_dot_number_re = re.compile(r"\b\d{1,3}(\.\d{3})*(\,\d+)?\b")
_decimal_number_re = re.compile(r"([0-9]+[.,][0-9]+)")

def _remove_commas(m):
    text = m.group(0)
    if "," in text:
        text = text.replace(",", "")
    return text

def _remove_dots(m):
    text = m.group(0)
    if "." in text:
        text = text.replace(".", "")
    return text

def _expand_decimal_point(m, lang="en"):
    amount = m.group(1).replace(",", ".")
    return num2words(float(amount), lang=lang if lang != "cs" else "cz")

def _expand_currency(m, lang="en", currency="USD"):
    amount = float((re.sub(r"[^\d.]", "", m.group(0).replace(",", "."))))
    full_amount = num2words(amount, to="currency", currency=currency, lang=lang if lang != "cs" else "cz")

    and_equivalents = {
        "en": ", ",
        "es": " con ",
        "fr": " et ",
        "de": " und ",
        "pt": " e ",
        "it": " e ",
        "pl": ", ",
        "cs": ", ",
        "ru": ", ",
        "nl": ", ",
        "ar": ", ",
        "tr": ", ",
        "hu": ", ",
        "ko": ", ",
    }

    if amount.is_integer():
        last_and = full_amount.rfind(and_equivalents.get(lang, ", "))
        if last_and != -1:
            full_amount = full_amount[:last_and]

    return full_amount

def _expand_ordinal(m, lang="en"):
    return num2words(int(m.group(1)), ordinal=True, lang=lang if lang != "cs" else "cz")

def _expand_number(m, lang="en"):
    return num2words(int(m.group(0)), lang=lang if lang != "cs" else "cz")

def expand_numbers_multilingual(text, lang="en"):
    if lang == "zh":
        text = zh_num2words()(text)
    else:
        if lang in ["en", "ru"]:
            text = re.sub(_comma_number_re, _remove_commas, text)
        else:
            text = re.sub(_dot_number_re, _remove_dots, text)
        try:
            text = re.sub(_currency_re["GBP"], lambda m: _expand_currency(m, lang, "GBP"), text)
            text = re.sub(_currency_re["USD"], lambda m: _expand_currency(m, lang, "USD"), text)
            text = re.sub(_currency_re["EUR"], lambda m: _expand_currency(m, lang, "EUR"), text)
        except Exception as e:
            pass
        if lang != "tr":
            text = re.sub(_decimal_number_re, lambda m: _expand_decimal_point(m, lang), text)
        if lang in _ordinal_re:
            text = re.sub(_ordinal_re[lang], lambda m: _expand_ordinal(m, lang), text)
        text = re.sub(_number_re, lambda m: _expand_number(m, lang), text)
    return text

def lowercase(text):
    return text.lower()

def collapse_whitespace(text):
    return re.sub(_whitespace_re, " ", text)

def multilingual_cleaners(text, lang):
    text = text.replace('"', "")
    if lang == "tr":
        text = text.replace("İ", "i")
        text = text.replace("Ö", "ö")
        text = text.replace("Ü", "ü")
    text = lowercase(text)
    text = expand_numbers_multilingual(text, lang)
    text = expand_abbreviations_multilingual(text, lang)
    text = expand_symbols_multilingual(text, lang=lang)
    text = collapse_whitespace(text)
    return text

def basic_cleaners(text):
    """Basic pipeline that lowercases and collapses whitespace without transliteration."""
    text = lowercase(text)
    text = collapse_whitespace(text)
    return text

def chinese_transliterate(text):
    return "".join(
        [p[0] for p in pypinyin.pinyin(text, style=pypinyin.Style.TONE3, heteronym=False, neutral_tone_with_five=True)]
    )

def japanese_cleaners(text, katsu):
    text = katsu.romaji(text)
    text = lowercase(text)
    return text

def korean_transliterate(text, transliter):
    return transliter.translit(text)

# Fast Tokenizer Class

class XTTSTokenizerFast(PreTrainedTokenizerFast):
    """
    Fast Tokenizer implementation for XTTS model using HuggingFace's PreTrainedTokenizerFast
    """

    def __init__(
            self,
            vocab_file: str = None,
            tokenizer_object: Optional[Tokenizer] = None,
            unk_token: str = "[UNK]",
            pad_token: str = "[PAD]",
            bos_token: str = "[START]",
            eos_token: str = "[STOP]",
            auto_map: dict = {"AutoTokenizer": ["AstraMindAI/xtts2-gpt--tokenizer.XTTSTokenizerFast", None]},
            clean_up_tokenization_spaces: bool = True,
            **kwargs
    ):
        if tokenizer_object is None and vocab_file is not None:
            tokenizer_object = Tokenizer.from_file(vocab_file)

        if tokenizer_object is not None:
            # Configure the tokenizer
            tokenizer_object.pre_tokenizer = WhitespaceSplit()
            tokenizer_object.post_processor = TemplateProcessing(
                single=f"{bos_token} $A {eos_token}",
                special_tokens=[
                    (bos_token, tokenizer_object.token_to_id(bos_token)),
                    (eos_token, tokenizer_object.token_to_id(eos_token)),
                ],
            )

        super().__init__(
            tokenizer_object=tokenizer_object,
            unk_token=unk_token,
            pad_token=pad_token,
            bos_token=bos_token,
            eos_token=eos_token,
            clean_up_tokenization_spaces=clean_up_tokenization_spaces,
            **kwargs
        )

        # Character limits per language
        self.char_limits = {
            "en": 250, "de": 253, "fr": 273, "es": 239,
            "it": 213, "pt": 203, "pl": 224, "zh": 82,
            "ar": 166, "cs": 186, "ru": 182, "nl": 251,
            "tr": 226, "ja": 71, "hu": 224, "ko": 95,
        }

        # Initialize language tools
        self._katsu = None
        self._korean_transliter = Transliter(academic)

        # Ensure pad_token_id is set
        if self.pad_token_id is None:
            self.pad_token_id = self.tokenizer.token_to_id(self.pad_token)

    @cached_property
    def katsu(self):
        if self._katsu is None:
            self._katsu = cutlet.Cutlet()
        return self._katsu

    def preprocess_text(self, text: str, lang: str) -> str:
        """Apply text preprocessing for language"""
        base_lang = lang.split("-")[0]  # remove region
        if base_lang in {"ar", "cs", "de", "en", "es", "fr", "hu", "it",
                         "nl", "pl", "pt", "ru", "tr", "zh", "ko"}:
            text = multilingual_cleaners(text, base_lang)
            if base_lang == "zh":
                text = chinese_transliterate(text)
            if base_lang == "ko":
                text = korean_transliterate(text, self._korean_transliter)
        elif base_lang == "ja":
            text = japanese_cleaners(text, self.katsu)
        else:
            text = basic_cleaners(text)
        return text

    def batch_encode_with_split(self, texts: Union[str, List[str]], lang: Union[str, List[str]],
                                **kwargs) -> torch.Tensor:
        """
        Split texts into smaller chunks based on language character limits and encode them using HuggingFace fast tokenizer.
        strictly mimic the xttsv2 tokenizer
        """
        # Convert single inputs to lists
        if isinstance(texts, str):
            texts = [texts]
        if isinstance(lang, str):
            lang = [lang]
        # Ensure lang list matches texts list
        if len(lang) == 1 and len(texts) > 1:
            lang = lang * len(texts)

        # Check if texts and lang have the same length
        if len(texts) != len(lang):
            raise ValueError(f"Number of texts ({len(texts)}) does not match number of languages ({len(lang)}).")

        chunk_list = []
        max_splits = 0

        # For each text, split into chunks based on character limit
        for text, text_lang in zip(texts, lang):
            # Get language character limit
            base_lang = text_lang.split("-")[0]
            char_limit = self.char_limits.get(base_lang, 250)

            # Clean and preprocess
            #text = self.preprocess_text(text, text_lang) we do this in the hidden function

            # Split text into sentences/chunks based on language
            chunk_list = split_sentence(text, base_lang, text_split_length=char_limit)

        # Ensure the tokenizer is a fast tokenizer
        if not self.is_fast:
            raise ValueError("The tokenizer must be a fast tokenizer.")

        # Encode all chunks using the fast tokenizer
        encoding: BatchEncoding = self(
            chunk_list,
            lang = lang,
            add_special_tokens=False,
            padding=False,
            **kwargs
        )

        # The 'input_ids' tensor will have shape [total_chunks, max_sequence_length]
        return encoding['input_ids']  # Tensor of shape [total_chunks, sequence_length]

    def _batch_encode_plus(
            self,
            batch_text_or_text_pairs,
            add_special_tokens: bool = True,
            padding_strategy=PaddingStrategy.DO_NOT_PAD,
            truncation_strategy=TruncationStrategy.DO_NOT_TRUNCATE,
            max_length: Optional[int] = None,
            stride: int = 0,
            is_split_into_words: bool = False,
            pad_to_multiple_of: Optional[int] = None,
            return_tensors: Optional[str] = None,
            return_token_type_ids: Optional[bool] = None,
            return_attention_mask: Optional[bool] = None,
            return_overflowing_tokens: bool = False,
            return_special_tokens_mask: bool = False,
            return_offsets_mapping: bool = False,
            return_length: bool = False,
            verbose: bool = True,
            **kwargs
    ) -> Dict[str, Any]:
        """
        Override batch encoding to handle language-specific preprocessing
        """
        lang = kwargs.pop("lang", ["en"] * len(batch_text_or_text_pairs))
        if isinstance(lang, str):
            lang = [lang]
        # Ensure lang list matches texts list
        if len(lang) == 1 and len(batch_text_or_text_pairs) > 1:
            lang = lang * len(batch_text_or_text_pairs)

        # Check if batch_text_or_text_pairs and lang have the same length
        if len(batch_text_or_text_pairs) != len(lang):
            raise ValueError(f"Number of texts ({len(batch_text_or_text_pairs)}) does not match number of languages ({len(lang)}).")

        # Preprocess each text in the batch with its corresponding language
        processed_texts = []
        for text, text_lang in zip(batch_text_or_text_pairs, lang):
            if isinstance(text, str):
                # Check length and preprocess
                #self.check_input_length(text, text_lang)
                processed_text = self.preprocess_text(text, text_lang)

                # Format text with language tag and spaces
                base_lang = text_lang.split("-")[0]
                lang_code = "zh-cn" if base_lang == "zh" else base_lang
                processed_text = f"[{lang_code}]{processed_text}"
                processed_text = processed_text.replace(" ", "[SPACE]")

                processed_texts.append(processed_text)
            else:
                processed_texts.append(text)

        # Call the parent class's encoding method with processed texts
        return super()._batch_encode_plus(
            processed_texts,
            add_special_tokens=add_special_tokens,
            padding_strategy=padding_strategy,
            truncation_strategy=truncation_strategy,
            max_length=max_length,
            stride=stride,
            is_split_into_words=is_split_into_words,
            pad_to_multiple_of=pad_to_multiple_of,
            return_tensors=return_tensors,
            return_token_type_ids=return_token_type_ids,
            return_attention_mask=return_attention_mask,
            return_overflowing_tokens=return_overflowing_tokens,
            return_special_tokens_mask=return_special_tokens_mask,
            return_offsets_mapping=return_offsets_mapping,
            return_length=return_length,
            verbose=verbose,
            **kwargs
        )


    def __call__(
            self,
            text: Union[str, List[str]],
            lang: Union[str, List[str]] = "en",
            add_special_tokens: bool = True,
            padding: Union[bool, str, PaddingStrategy] = False,
            truncation: Union[bool, str, TruncationStrategy] = False,
            max_length: Optional[int] = None,
            stride: int = 0,
            return_tensors: Optional[str] = None,
            return_token_type_ids: Optional[bool] = None,
            return_attention_mask: Optional[bool] = True,
            **kwargs
    ):
        """
        Main tokenization method
        """
        # Convert single string to list for batch processing
        if isinstance(text, str):
            text = [text]
        if isinstance(lang, str):
            lang = [lang]
        # Ensure lang list matches texts list
        if len(lang) == 1 and len(text) > 1:
            lang = lang * len(text)

        # Ensure text and lang lists have same length
        if len(text) != len(lang):
            raise ValueError(f"Number of texts ({len(text)}) does not match number of languages ({len(lang)}).")

        # Convert padding strategy
        if isinstance(padding, bool):
            padding_strategy = PaddingStrategy.LONGEST if padding else PaddingStrategy.DO_NOT_PAD
        else:
            padding_strategy = PaddingStrategy(padding)

        # Convert truncation strategy
        if isinstance(truncation, bool):
            truncation_strategy = TruncationStrategy.LONGEST_FIRST if truncation else TruncationStrategy.DO_NOT_TRUNCATE
        else:
            truncation_strategy = TruncationStrategy(truncation)

        # Use the batch encoding method
        encoded = self._batch_encode_plus(
            text,
            add_special_tokens=add_special_tokens,
            padding_strategy=padding_strategy,
            truncation_strategy=truncation_strategy,
            max_length=max_length,
            stride=stride,
            return_tensors=return_tensors,
            return_token_type_ids=return_token_type_ids,
            return_attention_mask=return_attention_mask,
            lang=lang,
            **kwargs
        )

        return encoded


================================================================================
# File: auralis/models/xttsv2/utils/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/xttsv2/utils/__init__.py
================================================================================



================================================================================
# File: auralis/models/xttsv2/utils/checkpoint_converter.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/xttsv2/utils/checkpoint_converter.py
================================================================================

import os
import argparse

import requests
import torch
from safetensors.torch import save_file
from huggingface_hub import snapshot_download


def download_repo_files(repo_id, output_path, exclude_extensions=['*.safetensors']):
    """
    Downloads all files from a GitHub repository except specified extensions.

    Args:
        owner (str): GitHub repository owner
        repo (str): Repository name
        exclude_extensions (list): List of file extensions to exclude
    """
    # Create base directory if it doesn't exist
    if not os.path.exists(output_path):
        os.makedirs(output_path)

    snapshot_download(repo_id=repo_id, ignore_patterns=exclude_extensions, local_dir=output_path)



def convert_checkpoint(pytorch_checkpoint_path, output_dir, args):
    """
    Convert PyTorch checkpoint to SafeTensors format, mapping weights to GPT2 or XTTSv2 models
    based on specific substrings.

    Args:
        pytorch_checkpoint_path: Path to input PyTorch checkpoint
        output_dir: Directory to save the output SafeTensors files
    """
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(os.path.join(output_dir, "gpt"), exist_ok=True)
    os.makedirs(os.path.join(output_dir, "core_xttsv2"), exist_ok=True)

    # Load PyTorch checkpoint
    checkpoint = torch.load(pytorch_checkpoint_path, map_location='cpu', weights_only=False) # to avoid warning

    # Initialize dictionaries for different models
    gpt2_weights = {}
    xtts_weights = {}

    # List of substrings to identify GPT2 weights
    gpt2_substrings = [
       'ln_1.weight', 'ln_1.bias', 'attn.c_attn.weight', 'attn.c_attn.bias', 'attn.c_proj.weight',
        'attn.c_proj.bias', 'ln_2.weight', 'ln_2.bias', 'mlp.c_fc.weight',
        'mlp.c_fc.bias', 'mlp.c_proj.weight', 'mlp.c_proj.bias', 'ln_f.weight',
        'ln_f.bias', 'mel_head.weight', 'mel_head.bias'

    ]
    ignore_in_check_components = ['mel_embedding.weight', 'mel_pos_embedding.emb.weight']
    # mel_emb -> wte.emb.weight, mel_pos_emb -> wpe.emb.weight
    ignore_keys_from_training = {"torch_mel_spectrogram_style_encoder", "torch_mel_spectrogram_dvae", "dvae"}

    all_sub_str = gpt2_substrings + ignore_in_check_components
    # Separate weights based on substrings
    for key, tensor in checkpoint['model'].items():
        # Check if any GPT2 substring is in the key
        if any(substring in key for substring in ignore_keys_from_training):
            continue # skip training layers
        key = key.replace('xtts.', '')

        is_gpt2_weight = any(substring in key for substring in all_sub_str)

        if is_gpt2_weight:

            if 'mel_embedding.weight' in key:
                key = 'gpt.wte.weight'
            elif 'mel_pos_embedding.emb.weight' in key:
                key = 'gpt.wpe.emb.weight'
            elif 'mel_head' in key:
                key = key.replace('gpt.', '')
            else:
                key = key.replace('gpt.gpt.', 'gpt.')
            # Use a modded name for GPT-2 weights
            gpt2_weights[key] = tensor
        elif 'final_norm' in key:
            gpt2_weights[key.replace('gpt.', '')] = tensor
            xtts_weights[key.replace('gpt.', '')] = tensor
        else:
            # All other weights go to XTTS
            xtts_weights[key.replace('gpt.', '')] = tensor

    # Check if all the weights keys are matched
    assert all(any(substr in key for key in gpt2_weights.keys()) for substr in gpt2_substrings), \
        f"Missing substrings: {[substr for substr in gpt2_substrings if not any(substr in key for key in gpt2_weights.keys())]}"


    gpt2_path = os.path.join(output_dir, "gpt", 'gpt2_model.safetensors')
    save_file(gpt2_weights, gpt2_path)
    download_repo_files("AstraMindAI/xtts2-gpt", os.path.join(output_dir, "gpt"))
    print(f"Saved XTTSv2 GPT-2 weights to {gpt2_path}")
    print(f"XTTSv2 GPT-2 weights: {list(gpt2_weights.keys())}")

    # Save XTTS weights if any exist
    if xtts_weights:
        xtts_path = os.path.join(output_dir, 'core_xttsv2', 'xtts-v2.safetensors')
        save_file(xtts_weights, xtts_path)
        download_repo_files("AstraMindAI/xttsv2", os.path.join(output_dir, "core_xttsv2"))
        print(f"Saved XTTSv2 weights to {xtts_path}")
        print(f"XTTSv2 weights: {list(xtts_weights.keys())}")

def main():
    parser = argparse.ArgumentParser(description='Convert PyTorch checkpoint to SafeTensors format')
    parser.add_argument('checkpoint_path', type=str, help='Path to PyTorch checkpoint file')
    parser.add_argument('--output_dir', type=str, default=os.getcwd(),
                        help='Output directory (defaults to current working directory)')

    args = parser.parse_args()

    # Verify checkpoint file exists
    if not os.path.exists(args.checkpoint_path):
        print(f"Error: Checkpoint file '{args.checkpoint_path}' does not exist")
        return

    # Convert the checkpoint
    convert_checkpoint(args.checkpoint_path, args.output_dir, args)

if __name__ == '__main__':
    main()

================================================================================
# File: auralis/models/xttsv2/components/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/xttsv2/components/__init__.py
================================================================================



================================================================================
# File: auralis/models/xttsv2/components/vllm_mm_gpt.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/xttsv2/components/vllm_mm_gpt.py
================================================================================

import functools
import random
from array import array
from dataclasses import dataclass

import torch
import torch.nn as nn
from typing import Optional, Union, Iterable, Tuple, Mapping

from networkx.algorithms.clique import enumerate_all_cliques
from torch import Tensor
from transformers import GPT2Config
from triton.language import dtype

from vllm.attention import AttentionMetadata
from vllm.config import CacheConfig, MultiModalConfig, VllmConfig
from vllm.distributed import get_pp_group
from vllm.inputs import InputContext, INPUT_REGISTRY, DecoderOnlyInputs, token_inputs, DummyData
from vllm.model_executor.layers.logits_processor import LogitsProcessor
from vllm.model_executor.layers.quantization import QuantizationConfig
from vllm.model_executor.layers.sampler import Sampler, SamplerOutput
from vllm.model_executor.layers.vocab_parallel_embedding import VocabParallelEmbedding, ParallelLMHead
from vllm.model_executor.model_loader.weight_utils import default_weight_loader
from vllm.model_executor.models.gpt2 import GPT2Block
from vllm.model_executor.models.utils import make_layers, make_empty_intermediate_tensors_factory
from vllm.model_executor.sampling_metadata import SamplingMetadata
from vllm.multimodal import MULTIMODAL_REGISTRY, MultiModalInputs, MultiModalKwargs
from vllm.multimodal.inputs import PlaceholderRange
from vllm.multimodal.utils import consecutive_placeholder_ranges
from vllm.sequence import IntermediateTensors, SequenceData, VLLM_TOKEN_ID_ARRAY_TYPE
from vllm.model_executor.models.interfaces import SupportsMultiModal, SupportsPP

from typing import Dict, List
from collections import defaultdict

from vllm.utils import is_list_of

PrefillLength= Union[int, List[int]]
TokenPosition= Union[int, List[int]]
TokenId = Union[Union[torch.Tensor,int], List[Union[torch.Tensor,int]]]

@dataclass
class TokenPositionAndPrefillTuple:
    prefill_len: Optional[PrefillLength] = None
    pos_id: Optional[TokenPosition] = None
    token_id: Optional[TokenId] = None

    def update_(self,
                prefill_len: Optional[PrefillLength] = None,
                pos_id: Optional[TokenPosition] = None,
                token_id: Optional[TokenId] = None):
        if prefill_len is not None:
            self.prefill_len=prefill_len
        if pos_id is not None:
            self.pos_id=pos_id
        if token_id is not None:
            self.token_id= token_id
        return self


class PositionalEmbeddingsCorrecter:
    """Corrects positional embeddings for XTTS model,
    since they have a different length than the text embeddings.
    This class tracks tokens both by request_id and position for vLLM compatibility.
    """

    def __init__(self):
        # Maps request_id to its prefill length
        self.request_tracker_dict: Dict[str, TokenPositionAndPrefillTuple] = defaultdict(lambda: TokenPositionAndPrefillTuple())
        # Maps token_position pairs to their request_id
        self.token_to_request: Dict[str, str] = {}

    def init_request_id_prefill(self, request_id: str, prefill_len: PrefillLength, nex_token: torch.Tensor):
        """Initialize a request_id with its prefill length."""
        self.request_tracker_dict[request_id] = TokenPositionAndPrefillTuple(prefill_len, prefill_len)
        self.token_to_request[f"{nex_token}_{prefill_len}"] = request_id

    def get_by_request_id(self, request_id: str) -> TokenPositionAndPrefillTuple:
        """Retrieve the prefill length for a given request_id."""
        return self.request_tracker_dict.get(request_id, None)

    def get_by_next_token(self,
                          next_token_ids: List[int],
                          next_position_ids: List[int]
                          ) -> List[Optional[TokenPositionAndPrefillTuple]]:
        """Retrieve prefill lengths for given token and position pairs.

        Args:
            next_token_ids: List of token IDs
            next_position_ids: List of position IDs, corresponding to token IDs

        Returns:
            List of prefill lengths for each token-position pair

        Raises:
            ValueError: If no valid token mappings are found
        """
        prefill_lengths = []
        assert len(next_token_ids) == len(next_position_ids), "Token and position lists must have the same length"
        if len(next_token_ids) == 0:
            return prefill_lengths
        for next_token_id, next_position_id in zip(next_token_ids, next_position_ids):
            token_key = f"{next_token_id}_{next_position_id}"
            if token_key in self.token_to_request:
                request_id = self.token_to_request[token_key]
                prefill_lengths.append(self.request_tracker_dict[request_id].update_(token_id=next_token_id))

        if not prefill_lengths:
            raise ValueError(f"No valid mappings found for token pairs")
        return prefill_lengths

    def _invalidate_previous_mapping(self, request_id: str):
        """Remove all token mappings associated with a given request_id.

        This prevents memory leaks from old token mappings and ensures
        we don't have stale token-to-request associations.
        """
        # Find all token keys that map to this request_id
        keys_to_remove = [
            token_key for token_key, req_id in self.token_to_request.items()
            if req_id == request_id
        ]

        # Remove all found mappings
        for token_key in keys_to_remove:
            del self.token_to_request[token_key]

    def _get_pos_id_and_update (self, request_id: str):
        """Get the position ID for a given request_id and update it."""
        tuple_prefill_token = self.get_by_request_id(request_id)
        # Update the position ID
        self.request_tracker_dict[request_id] = TokenPositionAndPrefillTuple(tuple_prefill_token.prefill_len, tuple_prefill_token.pos_id + 1)
        return tuple_prefill_token.pos_id + 1


    def associate_new_tokens(self, request_id: str, next_token_id: int):
        """Associate a new token-position pair with a request_id.

        Before creating the new association, it removes all previous
        token mappings for this request_id to maintain consistency.

        Args:
            request_id: The request identifier
            next_token_id: The token ID to associate
        """
        pos_id = self._get_pos_id_and_update(request_id)

        # Clean up old mappings first
        self._invalidate_previous_mapping(request_id)

        # Create new mapping
        self.token_to_request[f"{next_token_id}_{pos_id}"] = request_id

    def clear_request(self, request_id: str):
        """Remove all data associated with a request_id.

        This includes both the prefill length tracking and any token mappings.
        """
        if request_id in self.request_tracker_dict:
            # First remove all token mappings
            self._invalidate_previous_mapping(request_id)
            # Then remove the request tracking
            del self.request_tracker_dict[request_id]

class LearnedPositionEmbeddings(nn.Module):
    def __init__(self, seq_len, model_dim, init=0.02, relative=False, supports_pp=False):
        super().__init__()
        # nn.Embedding
        self.emb = VocabParallelEmbedding(seq_len, model_dim) if supports_pp else nn.Embedding(seq_len, model_dim)
        # Initializing this way is standard for GPT-2
        self.emb.weight.data.normal_(mean=0.0, std=init)
        self.relative = relative
        self.seq_len = seq_len

    def forward(self, x):
        sl = x.shape[1]
        if self.relative:
            start = random.randint(sl, self.seq_len) - sl
            indices = torch.arange(start, start + sl, device=x.device)
            # Validate indices
            assert (indices < self.seq_len).all() and (indices >= 0).all(), \
                f"Invalid position indices in forward: min={indices.min().item()}, max={indices.max().item()}, valid_range=[0,{self.seq_len-1}]"
            return self.emb(indices)
        else:
            indices = torch.arange(0, sl, device=x.device)
            # Validate indices
            assert (indices < self.seq_len).all(), \
                f"Sequence length {sl} exceeds maximum position embedding length {self.seq_len}"
            return self.emb(indices)

    def get_fixed_embedding(self, ind: torch.Tensor, dev: torch.device) -> torch.Tensor:
        """Get position embeddings with batch support.

        Args:
            ind: Position indices tensor. Can be single or batched
                 Shape: [..., seq_len] or [seq_len]
            dev: Target device for the embeddings

        Returns:
            Position embeddings tensor matching input shape plus embedding dimension
            Shape: [batch_size, seq_len, model_dim] or [1, 1, model_dim]
        """
        # Validation of indices to prevent unknown errors
        assert (ind < self.seq_len).all(), \
            f"Position indices out of range. Found max={ind.max().item()}, but maximum allowed is {self.seq_len-1}"
        assert (ind >= 0).all(), \
            f"Negative position indices found. Min value={ind.min().item()}"

        if ind.shape[0] > 1:

            return self.emb(ind)
        else:
            #assert ind.dim() <= 2, f"Single input should have 1 or 2 dimensions, got {ind.dim()}"
            return self.emb(torch.tensor([ind], device=dev)).unsqueeze(0)



def get_xtts_max_audio_tokens(ctx: InputContext) -> int:
    """Calculate maximum audio tokens based on text context and audio duration."""
    return 32 # the conditoning perciever output


def dummy_seq_data_for_xtts(
        ctx: InputContext,
        seq_len: int,
        audio_count: int,
):
    """Create dummy sequence data for XTTS profiling."""
    # Calculate audio token space needed
    conditioning_length = (32 # the conditioning perceiver output length in the sql (which is fixed)
                           +
                           1) # the start audio token

    return SequenceData.from_prompt_token_counts(
        (1, conditioning_length * audio_count),
        (0, seq_len - conditioning_length * audio_count)),{
        "audio":
            consecutive_placeholder_ranges(num_items=audio_count,
                                           item_size=conditioning_length)
    }


def dummy_conditioning_for_xtts(
        ctx: InputContext,
        seq_len: int,
        audio_count: int,
) -> dict:
    """Create dummy conditioning data for XTTS."""
    return {
        "audio": {
            "embeds":[
                torch.zeros(
                    (seq_len, ctx.model_config.hf_config.hidden_size),
                    dtype=ctx.model_config.dtype) for _ in range(audio_count)
            ],
            "is_logits_only_mode": False,
            "sequence_length": -1,
        }
    }


def dummy_data_for_xtts(
        ctx: InputContext,
        seq_len: int,
        mm_counts: Mapping[str, int],
):
    """Create complete dummy data for XTTS profiling."""
    audio_count = mm_counts["audio"]
    seq_data, ranges = dummy_seq_data_for_xtts(ctx, seq_len, audio_count)
    cond_data = dummy_conditioning_for_xtts(ctx, seq_len, audio_count)
    return DummyData(seq_data, cond_data, ranges)


def input_mapper_for_xtts(ctx: InputContext, data: Union[Dict, List[Tensor]]) -> MultiModalKwargs:
    """Map input data to XTTS format."""

    if not isinstance(data, list):
        data = [data]

    if len(data) == 0:
        return MultiModalKwargs()

    assert is_list_of(data, dict, check="all"), (f"Expected a list of dictionaries, "
                                                 f"but got a list of {[type(dat) for dat in data if type(dat) != dict][0]}")

    embeds = [dat["embeds"] for dat in data]
    is_logits_only_mode = [dat.get("is_logits_only_mode", False) for dat in data]
    sequence_length = [dat.get("sequence_length", -1) for dat in data]
    return MultiModalKwargs(
        {
            "cond_latents": embeds,
            "is_logits_only_mode": is_logits_only_mode,
            "sequence_length": sequence_length
        }
    )




def input_processor_for_xtts2_gpt(ctx: InputContext, inputs: DecoderOnlyInputs):
    """
    We'll accomodate for the extra contditioning token and for the start audio token,
    we actually insert a -1 repeated for the differecne in length between the conditioning and the tokenized text
    and then we add 1 for the start audio token
    Args:
        ctx:
        inputs:

    Returns:

    """
    multi_modal_data = inputs.get("multi_modal_data")
    if multi_modal_data is None or "audio" not in multi_modal_data:
        raise ValueError("Missing audio data in multi-modal inputs")

    audio_dict = multi_modal_data['audio']
    audio = audio_dict.get('embeds')

    is_last_decoding_pass = audio_dict.get("is_logits_only_mode", False)

    prompt_token_ids = inputs.get("prompt_token_ids")

    if not is_last_decoding_pass:
        # we fill everything with 1 since we don't actually needs text token ids, it would mess up in the sampling step
        new_token_ids = ([1] * (audio.shape[0])) + [ctx.model_config.hf_config.start_audio_token] # add the start audio generation token
    else:
        new_token_ids = ([1] * audio.shape[0]) + prompt_token_ids
    # the encoding had already been done externally to reuse the embeddings for later use but we
    # account for the new token that will be added before generation
    new_prompt = None
    return token_inputs(prompt_token_ids=new_token_ids,
                        prompt=new_prompt,
                        multi_modal_data=multi_modal_data,
                        multi_modal_placeholders={'audio':[PlaceholderRange(offset=0, length=len(new_token_ids))]})


@MULTIMODAL_REGISTRY.register_input_mapper("audio", input_mapper_for_xtts)
@MULTIMODAL_REGISTRY.register_max_multimodal_tokens("audio", get_xtts_max_audio_tokens)
@INPUT_REGISTRY.register_dummy_data(dummy_data_for_xtts)
@INPUT_REGISTRY.register_input_processor(input_processor_for_xtts2_gpt)
class XttsGPT(nn.Module, SupportsMultiModal, SupportsPP):
    def __init__( # type: ignore
            self,
            vllm_config: VllmConfig,
            prefix: str,
            cache_config: Optional[CacheConfig] = None,
            quant_config: Optional[QuantizationConfig] = None,
    ):
        super().__init__()
        self.config = vllm_config
        self.gpt_config = self.config.model_config.hf_config
        self.quant_config = quant_config

        self.prefix_sequence_dict: Dict[str, torch.Tensor] = {}
        # Core GPT components
        self.gpt = GPT2Model(
            self.gpt_config,
            cache_config,
            quant_config,
            prefix="gpt"
        )

        self.final_norm =  nn.LayerNorm(self.gpt_config.hidden_size, bias=True, eps=self.gpt_config.layer_norm_epsilon)
        # Output head for mel tokens
        self.mel_head = ParallelLMHead(
            self.gpt_config.num_audio_tokens,
            self.gpt_config.hidden_size,
            bias=True,
            quant_config=quant_config,
            prefix="mel_head"
        )
        self.audio_start_generation_token = self.gpt_config.start_audio_token

        self.gpt.audio_start_generation_token = self.audio_start_generation_token


        # Initialize logits processor and sampler
        logit_scale = getattr(self.gpt_config, "logit_scale", 1.0)
        self.logits_processor = LogitsProcessor(self.gpt_config.num_audio_tokens,
                                                self.gpt_config.num_audio_tokens,
                                                logit_scale)
        self.sampler = Sampler()

        self.positional_embeddings_correcter = PositionalEmbeddingsCorrecter()

    @staticmethod
    def _check_is_logits_only_mode(is_logits_only_mode) -> torch.Tensor:

        # First check if it's a boolean
        if isinstance(is_logits_only_mode, bool):
            return torch.tensor([is_logits_only_mode])

        # Then check if it's a tensor
        if torch.is_tensor(is_logits_only_mode):
            # if it's a scalar tensor, return the value
            if is_logits_only_mode.numel() == 1:
                return is_logits_only_mode
            # for non-scalar tensors, check if all elements are the same
            return is_logits_only_mode

        # Fallback
        return torch.tensor([bool(is_logits_only_mode)])

    @staticmethod
    def find_len_of_sequence(
            positions_ids: torch.Tensor,
            index: torch.Tensor
    ) -> torch.Tensor:
        """
        Starting from the index, it goes backward in the positions until it finds a jump higher than 1.
        This function is tensorized for efficiency.

        Args:
        positions_ids: Tensor of position IDs
        index: Tensor of indices to start searching from

        Returns:
        Tensor of sequence lengths
        """
        # Ensure index is a tensor
        if not isinstance(index, torch.Tensor):
            index = torch.tensor(index, device=positions_ids.device)

        # Create a mask for valid positions (from 0 to index for each element)
        mask = torch.arange(positions_ids.size(0), device=positions_ids.device).unsqueeze(0) <= index

        # Calculate differences between adjacent positions
        diffs = positions_ids[1:] - positions_ids[:-1]

        # Pad the diffs tensor to match the original size
        diffs = torch.cat([torch.ones(1, device=positions_ids.device), diffs])

        # Find where the difference is different from 1 and is within the valid range
        jumps = (diffs != 1) & mask

        # Get the indices of the jumps
        jump_indices = jumps.nonzero()

        # If no jumps are found, return the index itself (full length)
        if jump_indices.numel() == 0:
            return torch.tensor([0], device=positions_ids.device)

        # Get the last jump for each index
        last_jumps = jump_indices[:, 1].reshape(-1, jump_indices.size(0))[:, -1]

        # Calculate the sequence lengths
        return last_jumps

    def _maybe_correct_positions(self,
                                 input_ids: torch.Tensor,
                                 positions: torch.Tensor,
                                 conditioning_inputs_list: List[torch.Tensor]):
        correct_positions_ids = self.positional_embeddings_correcter.get_by_next_token(input_ids.tolist(),
                                                                                       positions.tolist())
        if len(correct_positions_ids) > 0:
            position_and_id_tensor = torch.cat(
                [positions.unsqueeze(0), input_ids.unsqueeze(0)],
                dim=0
            )

            index_2d = torch.tensor(
                [(correct_positions_id.pos_id, correct_positions_id.token_id) for
                 correct_positions_id in correct_positions_ids],
                device=positions.device
            )

            prefill_len_token = torch.tensor(
                [correct_positions_id.prefill_len for correct_positions_id in correct_positions_ids],
                device=positions.device)

            position_and_id_expanded = position_and_id_tensor.unsqueeze(-1)
            index_2d_expanded = index_2d.T.unsqueeze(1)

            matches = (position_and_id_expanded == index_2d_expanded).all(dim=0)
            matching_indices = matches.any(dim=1).nonzero().squeeze(1)

            if not isinstance(conditioning_inputs_list, list) or len(conditioning_inputs_list) < 1:
                # this is the case where all the tokens are a "second iter" token,
                # so we don't have mixed stages in the batch
                return 1 + positions - prefill_len_token
            # Iterate through all matching indices
            for idx, seq_idx in enumerate(matching_indices):

                # Ensure we have corresponding conditioning input
                if (isinstance(conditioning_inputs_list, list) and
                        len(conditioning_inputs_list) > 0 and
                        idx < len(conditioning_inputs_list)):
                    end_pos = seq_idx + 1
                    start_pos = self.find_len_of_sequence(positions, seq_idx)  # type: ignore

                    # Apply correction only to the relevant part of the sequence
                    positions[start_pos:end_pos] = 1 + positions[start_pos:end_pos] - \
                                                   correct_positions_ids[
                                                       idx].prefill_len

            return positions

    def _apply_op_to_seq_in_batch(self,
                                  input_ids: torch.Tensor,
                                  positions: torch.Tensor,
                                  conditioning_inputs_list: List[torch.Tensor],
                                  is_logit_only_mode: torch.Tensor,
                                  seq_len: Union[torch.Tensor],
                                  is_profiling_run: bool = False
                                  ) -> Tuple[List[int], torch.Tensor, torch.Tensor]:
        """
        Apply different ops to the tensors sequence in the batch
        Returns:
            - List of starting indexes
            - A tensor for the logit only mode
            - A mask to reinsert the tokens in the correct position for the logit only mode
            - Modified input IDs
            - Modified positions
        """
        if is_profiling_run:
            return [], input_ids, positions

        # Pre-allocate lists for better memory efficiency
        starting_indexes = []

        # Find all end markers at once
        end_markers = (input_ids == self.audio_start_generation_token).nonzero(as_tuple=True)[0]

        if len(end_markers) == 0:
            positions = self._maybe_correct_positions(input_ids, positions, conditioning_inputs_list)
            return [], input_ids, positions

        # Create mask for valid conditioning inputs
        cond_latent_mask = torch.tensor([
            isinstance(cond_latent, torch.Tensor) and cond_latent.dim() > 1
            for cond_latent in conditioning_inputs_list
        ], device=input_ids.device)

        effective_indexes = cond_latent_mask.nonzero(as_tuple=True)[0]

        # Pre-calculate all sequence lengths
        sequence_lengths = torch.tensor([
            cond.shape[(0 if cond.dim == 1 else 1)] if isinstance(cond, torch.Tensor) and cond.dim() > 1
            else 0 for cond in conditioning_inputs_list
        ], device=input_ids.device)

        # Create masks for efficient tensor operations
        keep_mask = torch.ones(len(input_ids), dtype=torch.bool, device=input_ids.device)
        non_logit_mask = torch.ones_like(keep_mask)

        cumulative_offset = 0

        for idx, end_marker in zip(effective_indexes, end_markers):
            # Calculate effective positions
            end_pos = end_marker.item() - cumulative_offset
            start_pos = end_pos - sequence_lengths[idx].item()
            start_pos_for_masking = start_pos + cumulative_offset

            # Store original starting index
            starting_indexes.append(start_pos_for_masking)

            if is_logit_only_mode[idx]:
                # here the logic is a bit messy:
                # in the og implementation, the treats the embedding for the star tof generation token differently.
                # during the autoregressive token generation phase they use the token embeddings of the start
                # of generation token as input for the position embeddings, but in the logit only mode they use the
                # position id of the start of generation token as input for the position embeddings

                non_logit_mask[start_pos_for_masking : end_pos + cumulative_offset + seq_len[idx]] = False
                keep_mask[start_pos_for_masking:end_pos + cumulative_offset] = False
                # Generate positions for this sequence
                new_positions = torch.arange(
                    0, seq_len[idx].item(), # starting from zero since we have the start audio token
                    device=input_ids.device,
                    dtype=positions.dtype
                )
                # Update positions
                if end_pos + len(new_positions) <= len(positions):
                    positions[end_pos + cumulative_offset:end_pos + cumulative_offset + seq_len[idx]] = new_positions

            else:

                # Update masks
                keep_mask[start_pos_for_masking:end_pos + cumulative_offset + 1] = False

            cumulative_offset += (end_pos - start_pos + 1)

        # Apply masks to get final tensors
        # First we select tokens that are not used in the logit only mode
        # we have tre scenarios here:
        # 1. We are in a first pass where we have a sequence of 1s tokens terminated by a start audio token,
        # we completely remove this and we keep the index on where to insert since we have already precomputed the values
        # 2. We are in a "second pass" (autoregressive pass), using the default process of vllm with corrected positions ids
        # 3. We are in a logit only mode, since in xttsv2 we need to capture the hs,
        # and to do this we pass the conditioning alongside the generated tokens,
        # we need to remove the placeholder sequence at the beginning while adjusting
        # the positioning inside that condition
        non_logit_input_ids = input_ids[non_logit_mask & keep_mask]
        non_logit_positions = positions[non_logit_mask & keep_mask]

        correct_positions = self._maybe_correct_positions(
            # if we arrive here it means that we had mixed "second passes" and "logit only mode" in the batch,
            non_logit_input_ids,
            non_logit_positions,
            conditioning_inputs_list
        )
        if correct_positions is not None:
            # only happens if chunk prefill is enabled
            positions[non_logit_mask & keep_mask] = correct_positions

        modified_input_ids = input_ids[keep_mask]
        modified_positions = positions[keep_mask]
        assert (modified_positions < 608).all()
        assert (modified_positions >= 0).all()
        return starting_indexes, modified_input_ids, modified_positions


    # noinspection PyMethodOverriding
    def forward( # type: ignore
            self,
            input_ids: torch.Tensor,
            positions: torch.Tensor,
            kv_caches: List[torch.Tensor],
            attn_metadata: AttentionMetadata,
            intermediate_tensors: Optional["IntermediateTensors"] = None,
            cond_latents: Optional[Union[torch.Tensor, List[torch.Tensor]]] = False, # so we can always have a list
            is_logits_only_mode: Union[torch.Tensor, bool] = False,
            sequence_length: Union[torch.Tensor,int] = -1,
            **kwargs,
    ) -> Union[torch.Tensor, "IntermediateTensors"]:
        """Forward pass following VLLM pattern."""

        is_profiling_run = False

        # we work with list conditioning so we convert them to list regardless of vllm batching
        if isinstance(cond_latents, torch.Tensor):
            if len(cond_latents.shape) > 4:
                is_profiling_run = True
            else:
                # if two equal tensors are passed, vllm aggregate them in a new (batched) tensor
                cond_latents = list(cond_latents)  # so we unbacth them :) (unless we are in the profiling run)

        is_logits_only_mode = self._check_is_logits_only_mode(is_logits_only_mode)

        starting_sequence_start_ids, input_ids, positions = self._apply_op_to_seq_in_batch(input_ids,
                                                                                           positions,
                                                                                           cond_latents,
                                                                                           is_logits_only_mode,
                                                                                           sequence_length,
                                                                                           is_profiling_run)


        hidden_states = self.gpt(
            input_ids=input_ids,
            position_ids=positions,
            kv_caches=kv_caches,
            attn_metadata=attn_metadata,
            intermediate_tensors=intermediate_tensors,
            # this is the conditioning input ( voice conditioning + text_embeds )
            input_embeds=cond_latents,
            starting_sequence_start_ids=starting_sequence_start_ids,
            is_profiling_run= is_profiling_run,
            is_logit_only=is_logits_only_mode
        )

        return hidden_states

    # noinspection PyUnresolvedReferences
    def compute_logits(
            self,
            hidden_states: torch.Tensor,
            sampling_metadata: SamplingMetadata,
    ) -> Optional[torch.Tensor]:
        # normalize the hidden states
        # we keep this, because in the xttsv2 code they have a nn.sequential with norm and then lm head
        hidden_states = self.final_norm(hidden_states)

        # we keep track of the last collected index to properly associate the hidden states with the correct request_id
        last_collected_idx = 0
        for seq in sampling_metadata.seq_groups:
            # Check if we need to collect hidden states
            sampling_params = seq.sampling_params
            if (hasattr(sampling_params, 'hidden_state_collector')
                    and sampling_params.hidden_state_collector is not None):
                self.positional_embeddings_correcter.clear_request(sampling_params.request_id)
                # Call the collector directly with the hidden states
                sampling_params.hidden_state_collector(hidden_states[last_collected_idx:last_collected_idx+seq.seq_len], sampling_params.request_id)  # The request_id is already bound

            last_collected_idx += seq.seq_len or 0

        # Compute logits using the mel_head
        logits = self.logits_processor(self.mel_head, hidden_states, sampling_metadata, self.mel_head.bias)
        return logits

    # noinspection PyUnresolvedReferences
    def sample(
            self,
            logits: torch.Tensor,
            sampling_metadata: SamplingMetadata,
    ) -> Optional[SamplerOutput]:
        next_tokens = self.sampler(logits, sampling_metadata)
        for seq_id, seq_groups in enumerate(sampling_metadata.seq_groups):
            if hasattr(seq_groups.sampling_params, 'request_id') and seq_groups.sampling_params.request_id is not None:
                idx = seq_groups.seq_ids[0]
                # Call the collector directly with the next tokens
                if not self.positional_embeddings_correcter.get_by_request_id(seq_groups.sampling_params.request_id):
                    self.positional_embeddings_correcter.init_request_id_prefill(
                        request_id = seq_groups.sampling_params.request_id,
                        prefill_len=len(seq_groups.seq_data[idx].prompt_token_ids),
                        nex_token=next_tokens.outputs[seq_id].samples[0].output_token # index out of error
                    )
                else:
                    self.positional_embeddings_correcter.associate_new_tokens(
                        request_id=seq_groups.sampling_params.request_id,
                        next_token_id=next_tokens.outputs[seq_id].samples[0].output_token)

        return next_tokens

    def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
        """Load weights following VLLM pattern."""
        params_dict = dict(self.named_parameters(remove_duplicate=False))
        loaded_names = set()
        for name, loaded_weight in weights:
            if name not in params_dict:
                continue

            param = params_dict[name]
            if "c_attn" in name or "c_proj" in name or "c_fc" in name:
                if name.endswith(".weight"):
                    loaded_weight = loaded_weight.t()

            weight_loader = getattr(param, "weight_loader", default_weight_loader)
            weight_loader(param, loaded_weight)
            loaded_names.add(name)
        # used to check if all weights were loaded
        assert set(params_dict.keys()) - loaded_names == set(), \
            (f"Missing weights: {set(params_dict.keys()) - loaded_names}, "
             f"this probably means you are using an incompatible model, \n\nyour model has this weights: {set(params_dict.keys())}")

class GPT2Model(nn.Module):

    def __init__(
            self,
            config: GPT2Config,
            cache_config: Optional[CacheConfig] = None,
            quant_config: Optional[QuantizationConfig] = None,
            prefix: str = "",
    ):
        super().__init__()
        self.config = config
        assert not config.add_cross_attention
        assert not config.scale_attn_by_inverse_layer_idx
        assert not config.reorder_and_upcast_attn
        self.audio_start_generation_token = None
        self.embed_dim = config.hidden_size
        self.wte = VocabParallelEmbedding(config.num_audio_tokens, self.embed_dim)
        self.wpe = (
            LearnedPositionEmbeddings(config.max_audio_tokens + 3, config.decoder_input_dim)
            if config.max_audio_tokens != -1
            else functools.partial(config.null_position_embeddings, dim=config.decoder_input_dim)
        )
        self.start_layer, self.end_layer, self.h = make_layers(
            config.num_hidden_layers,
            lambda prefix: GPT2Block(
                config, cache_config, quant_config, prefix=prefix),
            prefix=f"{prefix}.h")
        self.ln_f = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)
        self.make_empty_intermediate_tensors = (
            make_empty_intermediate_tensors_factory(["hidden_states"],
                                                    config.hidden_size))

    @staticmethod
    def _insert_conditioning_into_hidden_states(hidden_states: torch.Tensor,
                                                conditioning_inputs: Optional[List[torch.Tensor]],
                                                start_of_generation_embed: Optional[torch.Tensor],
                                                insertion_ids: List[int],
                                                is_logit_only: torch.Tensor) -> torch.Tensor:
        empty_tensor = torch.empty(
            (0,hidden_states.shape[-1]),
            device=hidden_states.device, dtype=hidden_states.dtype
        )
        for idx, (inserion_idx, conditioning_input) in enumerate(zip(insertion_ids, conditioning_inputs)):
                hidden_states = torch.cat([
                hidden_states[:inserion_idx],
                conditioning_input.squeeze(0),
                (start_of_generation_embed if ~is_logit_only[idx] else empty_tensor),
                hidden_states[inserion_idx:]], dim=0
            )

        return hidden_states

    def forward(
            self,
            input_ids: torch.Tensor,
            position_ids: torch.Tensor,
            kv_caches: List[torch.Tensor],
            attn_metadata: AttentionMetadata,
            intermediate_tensors: Optional[IntermediateTensors],
            input_embeds: Optional[torch.Tensor] = None,
            starting_sequence_start_ids: Optional[List[int]] = None,
            is_profiling_run: bool = False,
            is_logit_only: torch.Tensor = False
    ) -> Union[torch.Tensor, IntermediateTensors]:

        if get_pp_group().is_first_rank:
            starting_sequence_embed = None
            if isinstance(input_embeds, list) and len(input_embeds) > 0:
                # we could be either in start condition or in a final condition or both
                if len(starting_sequence_start_ids) > 0 and not (is_logit_only).all():
                    # we have starting sequences, so we just need to get one hs to insert later
                    starting_sequence_embed = self.wte(
                        torch.tensor(
                            self.audio_start_generation_token,
                            device=input_ids.device
                        ).unsqueeze(0)
                    )

                    starting_sequence_embed += self.wpe(starting_sequence_embed.reshape(-1, 1))

            audio_inputs_embeds = self.wte(input_ids).squeeze(0)

            if len(input_ids) == 0:
                # if we have just starting sequences audio_inputs_embeds is an empty tensor
                position_embeds = audio_inputs_embeds.clone()
            else:
                position_embeds = self.wpe.get_fixed_embedding(
                    position_ids, input_ids.device
                ) if not is_profiling_run else self.wpe(input_ids.reshape(-1, 1))

            hidden_states = (audio_inputs_embeds + position_embeds).view(-1, self.embed_dim)

            if isinstance(input_embeds, list) and len(input_embeds) > 0:
                hidden_states = self._insert_conditioning_into_hidden_states(
                    hidden_states,
                    input_embeds,
                    starting_sequence_embed,
                    starting_sequence_start_ids,
                    is_logit_only)

        else:
            assert intermediate_tensors is not None
            hidden_states = intermediate_tensors["hidden_states"]

        for i in range(self.start_layer, self.end_layer):
            layer = self.h[i]
            hidden_states = layer(hidden_states,
                                  kv_caches[i - self.start_layer],
                                  attn_metadata)

        if not get_pp_group().is_last_rank:
            return IntermediateTensors({"hidden_states": hidden_states})

        hidden_states = self.ln_f(hidden_states)
        return hidden_states



================================================================================
# File: auralis/models/xttsv2/components/tts/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/xttsv2/components/tts/__init__.py
================================================================================



================================================================================
# File: auralis/models/xttsv2/components/tts/layers/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/xttsv2/components/tts/layers/__init__.py
================================================================================



================================================================================
# File: auralis/models/xttsv2/components/tts/layers/xtts/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/xttsv2/components/tts/layers/xtts/__init__.py
================================================================================



================================================================================
# File: auralis/models/xttsv2/components/tts/layers/xtts/zh_num2words.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/xttsv2/components/tts/layers/xtts/zh_num2words.py
================================================================================

# Authors:
#   2019.5 Zhiyang Zhou (https://github.com/Joee1995/chn_text_norm.git)
#   2019.9 - 2022 Jiayu DU

import argparse
import csv
import os
import re
import string
import sys

# fmt: off

# ================================================================================ #
#                                    basic constant
# ================================================================================ #
CHINESE_DIGIS = "零一二三四五六七八九"
BIG_CHINESE_DIGIS_SIMPLIFIED = "零壹贰叁肆伍陆柒捌玖"
BIG_CHINESE_DIGIS_TRADITIONAL = "零壹貳參肆伍陸柒捌玖"
SMALLER_BIG_CHINESE_UNITS_SIMPLIFIED = "十百千万"
SMALLER_BIG_CHINESE_UNITS_TRADITIONAL = "拾佰仟萬"
LARGER_CHINESE_NUMERING_UNITS_SIMPLIFIED = "亿兆京垓秭穰沟涧正载"
LARGER_CHINESE_NUMERING_UNITS_TRADITIONAL = "億兆京垓秭穰溝澗正載"
SMALLER_CHINESE_NUMERING_UNITS_SIMPLIFIED = "十百千万"
SMALLER_CHINESE_NUMERING_UNITS_TRADITIONAL = "拾佰仟萬"

ZERO_ALT = "〇"
ONE_ALT = "幺"
TWO_ALTS = ["两", "兩"]

POSITIVE = ["正", "正"]
NEGATIVE = ["负", "負"]
POINT = ["点", "點"]
# PLUS = [u'加', u'加']
# SIL = [u'杠', u'槓']

FILLER_CHARS = ["呃", "啊"]

ER_WHITELIST = (
    "(儿女|儿子|儿孙|女儿|儿媳|妻儿|"
    "胎儿|婴儿|新生儿|婴幼儿|幼儿|少儿|小儿|儿歌|儿童|儿科|托儿所|孤儿|"
    "儿戏|儿化|台儿庄|鹿儿岛|正儿八经|吊儿郎当|生儿育女|托儿带女|养儿防老|痴儿呆女|"
    "佳儿佳妇|儿怜兽扰|儿无常父|儿不嫌母丑|儿行千里母担忧|儿大不由爷|苏乞儿)"
)
ER_WHITELIST_PATTERN = re.compile(ER_WHITELIST)

# 中文数字系统类型
NUMBERING_TYPES = ["low", "mid", "high"]

CURRENCY_NAMES = "(人民币|美元|日元|英镑|欧元|马克|法郎|加拿大元|澳元|港币|先令|芬兰马克|爱尔兰镑|" "里拉|荷兰盾|埃斯库多|比塞塔|印尼盾|林吉特|新西兰元|比索|卢布|新加坡元|韩元|泰铢)"
CURRENCY_UNITS = "((亿|千万|百万|万|千|百)|(亿|千万|百万|万|千|百|)元|(亿|千万|百万|万|千|百|)块|角|毛|分)"
COM_QUANTIFIERS = (
    "(匹|张|座|回|场|尾|条|个|首|阙|阵|网|炮|顶|丘|棵|只|支|袭|辆|挑|担|颗|壳|窠|曲|墙|群|腔|"
    "砣|座|客|贯|扎|捆|刀|令|打|手|罗|坡|山|岭|江|溪|钟|队|单|双|对|出|口|头|脚|板|跳|枝|件|贴|"
    "针|线|管|名|位|身|堂|课|本|页|家|户|层|丝|毫|厘|分|钱|两|斤|担|铢|石|钧|锱|忽|(千|毫|微)克|"
    "毫|厘|分|寸|尺|丈|里|寻|常|铺|程|(千|分|厘|毫|微)米|撮|勺|合|升|斗|石|盘|碗|碟|叠|桶|笼|盆|"
    "盒|杯|钟|斛|锅|簋|篮|盘|桶|罐|瓶|壶|卮|盏|箩|箱|煲|啖|袋|钵|年|月|日|季|刻|时|周|天|秒|分|旬|"
    "纪|岁|世|更|夜|春|夏|秋|冬|代|伏|辈|丸|泡|粒|颗|幢|堆|条|根|支|道|面|片|张|颗|块)"
)


# Punctuation information are based on Zhon project (https://github.com/tsroten/zhon.git)
CN_PUNCS_STOP = "！？｡。"
CN_PUNCS_NONSTOP = "＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃《》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏·〈〉-"
CN_PUNCS = CN_PUNCS_STOP + CN_PUNCS_NONSTOP

PUNCS = CN_PUNCS + string.punctuation
PUNCS_TRANSFORM = str.maketrans(PUNCS, "," * len(PUNCS), "")  # replace puncs with English comma


# https://zh.wikipedia.org/wiki/全行和半行
QJ2BJ = {
    "　": " ",
    "！": "!",
    "＂": '"',
    "＃": "#",
    "＄": "$",
    "％": "%",
    "＆": "&",
    "＇": "'",
    "（": "(",
    "）": ")",
    "＊": "*",
    "＋": "+",
    "，": ",",
    "－": "-",
    "．": ".",
    "／": "/",
    "０": "0",
    "１": "1",
    "２": "2",
    "３": "3",
    "４": "4",
    "５": "5",
    "６": "6",
    "７": "7",
    "８": "8",
    "９": "9",
    "：": ":",
    "；": ";",
    "＜": "<",
    "＝": "=",
    "＞": ">",
    "？": "?",
    "＠": "@",
    "Ａ": "A",
    "Ｂ": "B",
    "Ｃ": "C",
    "Ｄ": "D",
    "Ｅ": "E",
    "Ｆ": "F",
    "Ｇ": "G",
    "Ｈ": "H",
    "Ｉ": "I",
    "Ｊ": "J",
    "Ｋ": "K",
    "Ｌ": "L",
    "Ｍ": "M",
    "Ｎ": "N",
    "Ｏ": "O",
    "Ｐ": "P",
    "Ｑ": "Q",
    "Ｒ": "R",
    "Ｓ": "S",
    "Ｔ": "T",
    "Ｕ": "U",
    "Ｖ": "V",
    "Ｗ": "W",
    "Ｘ": "X",
    "Ｙ": "Y",
    "Ｚ": "Z",
    "［": "[",
    "＼": "\\",
    "］": "]",
    "＾": "^",
    "＿": "_",
    "｀": "`",
    "ａ": "a",
    "ｂ": "b",
    "ｃ": "c",
    "ｄ": "d",
    "ｅ": "e",
    "ｆ": "f",
    "ｇ": "g",
    "ｈ": "h",
    "ｉ": "i",
    "ｊ": "j",
    "ｋ": "k",
    "ｌ": "l",
    "ｍ": "m",
    "ｎ": "n",
    "ｏ": "o",
    "ｐ": "p",
    "ｑ": "q",
    "ｒ": "r",
    "ｓ": "s",
    "ｔ": "t",
    "ｕ": "u",
    "ｖ": "v",
    "ｗ": "w",
    "ｘ": "x",
    "ｙ": "y",
    "ｚ": "z",
    "｛": "{",
    "｜": "|",
    "｝": "}",
    "～": "~",
}
QJ2BJ_TRANSFORM = str.maketrans("".join(QJ2BJ.keys()), "".join(QJ2BJ.values()), "")


# 2013 China National Standard: https://zh.wikipedia.org/wiki/通用规范汉字表, raw resources:
#   https://github.com/mozillazg/pinyin-data/blob/master/kMandarin_8105.txt with 8105 chinese chars in total
CN_CHARS_COMMON = (
    "一丁七万丈三上下不与丏丐丑专且丕世丘丙业丛东丝丞丢两严丧个丫中丰串临丸丹为主丽举"
    "乂乃久么义之乌乍乎乏乐乒乓乔乖乘乙乜九乞也习乡书乩买乱乳乸乾了予争事二亍于亏云互"
    "亓五井亘亚些亟亡亢交亥亦产亨亩享京亭亮亲亳亵亶亸亹人亿什仁仂仃仄仅仆仇仉今介仍从"
    "仑仓仔仕他仗付仙仝仞仟仡代令以仨仪仫们仰仲仳仵件价任份仿企伈伉伊伋伍伎伏伐休众优"
    "伙会伛伞伟传伢伣伤伥伦伧伪伫伭伯估伲伴伶伸伺似伽伾佁佃但位低住佐佑体何佖佗佘余佚"
    "佛作佝佞佟你佣佤佥佩佬佯佰佳佴佶佸佺佻佼佽佾使侁侂侃侄侈侉例侍侏侑侔侗侘供依侠侣"
    "侥侦侧侨侩侪侬侮侯侴侵侹便促俄俅俊俍俎俏俐俑俗俘俙俚俜保俞俟信俣俦俨俩俪俫俭修俯"
    "俱俳俵俶俸俺俾倌倍倏倒倓倔倕倘候倚倜倞借倡倥倦倧倨倩倪倬倭倮倴债倻值倾偁偃假偈偌"
    "偎偏偓偕做停偡健偬偭偰偲偶偷偻偾偿傀傃傅傈傉傍傒傕傣傥傧储傩催傲傺傻僇僎像僔僖僚"
    "僦僧僬僭僮僰僳僵僻儆儇儋儒儡儦儳儴儿兀允元兄充兆先光克免兑兔兕兖党兜兢入全八公六"
    "兮兰共关兴兵其具典兹养兼兽冀冁内冈冉册再冏冒冔冕冗写军农冠冢冤冥冬冮冯冰冱冲决况"
    "冶冷冻冼冽净凄准凇凉凋凌减凑凓凘凛凝几凡凤凫凭凯凰凳凶凸凹出击凼函凿刀刁刃分切刈"
    "刊刍刎刑划刖列刘则刚创初删判刨利别刬刭刮到刳制刷券刹刺刻刽刿剀剁剂剃剅削剋剌前剐"
    "剑剔剕剖剜剞剟剡剥剧剩剪副割剽剿劁劂劄劈劐劓力劝办功加务劢劣动助努劫劬劭励劲劳劼"
    "劾势勃勇勉勋勍勐勒勔勖勘勚募勠勤勰勺勾勿匀包匆匈匍匏匐匕化北匙匜匝匠匡匣匦匪匮匹"
    "区医匼匾匿十千卅升午卉半华协卑卒卓单卖南博卜卞卟占卡卢卣卤卦卧卫卬卮卯印危即却卵"
    "卷卸卺卿厂厄厅历厉压厌厍厕厖厘厚厝原厢厣厥厦厨厩厮去厾县叁参叆叇又叉及友双反发叔"
    "叕取受变叙叚叛叟叠口古句另叨叩只叫召叭叮可台叱史右叵叶号司叹叻叼叽吁吃各吆合吉吊"
    "同名后吏吐向吒吓吕吖吗君吝吞吟吠吡吣否吧吨吩含听吭吮启吱吲吴吵吸吹吻吼吽吾呀呃呆"
    "呇呈告呋呐呒呓呔呕呖呗员呙呛呜呢呣呤呦周呱呲味呵呶呷呸呻呼命咀咂咄咆咇咉咋和咍咎"
    "咏咐咒咔咕咖咙咚咛咝咡咣咤咥咦咧咨咩咪咫咬咯咱咳咴咸咺咻咽咿哀品哂哃哄哆哇哈哉哌"
    "响哎哏哐哑哒哓哔哕哗哙哚哝哞哟哢哥哦哧哨哩哪哭哮哱哲哳哺哼哽哿唁唆唇唉唏唐唑唔唛"
    "唝唠唢唣唤唧唪唬售唯唰唱唳唵唷唼唾唿啁啃啄商啉啊啐啕啖啜啡啤啥啦啧啪啫啬啭啮啰啴"
    "啵啶啷啸啻啼啾喀喁喂喃善喆喇喈喉喊喋喏喑喔喘喙喜喝喟喤喧喱喳喵喷喹喻喽喾嗄嗅嗉嗌"
    "嗍嗐嗑嗒嗓嗔嗖嗜嗝嗞嗟嗡嗣嗤嗥嗦嗨嗪嗫嗬嗯嗲嗳嗵嗷嗽嗾嘀嘁嘈嘉嘌嘎嘏嘘嘚嘛嘞嘟嘡"
    "嘣嘤嘧嘬嘭嘱嘲嘴嘶嘹嘻嘿噀噂噇噌噍噎噔噗噘噙噜噢噤器噩噪噫噬噱噶噻噼嚄嚅嚆嚎嚏嚓"
    "嚚嚣嚭嚯嚷嚼囊囔囚四回囟因囡团囤囫园困囱围囵囷囹固国图囿圃圄圆圈圉圊圌圐圙圜土圢"
    "圣在圩圪圫圬圭圮圯地圲圳圹场圻圾址坂均坉坊坋坌坍坎坏坐坑坒块坚坛坜坝坞坟坠坡坤坥"
    "坦坨坩坪坫坬坭坯坰坳坷坻坼坽垂垃垄垆垈型垌垍垎垏垒垓垕垙垚垛垞垟垠垡垢垣垤垦垧垩"
    "垫垭垮垯垱垲垴垵垸垺垾垿埂埃埆埇埋埌城埏埒埔埕埗埘埙埚埝域埠埤埪埫埭埯埴埵埸培基"
    "埼埽堂堃堆堇堉堋堌堍堎堐堑堕堙堞堠堡堤堧堨堪堰堲堵堼堽堾塄塅塆塌塍塑塔塘塝塞塥填"
    "塬塱塾墀墁境墅墈墉墐墒墓墕墘墙墚增墟墡墣墦墨墩墼壁壅壑壕壤士壬壮声壳壶壸壹处备复"
    "夏夐夔夕外夙多夜够夤夥大天太夫夬夭央夯失头夷夸夹夺夼奁奂奄奇奈奉奋奎奏契奓奔奕奖"
    "套奘奚奠奡奢奥奭女奴奶奸她好妁如妃妄妆妇妈妊妍妒妓妖妗妘妙妞妣妤妥妧妨妩妪妫妭妮"
    "妯妲妹妻妾姆姈姊始姐姑姒姓委姗姘姚姜姝姞姣姤姥姨姬姮姱姶姹姻姽姿娀威娃娄娅娆娇娈"
    "娉娌娑娓娘娜娟娠娣娥娩娱娲娴娵娶娼婀婆婉婊婌婍婕婘婚婞婠婢婤婧婪婫婳婴婵婶婷婺婻"
    "婼婿媂媄媆媒媓媖媚媛媞媪媭媱媲媳媵媸媾嫁嫂嫄嫉嫌嫒嫔嫕嫖嫘嫚嫜嫠嫡嫣嫦嫩嫪嫫嫭嫱"
    "嫽嬉嬖嬗嬛嬥嬬嬴嬷嬿孀孅子孑孓孔孕孖字存孙孚孛孜孝孟孢季孤孥学孩孪孬孰孱孳孵孺孽"
    "宁它宄宅宇守安宋完宏宓宕宗官宙定宛宜宝实宠审客宣室宥宦宧宪宫宬宰害宴宵家宸容宽宾"
    "宿寁寂寄寅密寇富寐寒寓寝寞察寡寤寥寨寮寰寸对寺寻导寿封射将尉尊小少尔尕尖尘尚尜尝"
    "尢尤尥尧尨尪尬就尴尸尹尺尻尼尽尾尿局屁层屃居屈屉届屋屎屏屐屑展屙属屠屡屣履屦屯山"
    "屹屺屼屾屿岁岂岈岊岌岍岐岑岔岖岗岘岙岚岛岜岞岠岢岣岨岩岫岬岭岱岳岵岷岸岽岿峁峂峃"
    "峄峋峒峗峘峙峛峡峣峤峥峦峧峨峪峭峰峱峻峿崀崁崂崃崄崆崇崌崎崒崔崖崚崛崞崟崡崤崦崧"
    "崩崭崮崴崶崽崾崿嵁嵅嵇嵊嵋嵌嵎嵖嵘嵚嵛嵝嵩嵫嵬嵯嵲嵴嶂嶅嶍嶒嶓嶙嶝嶟嶦嶲嶷巅巇巉"
    "巍川州巡巢工左巧巨巩巫差巯己已巳巴巷巽巾币市布帅帆师希帏帐帑帔帕帖帘帙帚帛帜帝帡"
    "带帧帨席帮帱帷常帻帼帽幂幄幅幌幔幕幖幛幞幡幢幪干平年并幸幺幻幼幽广庄庆庇床庋序庐"
    "庑库应底庖店庙庚府庞废庠庤庥度座庭庱庳庵庶康庸庹庼庾廆廉廊廋廑廒廓廖廙廛廨廪延廷"
    "建廿开弁异弃弄弆弇弈弊弋式弑弓引弗弘弛弟张弢弥弦弧弨弩弭弯弱弶弸弹强弼彀归当录彖"
    "彗彘彝彟形彤彦彧彩彪彬彭彰影彳彷役彻彼往征徂径待徇很徉徊律徐徒徕得徘徙徛徜御徨循"
    "徭微徵德徼徽心必忆忉忌忍忏忐忑忒忖志忘忙忝忞忠忡忤忧忪快忭忮忱忳念忸忺忻忽忾忿怀"
    "态怂怃怄怅怆怊怍怎怏怒怔怕怖怙怛怜思怠怡急怦性怨怩怪怫怯怵总怼怿恁恂恃恋恍恐恒恓"
    "恔恕恙恚恝恢恣恤恧恨恩恪恫恬恭息恰恳恶恸恹恺恻恼恽恿悃悄悆悈悉悌悍悒悔悖悚悛悝悟"
    "悠悢患悦您悫悬悭悯悰悱悲悴悸悻悼情惆惇惊惋惎惑惔惕惘惙惚惛惜惝惟惠惦惧惨惩惫惬惭"
    "惮惯惰想惴惶惹惺愀愁愃愆愈愉愍愎意愐愔愕愚感愠愣愤愦愧愫愭愿慆慈慊慌慎慑慕慝慢慥"
    "慧慨慬慭慰慵慷憋憎憔憕憙憧憨憩憬憭憷憺憾懂懈懊懋懑懒懔懦懵懿戆戈戊戋戌戍戎戏成我"
    "戒戕或戗战戚戛戟戡戢戣戤戥截戬戭戮戳戴户戽戾房所扁扂扃扅扆扇扈扉扊手才扎扑扒打扔"
    "托扛扞扣扦执扩扪扫扬扭扮扯扰扳扶批扺扼扽找承技抃抄抉把抑抒抓抔投抖抗折抚抛抟抠抡"
    "抢护报抨披抬抱抵抹抻押抽抿拂拃拄担拆拇拈拉拊拌拍拎拐拒拓拔拖拗拘拙招拜拟拢拣拤拥"
    "拦拧拨择括拭拮拯拱拳拴拶拷拼拽拾拿持挂指挈按挎挑挓挖挚挛挝挞挟挠挡挣挤挥挦挨挪挫"
    "振挲挹挺挽捂捃捅捆捉捋捌捍捎捏捐捕捞损捡换捣捧捩捭据捯捶捷捺捻捽掀掂掇授掉掊掌掎"
    "掏掐排掖掘掞掠探掣接控推掩措掬掭掮掰掳掴掷掸掺掼掾揄揆揉揍描提插揕揖揠握揣揩揪揭"
    "揳援揶揸揽揿搀搁搂搅搋搌搏搐搒搓搔搛搜搞搠搡搦搪搬搭搴携搽摁摄摅摆摇摈摊摏摒摔摘"
    "摛摞摧摩摭摴摸摹摽撂撄撅撇撑撒撕撖撙撞撤撩撬播撮撰撵撷撸撺撼擀擂擅操擎擐擒擘擞擢"
    "擤擦擿攀攉攒攘攥攫攮支收攸改攻攽放政故效敉敌敏救敔敕敖教敛敝敞敢散敦敩敫敬数敲整"
    "敷文斋斌斐斑斓斗料斛斜斝斟斠斡斤斥斧斩斫断斯新斶方於施旁旃旄旅旆旋旌旎族旐旒旖旗"
    "旞无既日旦旧旨早旬旭旮旯旰旱旴旵时旷旸旺旻旿昀昂昃昄昆昇昈昉昊昌明昏昒易昔昕昙昝"
    "星映昡昣昤春昧昨昪昫昭是昱昳昴昵昶昺昼昽显晁晃晅晊晋晌晏晐晒晓晔晕晖晗晙晚晞晟晡"
    "晢晤晦晨晪晫普景晰晱晴晶晷智晾暂暄暅暇暌暑暕暖暗暝暧暨暮暲暴暵暶暹暾暿曈曌曙曛曜"
    "曝曦曩曰曲曳更曷曹曼曾替最月有朋服朏朐朓朔朕朗望朝期朦木未末本札术朱朳朴朵朸机朽"
    "杀杂权杄杆杈杉杌李杏材村杓杕杖杙杜杞束杠条来杧杨杩杪杭杯杰杲杳杵杷杻杼松板极构枅"
    "枇枉枋枍析枕林枘枚果枝枞枢枣枥枧枨枪枫枭枯枰枲枳枵架枷枸枹柁柃柄柈柊柏某柑柒染柔"
    "柖柘柙柚柜柝柞柠柢查柩柬柯柰柱柳柴柷柽柿栀栅标栈栉栊栋栌栎栏栐树栒栓栖栗栝栟校栩"
    "株栲栳栴样核根栻格栽栾桀桁桂桃桄桅框案桉桊桌桎桐桑桓桔桕桠桡桢档桤桥桦桧桨桩桫桯"
    "桲桴桶桷桹梁梃梅梆梌梏梓梗梠梢梣梦梧梨梭梯械梳梴梵梼梽梾梿检棁棂棉棋棍棐棒棓棕棘"
    "棚棠棣棤棨棪棫棬森棰棱棵棹棺棻棼棽椀椁椅椆椋植椎椐椑椒椓椟椠椤椪椭椰椴椸椹椽椿楂"
    "楒楔楗楙楚楝楞楠楣楦楩楪楫楮楯楷楸楹楼概榃榄榅榆榇榈榉榍榑榔榕榖榛榜榧榨榫榭榰榱"
    "榴榷榻槁槃槊槌槎槐槔槚槛槜槟槠槭槱槲槽槿樊樗樘樟模樨横樯樱樵樽樾橄橇橐橑橘橙橛橞"
    "橡橥橦橱橹橼檀檄檎檐檑檗檞檠檩檫檬櫆欂欠次欢欣欤欧欲欸欹欺欻款歃歅歆歇歉歌歙止正"
    "此步武歧歪歹死歼殁殂殃殄殆殇殉殊残殍殒殓殖殚殛殡殣殪殳殴段殷殿毁毂毅毋毌母每毐毒"
    "毓比毕毖毗毙毛毡毪毫毯毳毵毹毽氅氆氇氍氏氐民氓气氕氖氘氙氚氛氟氡氢氤氦氧氨氩氪氮"
    "氯氰氲水永氾氿汀汁求汆汇汈汉汊汋汐汔汕汗汛汜汝汞江池污汤汧汨汩汪汫汭汰汲汴汶汹汽"
    "汾沁沂沃沄沅沆沇沈沉沌沏沐沓沔沘沙沚沛沟没沣沤沥沦沧沨沩沪沫沭沮沱河沸油沺治沼沽"
    "沾沿泂泃泄泅泇泉泊泌泐泓泔法泖泗泙泚泛泜泞泠泡波泣泥注泪泫泮泯泰泱泳泵泷泸泺泻泼"
    "泽泾洁洄洇洈洋洌洎洑洒洓洗洘洙洚洛洞洢洣津洧洨洪洫洭洮洱洲洳洴洵洸洹洺活洼洽派洿"
    "流浃浅浆浇浈浉浊测浍济浏浐浑浒浓浔浕浙浚浛浜浞浟浠浡浣浥浦浩浪浬浭浮浯浰浲浴海浸"
    "浼涂涄涅消涉涌涍涎涐涑涓涔涕涘涛涝涞涟涠涡涢涣涤润涧涨涩涪涫涮涯液涴涵涸涿淀淄淅"
    "淆淇淋淌淏淑淖淘淙淜淝淞淟淠淡淤淦淫淬淮淯深淳淴混淹添淼清渊渌渍渎渐渑渔渗渚渝渟"
    "渠渡渣渤渥温渫渭港渰渲渴游渺渼湃湄湉湍湎湑湓湔湖湘湛湜湝湟湣湫湮湲湴湾湿溁溃溅溆"
    "溇溉溍溏源溘溚溜溞溟溠溢溥溦溧溪溯溱溲溴溵溶溷溹溺溻溽滁滂滃滆滇滉滋滍滏滑滓滔滕"
    "滗滘滚滞滟滠满滢滤滥滦滧滨滩滪滫滴滹漂漆漈漉漋漏漓演漕漖漠漤漦漩漪漫漭漯漱漳漴漶"
    "漷漹漻漼漾潆潇潋潍潏潖潘潜潞潟潢潦潩潭潮潲潴潵潸潺潼潽潾澂澄澈澉澌澍澎澛澜澡澥澧"
    "澪澭澳澴澶澹澼澽激濂濉濋濑濒濞濠濡濩濮濯瀌瀍瀑瀔瀚瀛瀣瀱瀵瀹瀼灈灌灏灞火灭灯灰灵"
    "灶灸灼灾灿炀炅炆炉炊炌炎炒炔炕炖炘炙炜炝炟炣炫炬炭炮炯炱炳炷炸点炻炼炽烀烁烂烃烈"
    "烊烔烘烙烛烜烝烟烠烤烦烧烨烩烫烬热烯烶烷烹烺烻烽焆焉焊焌焐焓焕焖焗焘焙焚焜焞焦焯"
    "焰焱然煁煃煅煊煋煌煎煓煜煞煟煤煦照煨煮煲煳煴煸煺煽熄熇熊熏熔熘熙熛熜熟熠熥熨熬熵"
    "熹熻燃燊燋燎燏燔燕燚燠燥燧燮燹爆爇爔爚爝爟爨爪爬爰爱爵父爷爸爹爻爽爿牁牂片版牌牍"
    "牒牖牙牚牛牝牟牡牢牤牥牦牧物牮牯牲牵特牺牻牾牿犀犁犄犇犊犋犍犏犒犟犨犬犯犰犴状犷"
    "犸犹狁狂狃狄狈狉狍狎狐狒狗狙狝狞狠狡狨狩独狭狮狯狰狱狲狳狴狷狸狺狻狼猁猃猄猇猊猎"
    "猕猖猗猛猜猝猞猡猢猥猩猪猫猬献猯猰猱猴猷猹猺猾猿獍獐獒獗獠獬獭獯獴獾玃玄率玉王玎"
    "玑玒玓玕玖玘玙玚玛玞玟玠玡玢玤玥玦玩玫玭玮环现玱玲玳玶玷玹玺玻玼玿珀珂珅珇珈珉珊"
    "珋珌珍珏珐珑珒珕珖珙珛珝珞珠珢珣珥珦珧珩珪珫班珰珲珵珷珸珹珺珽琀球琄琅理琇琈琉琊"
    "琎琏琐琔琚琛琟琡琢琤琥琦琨琪琫琬琭琮琯琰琲琳琴琵琶琼瑀瑁瑂瑃瑄瑅瑆瑑瑓瑔瑕瑖瑗瑙"
    "瑚瑛瑜瑝瑞瑟瑢瑧瑨瑬瑭瑰瑱瑳瑶瑷瑾璀璁璃璆璇璈璋璎璐璒璘璜璞璟璠璥璧璨璩璪璬璮璱"
    "璲璺瓀瓒瓖瓘瓜瓞瓠瓢瓣瓤瓦瓮瓯瓴瓶瓷瓻瓿甄甍甏甑甓甗甘甚甜生甡甥甦用甩甪甫甬甭甯"
    "田由甲申电男甸町画甾畀畅畈畋界畎畏畔畖留畚畛畜畤略畦番畬畯畲畴畸畹畿疁疃疆疍疏疐"
    "疑疔疖疗疙疚疝疟疠疡疢疣疤疥疫疬疭疮疯疰疱疲疳疴疵疸疹疼疽疾痂痃痄病症痈痉痊痍痒"
    "痓痔痕痘痛痞痢痣痤痦痧痨痪痫痰痱痴痹痼痿瘀瘁瘃瘅瘆瘊瘌瘐瘕瘗瘘瘙瘛瘟瘠瘢瘤瘥瘦瘩"
    "瘪瘫瘭瘰瘳瘴瘵瘸瘼瘾瘿癀癃癌癍癔癖癗癜癞癣癫癯癸登白百癿皂的皆皇皈皋皎皑皓皕皖皙"
    "皛皞皤皦皭皮皱皲皴皿盂盅盆盈盉益盍盎盏盐监盒盔盖盗盘盛盟盥盦目盯盱盲直盷相盹盼盾"
    "省眄眇眈眉眊看眍眙眚真眠眢眦眨眩眬眭眯眵眶眷眸眺眼着睁睃睄睇睎睐睑睚睛睡睢督睥睦"
    "睨睫睬睹睽睾睿瞀瞄瞅瞋瞌瞍瞎瞑瞒瞟瞠瞢瞥瞧瞩瞪瞫瞬瞭瞰瞳瞵瞻瞽瞿矍矗矛矜矞矢矣知"
    "矧矩矫矬短矮矰石矶矸矻矼矾矿砀码砂砄砆砉砌砍砑砒研砖砗砘砚砜砝砟砠砣砥砧砫砬砭砮"
    "砰破砵砷砸砹砺砻砼砾础硁硅硇硊硌硍硎硐硒硔硕硖硗硙硚硝硪硫硬硭确硼硿碃碇碈碉碌碍"
    "碎碏碑碓碗碘碚碛碜碟碡碣碥碧碨碰碱碲碳碴碶碹碾磁磅磉磊磋磏磐磔磕磙磜磡磨磬磲磴磷"
    "磹磻礁礅礌礓礞礴礵示礼社祀祁祃祆祇祈祉祊祋祎祏祐祓祕祖祗祚祛祜祝神祟祠祢祥祧票祭"
    "祯祲祷祸祺祼祾禀禁禄禅禊禋福禒禔禘禚禛禤禧禳禹禺离禽禾秀私秃秆秉秋种科秒秕秘租秣"
    "秤秦秧秩秫秬秭积称秸移秽秾稀稂稃稆程稌稍税稑稔稗稙稚稞稠稣稳稷稹稻稼稽稿穄穆穑穗"
    "穙穜穟穰穴究穷穸穹空穿窀突窃窄窅窈窊窍窎窑窒窕窖窗窘窜窝窟窠窣窥窦窨窬窭窳窸窿立"
    "竑竖竘站竞竟章竣童竦竫竭端竹竺竽竿笃笄笆笈笊笋笏笑笔笕笙笛笞笠笤笥符笨笪笫第笮笯"
    "笱笳笸笺笼笾筀筅筇等筋筌筏筐筑筒答策筘筚筛筜筝筠筢筤筥筦筮筱筲筵筶筷筹筻筼签简箅"
    "箍箐箓箔箕箖算箜管箢箦箧箨箩箪箫箬箭箱箴箸篁篆篇篌篑篓篙篚篝篡篥篦篪篮篯篱篷篼篾"
    "簃簇簉簋簌簏簕簖簝簟簠簧簪簰簸簿籀籁籍籥米籴类籼籽粉粑粒粕粗粘粜粝粞粟粢粤粥粪粮"
    "粱粲粳粹粼粽精粿糁糅糇糈糊糌糍糒糕糖糗糙糜糟糠糨糯糵系紊素索紧紫累絜絮絷綦綮縠縢"
    "縻繁繄繇纂纛纠纡红纣纤纥约级纨纩纪纫纬纭纮纯纰纱纲纳纴纵纶纷纸纹纺纻纼纽纾线绀绁"
    "绂练组绅细织终绉绊绋绌绍绎经绐绑绒结绔绕绖绗绘给绚绛络绝绞统绠绡绢绣绤绥绦继绨绩"
    "绪绫续绮绯绰绱绲绳维绵绶绷绸绹绺绻综绽绾绿缀缁缂缃缄缅缆缇缈缉缊缌缎缐缑缒缓缔缕"
    "编缗缘缙缚缛缜缝缞缟缠缡缢缣缤缥缦缧缨缩缪缫缬缭缮缯缰缱缲缳缴缵缶缸缺罂罄罅罍罐"
    "网罔罕罗罘罚罟罡罢罨罩罪置罱署罴罶罹罽罾羁羊羌美羑羓羔羕羖羚羝羞羟羡群羧羯羰羱羲"
    "羸羹羼羽羿翀翁翂翃翅翈翊翌翎翔翕翘翙翚翛翟翠翡翥翦翩翮翯翰翱翳翷翻翼翾耀老考耄者"
    "耆耇耋而耍耏耐耑耒耔耕耖耗耘耙耜耠耢耤耥耦耧耨耩耪耰耱耳耵耶耷耸耻耽耿聂聃聆聊聋"
    "职聍聒联聘聚聩聪聱聿肃肄肆肇肉肋肌肓肖肘肚肛肝肟肠股肢肤肥肩肪肫肭肮肯肱育肴肷肸"
    "肺肼肽肾肿胀胁胂胃胄胆胈背胍胎胖胗胙胚胛胜胝胞胠胡胣胤胥胧胨胩胪胫胬胭胯胰胱胲胳"
    "胴胶胸胺胼能脂脆脉脊脍脎脏脐脑脒脓脔脖脘脚脞脟脩脬脯脱脲脶脸脾脿腆腈腊腋腌腐腑腒"
    "腓腔腕腘腙腚腠腥腧腨腩腭腮腯腰腱腴腹腺腻腼腽腾腿膀膂膈膊膏膑膘膙膛膜膝膦膨膳膺膻"
    "臀臂臃臆臊臌臑臜臣臧自臬臭至致臻臼臾舀舁舂舄舅舆舌舍舐舒舔舛舜舞舟舠舢舣舥航舫般"
    "舭舯舰舱舲舳舴舵舶舷舸船舻舾艄艅艇艉艋艎艏艘艚艟艨艮良艰色艳艴艺艽艾艿节芃芄芈芊"
    "芋芍芎芏芑芒芗芘芙芜芝芟芠芡芣芤芥芦芨芩芪芫芬芭芮芯芰花芳芴芷芸芹芼芽芾苁苄苇苈"
    "苉苊苋苌苍苎苏苑苒苓苔苕苗苘苛苜苞苟苠苡苣苤若苦苧苫苯英苴苷苹苻苾茀茁茂范茄茅茆"
    "茈茉茋茌茎茏茑茓茔茕茗茚茛茜茝茧茨茫茬茭茯茱茳茴茵茶茸茹茺茼茽荀荁荃荄荆荇草荏荐"
    "荑荒荓荔荖荙荚荛荜荞荟荠荡荣荤荥荦荧荨荩荪荫荬荭荮药荷荸荻荼荽莅莆莉莎莒莓莘莙莛"
    "莜莝莞莠莨莩莪莫莰莱莲莳莴莶获莸莹莺莼莽莿菀菁菂菅菇菉菊菌菍菏菔菖菘菜菝菟菠菡菥"
    "菩菪菰菱菲菹菼菽萁萃萄萆萋萌萍萎萏萑萘萚萜萝萣萤营萦萧萨萩萱萳萸萹萼落葆葎葑葖著"
    "葙葚葛葜葡董葩葫葬葭葰葱葳葴葵葶葸葺蒂蒄蒇蒈蒉蒋蒌蒎蒐蒗蒙蒜蒟蒡蒨蒯蒱蒲蒴蒸蒹蒺"
    "蒻蒽蒿蓁蓂蓄蓇蓉蓊蓍蓏蓐蓑蓓蓖蓝蓟蓠蓢蓣蓥蓦蓬蓰蓼蓿蔀蔃蔈蔊蔌蔑蔓蔗蔚蔟蔡蔫蔬蔷"
    "蔸蔹蔺蔻蔼蔽蕃蕈蕉蕊蕖蕗蕙蕞蕤蕨蕰蕲蕴蕹蕺蕻蕾薁薄薅薇薏薛薜薢薤薨薪薮薯薰薳薷薸"
    "薹薿藁藉藏藐藓藕藜藟藠藤藦藨藩藻藿蘅蘑蘖蘘蘧蘩蘸蘼虎虏虐虑虒虓虔虚虞虢虤虫虬虮虱"
    "虷虸虹虺虻虼虽虾虿蚀蚁蚂蚄蚆蚊蚋蚌蚍蚓蚕蚜蚝蚣蚤蚧蚨蚩蚪蚬蚯蚰蚱蚲蚴蚶蚺蛀蛃蛄蛆"
    "蛇蛉蛊蛋蛎蛏蛐蛑蛔蛘蛙蛛蛞蛟蛤蛩蛭蛮蛰蛱蛲蛳蛴蛸蛹蛾蜀蜂蜃蜇蜈蜉蜊蜍蜎蜐蜒蜓蜕蜗"
    "蜘蜚蜜蜞蜡蜢蜣蜥蜩蜮蜱蜴蜷蜻蜾蜿蝇蝈蝉蝌蝎蝓蝗蝘蝙蝠蝣蝤蝥蝮蝰蝲蝴蝶蝻蝼蝽蝾螂螃"
    "螅螈螋融螗螟螠螣螨螫螬螭螯螱螳螵螺螽蟀蟆蟊蟋蟏蟑蟒蟛蟠蟥蟪蟫蟮蟹蟾蠃蠊蠋蠓蠕蠖蠡"
    "蠢蠲蠹蠼血衃衄衅行衍衎衒衔街衙衠衡衢衣补表衩衫衬衮衰衲衷衽衾衿袁袂袄袅袆袈袋袍袒"
    "袖袗袜袢袤袪被袭袯袱袷袼裁裂装裆裈裉裎裒裔裕裘裙裛裟裢裣裤裥裨裰裱裳裴裸裹裼裾褂"
    "褊褐褒褓褕褙褚褛褟褡褥褪褫褯褰褴褶襁襄襕襚襜襞襟襦襫襻西要覃覆见观觃规觅视觇览觉"
    "觊觋觌觎觏觐觑角觖觚觜觞觟解觥触觫觭觯觱觳觿言訄訇訚訾詈詟詹誉誊誓謇警譬计订讣认"
    "讥讦讧讨让讪讫训议讯记讱讲讳讴讵讶讷许讹论讻讼讽设访诀证诂诃评诅识诇诈诉诊诋诌词"
    "诎诏诐译诒诓诔试诖诗诘诙诚诛诜话诞诟诠诡询诣诤该详诧诨诩诫诬语诮误诰诱诲诳说诵请"
    "诸诹诺读诼诽课诿谀谁谂调谄谅谆谇谈谊谋谌谍谎谏谐谑谒谓谔谕谖谗谙谚谛谜谝谞谟谠谡"
    "谢谣谤谥谦谧谨谩谪谫谬谭谮谯谰谱谲谳谴谵谶谷谼谿豁豆豇豉豌豕豚象豢豨豪豫豮豳豸豹"
    "豺貂貅貆貉貊貌貔貘贝贞负贡财责贤败账货质贩贪贫贬购贮贯贰贱贲贳贴贵贶贷贸费贺贻贼"
    "贽贾贿赀赁赂赃资赅赆赇赈赉赊赋赌赍赎赏赐赑赒赓赔赕赖赗赘赙赚赛赜赝赞赟赠赡赢赣赤"
    "赦赧赪赫赭走赳赴赵赶起趁趄超越趋趑趔趟趣趯趱足趴趵趸趺趼趾趿跂跃跄跆跋跌跎跏跐跑"
    "跖跗跚跛距跞跟跣跤跨跪跬路跱跳践跶跷跸跹跺跻跽踅踉踊踌踏踒踔踝踞踟踢踣踦踩踪踬踮"
    "踯踱踵踶踹踺踽蹀蹁蹂蹄蹅蹇蹈蹉蹊蹋蹐蹑蹒蹙蹚蹜蹢蹦蹩蹬蹭蹯蹰蹲蹴蹶蹼蹽蹾蹿躁躅躇"
    "躏躐躔躜躞身躬躯躲躺车轧轨轩轪轫转轭轮软轰轱轲轳轴轵轶轷轸轹轺轻轼载轾轿辀辁辂较"
    "辄辅辆辇辈辉辊辋辌辍辎辏辐辑辒输辔辕辖辗辘辙辚辛辜辞辟辣辨辩辫辰辱边辽达辿迁迂迄"
    "迅过迈迎运近迓返迕还这进远违连迟迢迤迥迦迨迩迪迫迭迮述迳迷迸迹迺追退送适逃逄逅逆"
    "选逊逋逍透逐逑递途逖逗通逛逝逞速造逡逢逦逭逮逯逴逵逶逸逻逼逾遁遂遄遆遇遍遏遐遑遒"
    "道遗遘遛遢遣遥遨遭遮遴遵遹遽避邀邂邃邈邋邑邓邕邗邘邙邛邝邠邡邢那邦邨邪邬邮邯邰邱"
    "邲邳邴邵邶邸邹邺邻邽邾邿郁郃郄郅郇郈郊郎郏郐郑郓郗郚郛郜郝郡郢郤郦郧部郪郫郭郯郴"
    "郸都郾郿鄀鄂鄃鄄鄅鄌鄑鄗鄘鄙鄚鄜鄞鄠鄢鄣鄫鄯鄱鄹酂酃酅酆酉酊酋酌配酎酏酐酒酗酚酝"
    "酞酡酢酣酤酥酦酩酪酬酮酯酰酱酲酴酵酶酷酸酹酺酽酾酿醅醇醉醋醌醍醐醑醒醚醛醢醨醪醭"
    "醮醯醴醵醺醾采釉释里重野量釐金釜鉴銎銮鋆鋈錾鍪鎏鏊鏖鐾鑫钆钇针钉钊钋钌钍钎钏钐钒"
    "钓钔钕钖钗钘钙钚钛钜钝钞钟钠钡钢钣钤钥钦钧钨钩钪钫钬钭钮钯钰钱钲钳钴钵钷钹钺钻钼"
    "钽钾钿铀铁铂铃铄铅铆铈铉铊铋铌铍铎铏铐铑铒铕铖铗铘铙铚铛铜铝铞铟铠铡铢铣铤铥铧铨"
    "铩铪铫铬铭铮铯铰铱铲铳铴铵银铷铸铹铺铻铼铽链铿销锁锂锃锄锅锆锇锈锉锊锋锌锍锎锏锐"
    "锑锒锓锔锕锖锗锘错锚锛锜锝锞锟锡锢锣锤锥锦锧锨锩锪锫锬锭键锯锰锱锲锳锴锵锶锷锸锹"
    "锺锻锼锽锾锿镀镁镂镃镄镅镆镇镈镉镊镋镌镍镎镏镐镑镒镓镔镕镖镗镘镚镛镜镝镞镠镡镢镣"
    "镤镥镦镧镨镩镪镫镬镭镮镯镰镱镲镳镴镵镶长门闩闪闫闭问闯闰闱闲闳间闵闶闷闸闹闺闻闼"
    "闽闾闿阀阁阂阃阄阅阆阇阈阉阊阋阌阍阎阏阐阑阒阔阕阖阗阘阙阚阜队阡阪阮阱防阳阴阵阶"
    "阻阼阽阿陀陂附际陆陇陈陉陋陌降陎限陑陔陕陛陞陟陡院除陧陨险陪陬陲陴陵陶陷隃隅隆隈"
    "隋隍随隐隔隗隘隙障隧隩隰隳隶隹隺隼隽难雀雁雄雅集雇雉雊雌雍雎雏雒雕雠雨雩雪雯雱雳"
    "零雷雹雾需霁霄霅霆震霈霉霍霎霏霓霖霜霞霨霪霭霰露霸霹霾青靓靖静靛非靠靡面靥革靬靰"
    "靳靴靶靸靺靼靽靿鞁鞅鞋鞍鞑鞒鞔鞘鞠鞡鞣鞧鞨鞫鞬鞭鞮鞯鞲鞳鞴韂韦韧韨韩韪韫韬韭音韵"
    "韶页顶顷顸项顺须顼顽顾顿颀颁颂颃预颅领颇颈颉颊颋颌颍颎颏颐频颓颔颖颗题颙颚颛颜额"
    "颞颟颠颡颢颤颥颦颧风飏飐飑飒飓飔飕飗飘飙飞食飧飨餍餐餮饔饕饥饧饨饩饪饫饬饭饮饯饰"
    "饱饲饳饴饵饶饷饸饹饺饻饼饽饿馁馃馄馅馆馇馈馉馊馋馌馍馏馐馑馒馓馔馕首馗馘香馝馞馥"
    "馧馨马驭驮驯驰驱驲驳驴驵驶驷驸驹驺驻驼驽驾驿骀骁骂骃骄骅骆骇骈骉骊骋验骍骎骏骐骑"
    "骒骓骕骖骗骘骙骚骛骜骝骞骟骠骡骢骣骤骥骦骧骨骰骱骶骷骸骺骼髀髁髂髃髅髋髌髎髑髓高"
    "髡髢髦髫髭髯髹髻髽鬃鬈鬏鬒鬓鬘鬟鬣鬯鬲鬶鬷鬻鬼魁魂魃魄魅魆魇魈魉魋魍魏魑魔鱼鱽鱾"
    "鱿鲀鲁鲂鲃鲅鲆鲇鲈鲉鲊鲋鲌鲍鲎鲏鲐鲑鲒鲔鲕鲖鲗鲘鲙鲚鲛鲜鲝鲞鲟鲠鲡鲢鲣鲤鲥鲦鲧鲨"
    "鲩鲪鲫鲬鲭鲮鲯鲰鲱鲲鲳鲴鲵鲷鲸鲹鲺鲻鲼鲽鲾鲿鳀鳁鳂鳃鳄鳅鳇鳈鳉鳊鳌鳍鳎鳏鳐鳑鳒鳓"
    "鳔鳕鳖鳗鳘鳙鳚鳛鳜鳝鳞鳟鳠鳡鳢鳣鳤鸟鸠鸡鸢鸣鸤鸥鸦鸧鸨鸩鸪鸫鸬鸭鸮鸯鸰鸱鸲鸳鸵鸶"
    "鸷鸸鸹鸺鸻鸼鸽鸾鸿鹀鹁鹂鹃鹄鹅鹆鹇鹈鹉鹊鹋鹌鹍鹎鹏鹐鹑鹒鹔鹕鹖鹗鹘鹙鹚鹛鹜鹝鹞鹟"
    "鹠鹡鹢鹣鹤鹦鹧鹨鹩鹪鹫鹬鹭鹮鹯鹰鹱鹲鹳鹴鹾鹿麀麂麇麈麋麑麒麓麖麝麟麦麸麹麻麽麾黄"
    "黇黉黍黎黏黑黔默黛黜黝黟黠黡黢黥黧黩黪黯黹黻黼黾鼋鼍鼎鼐鼒鼓鼗鼙鼠鼢鼩鼫鼬鼯鼱鼷"
    "鼹鼻鼽鼾齁齇齉齐齑齿龀龁龂龃龄龅龆龇龈龉龊龋龌龙龚龛龟龠龢鿍鿎鿏㑇㑊㕮㘎㙍㙘㙦㛃"
    "㛚㛹㟃㠇㠓㤘㥄㧐㧑㧟㫰㬊㬎㬚㭎㭕㮾㰀㳇㳘㳚㴔㵐㶲㸆㸌㺄㻬㽏㿠䁖䂮䃅䃎䅟䌹䎃䎖䏝䏡"
    "䏲䐃䓖䓛䓨䓫䓬䗖䗛䗪䗴䜣䝙䢺䢼䣘䥽䦃䲟䲠䲢䴓䴔䴕䴖䴗䴘䴙䶮𠅤𠙶𠳐𡎚𡐓𣗋𣲗𣲘𣸣𤧛𤩽"
    "𤫉𥔲𥕢𥖨𥻗𦈡𦒍𦙶𦝼𦭜𦰡𧿹𨐈𨙸𨚕𨟠𨭉𨱇𨱏𨱑𨱔𨺙𩽾𩾃𩾌𪟝𪣻𪤗𪨰𪨶𪩘𪾢𫄧𫄨𫄷𫄸𫇭𫌀𫍣𫍯"
    "𫍲𫍽𫐄𫐐𫐓𫑡𫓧𫓯𫓶𫓹𫔍𫔎𫔶𫖮𫖯𫖳𫗧𫗴𫘜𫘝𫘦𫘧𫘨𫘪𫘬𫚕𫚖𫚭𫛭𫞩𫟅𫟦𫟹𫟼𫠆𫠊𫠜𫢸𫫇𫭟"
    "𫭢𫭼𫮃𫰛𫵷𫶇𫷷𫸩𬀩𬀪𬂩𬃊𬇕𬇙𬇹𬉼𬊈𬊤𬌗𬍛𬍡𬍤𬒈𬒔𬒗𬕂𬘓𬘘𬘡𬘩𬘫𬘬𬘭𬘯𬙂𬙊𬙋𬜬𬜯𬞟"
    "𬟁𬟽𬣙𬣞𬣡𬣳𬤇𬤊𬤝𬨂𬨎𬩽𬪩𬬩𬬭𬬮𬬱𬬸𬬹𬬻𬬿𬭁𬭊𬭎𬭚𬭛𬭤𬭩𬭬𬭯𬭳𬭶𬭸𬭼𬮱𬮿𬯀𬯎𬱖𬱟"
    "𬳵𬳶𬳽𬳿𬴂𬴃𬴊𬶋𬶍𬶏𬶐𬶟𬶠𬶨𬶭𬶮𬷕𬸘𬸚𬸣𬸦𬸪𬹼𬺈𬺓"
)
CN_CHARS_EXT = "吶诶屌囧飚屄"

CN_CHARS = CN_CHARS_COMMON + CN_CHARS_EXT
IN_CH_CHARS = {c: True for c in CN_CHARS}

EN_CHARS = string.ascii_letters + string.digits
IN_EN_CHARS = {c: True for c in EN_CHARS}

VALID_CHARS = CN_CHARS + EN_CHARS + " "
IN_VALID_CHARS = {c: True for c in VALID_CHARS}


# ================================================================================ #
#                                    basic class
# ================================================================================ #
class ChineseChar(object):
    """
    中文字符
    每个字符对应简体和繁体,
    e.g. 简体 = '负', 繁体 = '負'
    转换时可转换为简体或繁体
    """

    def __init__(self, simplified, traditional):
        self.simplified = simplified
        self.traditional = traditional
        # self.__repr__ = self.__str__

    def __str__(self):
        return self.simplified or self.traditional or None

    def __repr__(self):
        return self.__str__()


class ChineseNumberUnit(ChineseChar):
    """
    中文数字/数位字符
    每个字符除繁简体外还有一个额外的大写字符
    e.g. '陆' 和 '陸'
    """

    def __init__(self, power, simplified, traditional, big_s, big_t):
        super(ChineseNumberUnit, self).__init__(simplified, traditional)
        self.power = power
        self.big_s = big_s
        self.big_t = big_t

    def __str__(self):
        return "10^{}".format(self.power)

    @classmethod
    def create(cls, index, value, numbering_type=NUMBERING_TYPES[1], small_unit=False):
        if small_unit:
            return ChineseNumberUnit(
                power=index + 1, simplified=value[0], traditional=value[1], big_s=value[1], big_t=value[1]
            )
        elif numbering_type == NUMBERING_TYPES[0]:
            return ChineseNumberUnit(
                power=index + 8, simplified=value[0], traditional=value[1], big_s=value[0], big_t=value[1]
            )
        elif numbering_type == NUMBERING_TYPES[1]:
            return ChineseNumberUnit(
                power=(index + 2) * 4, simplified=value[0], traditional=value[1], big_s=value[0], big_t=value[1]
            )
        elif numbering_type == NUMBERING_TYPES[2]:
            return ChineseNumberUnit(
                power=pow(2, index + 3), simplified=value[0], traditional=value[1], big_s=value[0], big_t=value[1]
            )
        else:
            raise ValueError("Counting type should be in {0} ({1} provided).".format(NUMBERING_TYPES, numbering_type))


class ChineseNumberDigit(ChineseChar):
    """
    中文数字字符
    """

    def __init__(self, value, simplified, traditional, big_s, big_t, alt_s=None, alt_t=None):
        super(ChineseNumberDigit, self).__init__(simplified, traditional)
        self.value = value
        self.big_s = big_s
        self.big_t = big_t
        self.alt_s = alt_s
        self.alt_t = alt_t

    def __str__(self):
        return str(self.value)

    @classmethod
    def create(cls, i, v):
        return ChineseNumberDigit(i, v[0], v[1], v[2], v[3])


class ChineseMath(ChineseChar):
    """
    中文数位字符
    """

    def __init__(self, simplified, traditional, symbol, expression=None):
        super(ChineseMath, self).__init__(simplified, traditional)
        self.symbol = symbol
        self.expression = expression
        self.big_s = simplified
        self.big_t = traditional


CC, CNU, CND, CM = ChineseChar, ChineseNumberUnit, ChineseNumberDigit, ChineseMath


class NumberSystem(object):
    """
    中文数字系统
    """

    pass


class MathSymbol(object):
    """
    用于中文数字系统的数学符号 (繁/简体), e.g.
    positive = ['正', '正']
    negative = ['负', '負']
    point = ['点', '點']
    """

    def __init__(self, positive, negative, point):
        self.positive = positive
        self.negative = negative
        self.point = point

    def __iter__(self):
        for v in self.__dict__.values():
            yield v


# class OtherSymbol(object):
#     """
#     其他符号
#     """
#
#     def __init__(self, sil):
#         self.sil = sil
#
#     def __iter__(self):
#         for v in self.__dict__.values():
#             yield v


# ================================================================================ #
#                                    basic utils
# ================================================================================ #
def create_system(numbering_type=NUMBERING_TYPES[1]):
    """
    根据数字系统类型返回创建相应的数字系统，默认为 mid
    NUMBERING_TYPES = ['low', 'mid', 'high']: 中文数字系统类型
        low:  '兆' = '亿' * '十' = $10^{9}$,  '京' = '兆' * '十', etc.
        mid:  '兆' = '亿' * '万' = $10^{12}$, '京' = '兆' * '万', etc.
        high: '兆' = '亿' * '亿' = $10^{16}$, '京' = '兆' * '兆', etc.
    返回对应的数字系统
    """

    # chinese number units of '亿' and larger
    all_larger_units = zip(LARGER_CHINESE_NUMERING_UNITS_SIMPLIFIED, LARGER_CHINESE_NUMERING_UNITS_TRADITIONAL)
    larger_units = [CNU.create(i, v, numbering_type, False) for i, v in enumerate(all_larger_units)]
    # chinese number units of '十, 百, 千, 万'
    all_smaller_units = zip(SMALLER_CHINESE_NUMERING_UNITS_SIMPLIFIED, SMALLER_CHINESE_NUMERING_UNITS_TRADITIONAL)
    smaller_units = [CNU.create(i, v, small_unit=True) for i, v in enumerate(all_smaller_units)]
    # digis
    chinese_digis = zip(CHINESE_DIGIS, CHINESE_DIGIS, BIG_CHINESE_DIGIS_SIMPLIFIED, BIG_CHINESE_DIGIS_TRADITIONAL)
    digits = [CND.create(i, v) for i, v in enumerate(chinese_digis)]
    digits[0].alt_s, digits[0].alt_t = ZERO_ALT, ZERO_ALT
    digits[1].alt_s, digits[1].alt_t = ONE_ALT, ONE_ALT
    digits[2].alt_s, digits[2].alt_t = TWO_ALTS[0], TWO_ALTS[1]

    # symbols
    positive_cn = CM(POSITIVE[0], POSITIVE[1], "+", lambda x: x)
    negative_cn = CM(NEGATIVE[0], NEGATIVE[1], "-", lambda x: -x)
    point_cn = CM(POINT[0], POINT[1], ".", lambda x, y: float(str(x) + "." + str(y)))
    # sil_cn = CM(SIL[0], SIL[1], '-', lambda x, y: float(str(x) + '-' + str(y)))
    system = NumberSystem()
    system.units = smaller_units + larger_units
    system.digits = digits
    system.math = MathSymbol(positive_cn, negative_cn, point_cn)
    # system.symbols = OtherSymbol(sil_cn)
    return system


def chn2num(chinese_string, numbering_type=NUMBERING_TYPES[1]):
    def get_symbol(char, system):
        for u in system.units:
            if char in [u.traditional, u.simplified, u.big_s, u.big_t]:
                return u
        for d in system.digits:
            if char in [d.traditional, d.simplified, d.big_s, d.big_t, d.alt_s, d.alt_t]:
                return d
        for m in system.math:
            if char in [m.traditional, m.simplified]:
                return m

    def string2symbols(chinese_string, system):
        int_string, dec_string = chinese_string, ""
        for p in [system.math.point.simplified, system.math.point.traditional]:
            if p in chinese_string:
                int_string, dec_string = chinese_string.split(p)
                break
        return [get_symbol(c, system) for c in int_string], [get_symbol(c, system) for c in dec_string]

    def correct_symbols(integer_symbols, system):
        """
        一百八 to 一百八十
        一亿一千三百万 to 一亿 一千万 三百万
        """

        if integer_symbols and isinstance(integer_symbols[0], CNU):
            if integer_symbols[0].power == 1:
                integer_symbols = [system.digits[1]] + integer_symbols

        if len(integer_symbols) > 1:
            if isinstance(integer_symbols[-1], CND) and isinstance(integer_symbols[-2], CNU):
                integer_symbols.append(CNU(integer_symbols[-2].power - 1, None, None, None, None))

        result = []
        unit_count = 0
        for s in integer_symbols:
            if isinstance(s, CND):
                result.append(s)
                unit_count = 0
            elif isinstance(s, CNU):
                current_unit = CNU(s.power, None, None, None, None)
                unit_count += 1

            if unit_count == 1:
                result.append(current_unit)
            elif unit_count > 1:
                for i in range(len(result)):
                    if isinstance(result[-i - 1], CNU) and result[-i - 1].power < current_unit.power:
                        result[-i - 1] = CNU(result[-i - 1].power + current_unit.power, None, None, None, None)
        return result

    def compute_value(integer_symbols):
        """
        Compute the value.
        When current unit is larger than previous unit, current unit * all previous units will be used as all previous units.
        e.g. '两千万' = 2000 * 10000 not 2000 + 10000
        """
        value = [0]
        last_power = 0
        for s in integer_symbols:
            if isinstance(s, CND):
                value[-1] = s.value
            elif isinstance(s, CNU):
                value[-1] *= pow(10, s.power)
                if s.power > last_power:
                    value[:-1] = list(map(lambda v: v * pow(10, s.power), value[:-1]))
                    last_power = s.power
                value.append(0)
        return sum(value)

    system = create_system(numbering_type)
    int_part, dec_part = string2symbols(chinese_string, system)
    int_part = correct_symbols(int_part, system)
    int_str = str(compute_value(int_part))
    dec_str = "".join([str(d.value) for d in dec_part])
    if dec_part:
        return "{0}.{1}".format(int_str, dec_str)
    else:
        return int_str


def num2chn(
    number_string,
    numbering_type=NUMBERING_TYPES[1],
    big=False,
    traditional=False,
    alt_zero=False,
    alt_one=False,
    alt_two=True,
    use_zeros=True,
    use_units=True,
):
    def get_value(value_string, use_zeros=True):
        striped_string = value_string.lstrip("0")

        # record nothing if all zeros
        if not striped_string:
            return []

        # record one digits
        elif len(striped_string) == 1:
            if use_zeros and len(value_string) != len(striped_string):
                return [system.digits[0], system.digits[int(striped_string)]]
            else:
                return [system.digits[int(striped_string)]]

        # recursively record multiple digits
        else:
            result_unit = next(u for u in reversed(system.units) if u.power < len(striped_string))
            result_string = value_string[: -result_unit.power]
            return get_value(result_string) + [result_unit] + get_value(striped_string[-result_unit.power :])

    system = create_system(numbering_type)

    int_dec = number_string.split(".")
    if len(int_dec) == 1:
        int_string = int_dec[0]
        dec_string = ""
    elif len(int_dec) == 2:
        int_string = int_dec[0]
        dec_string = int_dec[1]
    else:
        raise ValueError("invalid input num string with more than one dot: {}".format(number_string))

    if use_units and len(int_string) > 1:
        result_symbols = get_value(int_string)
    else:
        result_symbols = [system.digits[int(c)] for c in int_string]
    dec_symbols = [system.digits[int(c)] for c in dec_string]
    if dec_string:
        result_symbols += [system.math.point] + dec_symbols

    if alt_two:
        liang = CND(2, system.digits[2].alt_s, system.digits[2].alt_t, system.digits[2].big_s, system.digits[2].big_t)
        for i, v in enumerate(result_symbols):
            if isinstance(v, CND) and v.value == 2:
                next_symbol = result_symbols[i + 1] if i < len(result_symbols) - 1 else None
                previous_symbol = result_symbols[i - 1] if i > 0 else None
                if isinstance(next_symbol, CNU) and isinstance(previous_symbol, (CNU, type(None))):
                    if next_symbol.power != 1 and ((previous_symbol is None) or (previous_symbol.power != 1)):
                        result_symbols[i] = liang

    # if big is True, '两' will not be used and `alt_two` has no impact on output
    if big:
        attr_name = "big_"
        if traditional:
            attr_name += "t"
        else:
            attr_name += "s"
    else:
        if traditional:
            attr_name = "traditional"
        else:
            attr_name = "simplified"

    result = "".join([getattr(s, attr_name) for s in result_symbols])

    # if not use_zeros:
    #     result = result.strip(getattr(system.digits[0], attr_name))

    if alt_zero:
        result = result.replace(getattr(system.digits[0], attr_name), system.digits[0].alt_s)

    if alt_one:
        result = result.replace(getattr(system.digits[1], attr_name), system.digits[1].alt_s)

    for i, p in enumerate(POINT):
        if result.startswith(p):
            return CHINESE_DIGIS[0] + result

    # ^10, 11, .., 19
    if (
        len(result) >= 2
        and result[1] in [SMALLER_CHINESE_NUMERING_UNITS_SIMPLIFIED[0], SMALLER_CHINESE_NUMERING_UNITS_TRADITIONAL[0]]
        and result[0] in [CHINESE_DIGIS[1], BIG_CHINESE_DIGIS_SIMPLIFIED[1], BIG_CHINESE_DIGIS_TRADITIONAL[1]]
    ):
        result = result[1:]

    return result


# ================================================================================ #
#                          different types of rewriters
# ================================================================================ #
class Cardinal:
    """
    CARDINAL类
    """

    def __init__(self, cardinal=None, chntext=None):
        self.cardinal = cardinal
        self.chntext = chntext

    def chntext2cardinal(self):
        return chn2num(self.chntext)

    def cardinal2chntext(self):
        return num2chn(self.cardinal)


class Digit:
    """
    DIGIT类
    """

    def __init__(self, digit=None, chntext=None):
        self.digit = digit
        self.chntext = chntext

    # def chntext2digit(self):
    #     return chn2num(self.chntext)

    def digit2chntext(self):
        return num2chn(self.digit, alt_two=False, use_units=False)


class TelePhone:
    """
    TELEPHONE类
    """

    def __init__(self, telephone=None, raw_chntext=None, chntext=None):
        self.telephone = telephone
        self.raw_chntext = raw_chntext
        self.chntext = chntext

    # def chntext2telephone(self):
    #     sil_parts = self.raw_chntext.split('<SIL>')
    #     self.telephone = '-'.join([
    #         str(chn2num(p)) for p in sil_parts
    #     ])
    #     return self.telephone

    def telephone2chntext(self, fixed=False):
        if fixed:
            sil_parts = self.telephone.split("-")
            self.raw_chntext = "<SIL>".join([num2chn(part, alt_two=False, use_units=False) for part in sil_parts])
            self.chntext = self.raw_chntext.replace("<SIL>", "")
        else:
            sp_parts = self.telephone.strip("+").split()
            self.raw_chntext = "<SP>".join([num2chn(part, alt_two=False, use_units=False) for part in sp_parts])
            self.chntext = self.raw_chntext.replace("<SP>", "")
        return self.chntext


class Fraction:
    """
    FRACTION类
    """

    def __init__(self, fraction=None, chntext=None):
        self.fraction = fraction
        self.chntext = chntext

    def chntext2fraction(self):
        denominator, numerator = self.chntext.split("分之")
        return chn2num(numerator) + "/" + chn2num(denominator)

    def fraction2chntext(self):
        numerator, denominator = self.fraction.split("/")
        return num2chn(denominator) + "分之" + num2chn(numerator)


class Date:
    """
    DATE类
    """

    def __init__(self, date=None, chntext=None):
        self.date = date
        self.chntext = chntext

    # def chntext2date(self):
    #     chntext = self.chntext
    #     try:
    #         year, other = chntext.strip().split('年', maxsplit=1)
    #         year = Digit(chntext=year).digit2chntext() + '年'
    #     except ValueError:
    #         other = chntext
    #         year = ''
    #     if other:
    #         try:
    #             month, day = other.strip().split('月', maxsplit=1)
    #             month = Cardinal(chntext=month).chntext2cardinal() + '月'
    #         except ValueError:
    #             day = chntext
    #             month = ''
    #         if day:
    #             day = Cardinal(chntext=day[:-1]).chntext2cardinal() + day[-1]
    #     else:
    #         month = ''
    #         day = ''
    #     date = year + month + day
    #     self.date = date
    #     return self.date

    def date2chntext(self):
        date = self.date
        try:
            year, other = date.strip().split("年", 1)
            year = Digit(digit=year).digit2chntext() + "年"
        except ValueError:
            other = date
            year = ""
        if other:
            try:
                month, day = other.strip().split("月", 1)
                month = Cardinal(cardinal=month).cardinal2chntext() + "月"
            except ValueError:
                day = date
                month = ""
            if day:
                day = Cardinal(cardinal=day[:-1]).cardinal2chntext() + day[-1]
        else:
            month = ""
            day = ""
        chntext = year + month + day
        self.chntext = chntext
        return self.chntext


class Money:
    """
    MONEY类
    """

    def __init__(self, money=None, chntext=None):
        self.money = money
        self.chntext = chntext

    # def chntext2money(self):
    #     return self.money

    def money2chntext(self):
        money = self.money
        pattern = re.compile(r"(\d+(\.\d+)?)")
        matchers = pattern.findall(money)
        if matchers:
            for matcher in matchers:
                money = money.replace(matcher[0], Cardinal(cardinal=matcher[0]).cardinal2chntext())
        self.chntext = money
        return self.chntext


class Percentage:
    """
    PERCENTAGE类
    """

    def __init__(self, percentage=None, chntext=None):
        self.percentage = percentage
        self.chntext = chntext

    def chntext2percentage(self):
        return chn2num(self.chntext.strip().strip("百分之")) + "%"

    def percentage2chntext(self):
        return "百分之" + num2chn(self.percentage.strip().strip("%"))


def normalize_nsw(raw_text):
    text = "^" + raw_text + "$"

    # 规范化日期
    pattern = re.compile(r"\D+((([089]\d|(19|20)\d{2})年)?(\d{1,2}月(\d{1,2}[日号])?)?)")
    matchers = pattern.findall(text)
    if matchers:
        # print('date')
        for matcher in matchers:
            text = text.replace(matcher[0], Date(date=matcher[0]).date2chntext(), 1)

    # 规范化金钱
    pattern = re.compile(r"\D+((\d+(\.\d+)?)[多余几]?" + CURRENCY_UNITS + r"(\d" + CURRENCY_UNITS + r"?)?)")
    matchers = pattern.findall(text)
    if matchers:
        # print('money')
        for matcher in matchers:
            text = text.replace(matcher[0], Money(money=matcher[0]).money2chntext(), 1)

    # 规范化固话/手机号码
    # 手机
    # http://www.jihaoba.com/news/show/13680
    # 移动：139、138、137、136、135、134、159、158、157、150、151、152、188、187、182、183、184、178、198
    # 联通：130、131、132、156、155、186、185、176
    # 电信：133、153、189、180、181、177
    pattern = re.compile(r"\D((\+?86 ?)?1([38]\d|5[0-35-9]|7[678]|9[89])\d{8})\D")
    matchers = pattern.findall(text)
    if matchers:
        # print('telephone')
        for matcher in matchers:
            text = text.replace(matcher[0], TelePhone(telephone=matcher[0]).telephone2chntext(), 1)
    # 固话
    pattern = re.compile(r"\D((0(10|2[1-3]|[3-9]\d{2})-?)?[1-9]\d{6,7})\D")
    matchers = pattern.findall(text)
    if matchers:
        # print('fixed telephone')
        for matcher in matchers:
            text = text.replace(matcher[0], TelePhone(telephone=matcher[0]).telephone2chntext(fixed=True), 1)

    # 规范化分数
    pattern = re.compile(r"(\d+/\d+)")
    matchers = pattern.findall(text)
    if matchers:
        # print('fraction')
        for matcher in matchers:
            text = text.replace(matcher, Fraction(fraction=matcher).fraction2chntext(), 1)

    # 规范化百分数
    text = text.replace("％", "%")
    pattern = re.compile(r"(\d+(\.\d+)?%)")
    matchers = pattern.findall(text)
    if matchers:
        # print('percentage')
        for matcher in matchers:
            text = text.replace(matcher[0], Percentage(percentage=matcher[0]).percentage2chntext(), 1)

    # 规范化纯数+量词
    pattern = re.compile(r"(\d+(\.\d+)?)[多余几]?" + COM_QUANTIFIERS)
    matchers = pattern.findall(text)
    if matchers:
        # print('cardinal+quantifier')
        for matcher in matchers:
            text = text.replace(matcher[0], Cardinal(cardinal=matcher[0]).cardinal2chntext(), 1)

    # 规范化数字编号
    pattern = re.compile(r"(\d{4,32})")
    matchers = pattern.findall(text)
    if matchers:
        # print('digit')
        for matcher in matchers:
            text = text.replace(matcher, Digit(digit=matcher).digit2chntext(), 1)

    # 规范化纯数
    pattern = re.compile(r"(\d+(\.\d+)?)")
    matchers = pattern.findall(text)
    if matchers:
        # print('cardinal')
        for matcher in matchers:
            text = text.replace(matcher[0], Cardinal(cardinal=matcher[0]).cardinal2chntext(), 1)

    # restore P2P, O2O, B2C, B2B etc
    pattern = re.compile(r"(([a-zA-Z]+)二([a-zA-Z]+))")
    matchers = pattern.findall(text)
    if matchers:
        # print('particular')
        for matcher in matchers:
            text = text.replace(matcher[0], matcher[1] + "2" + matcher[2], 1)

    return text.lstrip("^").rstrip("$")


def remove_erhua(text):
    """
    去除儿化音词中的儿:
    他女儿在那边儿 -> 他女儿在那边
    """

    new_str = ""
    while re.search("儿", text):
        a = re.search("儿", text).span()
        remove_er_flag = 0

        if ER_WHITELIST_PATTERN.search(text):
            b = ER_WHITELIST_PATTERN.search(text).span()
            if b[0] <= a[0]:
                remove_er_flag = 1

        if remove_er_flag == 0:
            new_str = new_str + text[0 : a[0]]
            text = text[a[1] :]
        else:
            new_str = new_str + text[0 : b[1]]
            text = text[b[1] :]

    text = new_str + text
    return text


def remove_space(text):
    tokens = text.split()
    new = []
    for k, t in enumerate(tokens):
        if k != 0:
            if IN_EN_CHARS.get(tokens[k - 1][-1]) and IN_EN_CHARS.get(t[0]):
                new.append(" ")
        new.append(t)
    return "".join(new)


class TextNorm:
    def __init__(
        self,
        to_banjiao: bool = False,
        to_upper: bool = False,
        to_lower: bool = False,
        remove_fillers: bool = False,
        remove_erhua: bool = False,
        check_chars: bool = False,
        remove_space: bool = False,
        cc_mode: str = "",
    ):
        self.to_banjiao = to_banjiao
        self.to_upper = to_upper
        self.to_lower = to_lower
        self.remove_fillers = remove_fillers
        self.remove_erhua = remove_erhua
        self.check_chars = check_chars
        self.remove_space = remove_space

        self.cc = None
        if cc_mode:
            from opencc import OpenCC  # Open Chinese Convert: pip install opencc

            self.cc = OpenCC(cc_mode)

    def __call__(self, text):
        if self.cc:
            text = self.cc.convert(text)

        if self.to_banjiao:
            text = text.translate(QJ2BJ_TRANSFORM)

        if self.to_upper:
            text = text.upper()

        if self.to_lower:
            text = text.lower()

        if self.remove_fillers:
            for c in FILLER_CHARS:
                text = text.replace(c, "")

        if self.remove_erhua:
            text = remove_erhua(text)

        text = normalize_nsw(text)

        text = text.translate(PUNCS_TRANSFORM)

        if self.check_chars:
            for c in text:
                if not IN_VALID_CHARS.get(c):
                    print(f"WARNING: illegal char {c} in: {text}", file=sys.stderr)
                    return ""

        if self.remove_space:
            text = remove_space(text)

        return text


if __name__ == "__main__":
    p = argparse.ArgumentParser()

    # normalizer options
    p.add_argument("--to_banjiao", action="store_true", help="convert quanjiao chars to banjiao")
    p.add_argument("--to_upper", action="store_true", help="convert to upper case")
    p.add_argument("--to_lower", action="store_true", help="convert to lower case")
    p.add_argument("--remove_fillers", action="store_true", help='remove filler chars such as "呃, 啊"')
    p.add_argument("--remove_erhua", action="store_true", help='remove erhua chars such as "他女儿在那边儿 -> 他女儿在那边"')
    p.add_argument("--check_chars", action="store_true", help="skip sentences containing illegal chars")
    p.add_argument("--remove_space", action="store_true", help="remove whitespace")
    p.add_argument(
        "--cc_mode", choices=["", "t2s", "s2t"], default="", help="convert between traditional to simplified"
    )

    # I/O options
    p.add_argument("--log_interval", type=int, default=10000, help="log interval in number of processed lines")
    p.add_argument("--has_key", action="store_true", help="will be deprecated, set --format ark instead")
    p.add_argument("--format", type=str, choices=["txt", "ark", "tsv"], default="txt", help="input format")
    p.add_argument("ifile", help="input filename, assume utf-8 encoding")
    p.add_argument("ofile", help="output filename")

    args = p.parse_args()

    if args.has_key:
        args.format = "ark"

    normalizer = TextNorm(
        to_banjiao=args.to_banjiao,
        to_upper=args.to_upper,
        to_lower=args.to_lower,
        remove_fillers=args.remove_fillers,
        remove_erhua=args.remove_erhua,
        check_chars=args.check_chars,
        remove_space=args.remove_space,
        cc_mode=args.cc_mode,
    )

    normalizer = TextNorm(
        to_banjiao=args.to_banjiao,
        to_upper=args.to_upper,
        to_lower=args.to_lower,
        remove_fillers=args.remove_fillers,
        remove_erhua=args.remove_erhua,
        check_chars=args.check_chars,
        remove_space=args.remove_space,
        cc_mode=args.cc_mode,
    )

    ndone = 0
    with open(args.ifile, "r", encoding="utf8") as istream, open(args.ofile, "w+", encoding="utf8") as ostream:
        if args.format == "tsv":
            reader = csv.DictReader(istream, delimiter="\t")
            assert "TEXT" in reader.fieldnames
            print("\t".join(reader.fieldnames), file=ostream)

            for item in reader:
                text = item["TEXT"]

                if text:
                    text = normalizer(text)

                if text:
                    item["TEXT"] = text
                    print("\t".join([item[f] for f in reader.fieldnames]), file=ostream)

                ndone += 1
                if ndone % args.log_interval == 0:
                    print(f"text norm: {ndone} lines done.", file=sys.stderr, flush=True)
        else:
            for l in istream:
                key, text = "", ""
                if args.format == "ark":  # KALDI archive, line format: "key text"
                    cols = l.strip().split(maxsplit=1)
                    key, text = cols[0], cols[1] if len(cols) == 2 else ""
                else:
                    text = l.strip()

                if text:
                    text = normalizer(text)

                if text:
                    if args.format == "ark":
                        print(key + "\t" + text, file=ostream)
                    else:
                        print(text, file=ostream)

                ndone += 1
                if ndone % args.log_interval == 0:
                    print(f"text norm: {ndone} lines done.", file=sys.stderr, flush=True)
    print(f"text norm: {ndone} lines done in total.", file=sys.stderr, flush=True)


================================================================================
# File: auralis/models/xttsv2/components/tts/layers/xtts/perceiver_encoder.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/xttsv2/components/tts/layers/xtts/perceiver_encoder.py
================================================================================

# Adapted from https://github.com/lucidrains/naturalspeech2-pytorch/blob/659bec7f7543e7747e809e950cc2f84242fbeec7/naturalspeech2_pytorch/naturalspeech2_pytorch.py#L532

from collections import namedtuple
from functools import wraps

import torch
import torch.nn.functional as F
from einops import rearrange, repeat
from einops.layers.torch import Rearrange
from packaging import version
from torch import einsum, nn


def exists(val):
    """Check if a value exists (is not None).

    Args:
        val: Any value to check.

    Returns:
        bool: True if value is not None, False otherwise.
    """
    return val is not None


def once(fn):
    """Decorator to ensure a function is called only once.

    Args:
        fn (Callable): Function to wrap.

    Returns:
        Callable: Wrapped function that will only execute on first call.
    """
    called = False

    @wraps(fn)
    def inner(x):
        nonlocal called
        if called:
            return
        called = True
        return fn(x)

    return inner


print_once = once(print)

# main class


class Attend(nn.Module):
    """Efficient attention implementation with support for flash attention.
    
    This module implements scaled dot-product attention with support for both
    regular and flash attention mechanisms. It includes optimizations for
    different GPU architectures and causal masking support.
    """

    def __init__(self, dropout=0.0, causal=False, use_flash=False):
        """Initialize attention module.

        Args:
            dropout (float, optional): Attention dropout probability. Defaults to 0.0.
            causal (bool, optional): Whether to use causal masking. Defaults to False.
            use_flash (bool, optional): Whether to use flash attention when available.
                Defaults to False.
        """
        super().__init__()
        self.dropout = dropout
        self.attn_dropout = nn.Dropout(dropout)

        self.causal = causal
        self.register_buffer("mask", None, persistent=False)

        self.use_flash = use_flash
        assert not (
            use_flash and version.parse(torch.__version__) < version.parse("2.0.0")
        ), "in order to use flash attention, you must be using pytorch 2.0 or above"

        # determine efficient attention configs for cuda and cpu
        self.config = namedtuple("EfficientAttentionConfig", ["enable_flash", "enable_math", "enable_mem_efficient"])
        self.cpu_config = self.config(True, True, True)
        self.cuda_config = None

        if not torch.cuda.is_available() or not use_flash:
            return

        device_properties = torch.cuda.get_device_properties(torch.device("cuda"))

        if device_properties.major == 8 and device_properties.minor == 0:
            print_once("A100 GPU detected, using flash attention if input tensor is on cuda")
            self.cuda_config = self.config(True, False, False)
        else:
            print_once("Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda")
            self.cuda_config = self.config(False, True, True)

    def get_mask(self, n, device):
        """Get or create causal attention mask.

        Args:
            n (int): Sequence length.
            device: Device to create mask on.

        Returns:
            torch.Tensor: Causal attention mask.
        """
        if exists(self.mask) and self.mask.shape[-1] >= n:
            return self.mask[:n, :n]

        mask = torch.ones((n, n), device=device, dtype=torch.bool).triu(1)
        self.register_buffer("mask", mask, persistent=False)
        return mask

    def flash_attn(self, q, k, v, mask=None):
        """Compute attention using flash attention mechanism.

        Args:
            q (torch.Tensor): Query tensor.
            k (torch.Tensor): Key tensor.
            v (torch.Tensor): Value tensor.
            mask (torch.Tensor, optional): Attention mask. Defaults to None.

        Returns:
            torch.Tensor: Output tensor after attention.
        """
        _, heads, q_len, _, k_len, is_cuda = *q.shape, k.shape[-2], q.is_cuda

        # Recommended for multi-query single-key-value attention by Tri Dao
        # kv shape torch.Size([1, 512, 64]) -> torch.Size([1, 8, 512, 64])

        if k.ndim == 3:
            k = rearrange(k, "b ... -> b 1 ...").expand_as(q)

        if v.ndim == 3:
            v = rearrange(v, "b ... -> b 1 ...").expand_as(q)

        # Check if mask exists and expand to compatible shape
        # The mask is B L, so it would have to be expanded to B H N L

        if exists(mask):
            mask = rearrange(mask, "b j -> b 1 1 j")
            mask = mask.expand(-1, heads, q_len, -1)

        # Check if there is a compatible device for flash attention

        config = self.cuda_config if is_cuda else self.cpu_config

        # pytorch 2.0 flash attn: q, k, v, mask, dropout, causal, softmax_scale

        with torch.backends.cuda.sdp_kernel(**config._asdict()):
            out = F.scaled_dot_product_attention(
                q, k, v, attn_mask=mask, dropout_p=self.dropout if self.training else 0.0, is_causal=self.causal
            )

        return out

    def forward(self, q, k, v, mask=None):
        """Compute attention scores and aggregate values.

        Args:
            q (torch.Tensor): Query tensor.
            k (torch.Tensor): Key tensor.
            v (torch.Tensor): Value tensor.
            mask (torch.Tensor, optional): Attention mask. Defaults to None.

        Returns:
            torch.Tensor: Output tensor after attention.
        """
        """
        einstein notation
        b - batch
        h - heads
        n, i, j - sequence length (base sequence length, source, target)
        d - feature dimension
        """

        n, device = q.shape[-2], q.device

        scale = q.shape[-1] ** -0.5

        if self.use_flash:
            return self.flash_attn(q, k, v, mask=mask)

        kv_einsum_eq = "b j d" if k.ndim == 3 else "b h j d"

        # similarity

        sim = einsum(f"b h i d, {kv_einsum_eq} -> b h i j", q, k) * scale

        # key padding mask

        if exists(mask):
            mask = rearrange(mask, "b j -> b 1 1 j")
            sim = sim.masked_fill(~mask, -torch.finfo(sim.dtype).max)

        # causal mask

        if self.causal:
            causal_mask = self.get_mask(n, device)
            sim = sim.masked_fill(causal_mask, -torch.finfo(sim.dtype).max)

        # attention

        attn = sim.softmax(dim=-1)
        attn = self.attn_dropout(attn)

        # aggregate values

        out = einsum(f"b h i j, {kv_einsum_eq} -> b h i d", attn, v)

        return out


def Sequential(*mods):
    """Create sequential module with automatic filtering of None modules.

    Args:
        *mods: Variable number of modules.

    Returns:
        nn.Sequential: Sequential container of non-None modules.
    """
    return nn.Sequential(*filter(exists, mods))


def default(val, d):
    """Return default value if input is None.

    Args:
        val: Input value.
        d: Default value or callable.

    Returns:
        Value to use (input value if it exists, otherwise default).
    """
    if exists(val):
        return val
    return d() if callable(d) else d


class RMSNorm(nn.Module):
    """Root Mean Square Layer Normalization with optional conditioning.
    
    This module implements RMS normalization with learnable scale and optional
    conditional scaling and bias.
    """

    def __init__(self, dim, scale=True, dim_cond=None):
        """Initialize RMS normalization.

        Args:
            dim (int): Feature dimension to normalize.
            scale (bool, optional): Whether to use learnable scale. Defaults to True.
            dim_cond (int, optional): Dimension of conditioning input. Defaults to None.
        """
        super().__init__()
        self.cond = exists(dim_cond)
        self.to_gamma_beta = nn.Linear(dim_cond, dim * 2) if self.cond else None

        self.scale = dim**0.5
        self.gamma = nn.Parameter(torch.ones(dim)) if scale else None

    def forward(self, x, cond=None):
        """Apply RMS normalization.

        Args:
            x (torch.Tensor): Input tensor.
            cond (torch.Tensor, optional): Conditioning tensor. Defaults to None.

        Returns:
            torch.Tensor: Normalized tensor.
        """
        gamma = default(self.gamma, 1)
        out = F.normalize(x, dim=-1) * self.scale * gamma

        if not self.cond:
            return out

        assert exists(cond)
        gamma, beta = self.to_gamma_beta(cond).chunk(2, dim=-1)
        gamma, beta = map(lambda t: rearrange(t, "b d -> b 1 d"), (gamma, beta))
        return out * gamma + beta


class CausalConv1d(nn.Conv1d):
    """1D causal convolution layer.
    
    This layer implements 1D convolution with causal padding to prevent information
    leakage from future timesteps.
    """

    def __init__(self, *args, **kwargs):
        """Initialize causal convolution.

        Args:
            *args: Arguments passed to Conv1d.
            **kwargs: Keyword arguments passed to Conv1d.
        """
        super().__init__(*args, **kwargs)
        (kernel_size,) = self.kernel_size
        (dilation,) = self.dilation
        (stride,) = self.stride

        assert stride == 1
        self.causal_padding = dilation * (kernel_size - 1)

    def forward(self, x):
        """Apply causal convolution.

        Args:
            x (torch.Tensor): Input tensor.

        Returns:
            torch.Tensor: Output after causal convolution.
        """
        causal_padded_x = F.pad(x, (self.causal_padding, 0), value=0.0)
        return super().forward(causal_padded_x)


class GEGLU(nn.Module):
    """Gated Gaussian Error Linear Unit activation function."""

    def forward(self, x):
        """Apply GEGLU activation.

        Args:
            x (torch.Tensor): Input tensor with last dimension split for gating.

        Returns:
            torch.Tensor: Output after GEGLU activation.
        """
        x, gate = x.chunk(2, dim=-1)
        return F.gelu(gate) * x


def FeedForward(dim, mult=4, causal_conv=False):
    """Create a feed-forward network with GEGLU activation.

    Args:
        dim (int): Input dimension.
        mult (int, optional): Multiplier for inner dimension. Defaults to 4.
        causal_conv (bool, optional): Whether to use causal convolution.
            Defaults to False.

    Returns:
        nn.Sequential: Feed-forward network module.
    """
    dim_inner = int(dim * mult * 2 / 3)

    conv = None
    if causal_conv:
        conv = nn.Sequential(
            Rearrange("b n d -> b d n"),
            CausalConv1d(dim_inner, dim_inner, 3),
            Rearrange("b d n -> b n d"),
        )

    return Sequential(nn.Linear(dim, dim_inner * 2), GEGLU(), conv, nn.Linear(dim_inner, dim))


class PerceiverResampler(nn.Module):
    """Perceiver-based resampling module for sequence processing.
    
    This module uses learnable latent vectors to process and resample input sequences
    through cross-attention and self-attention mechanisms. It's particularly useful
    for processing variable-length sequences into fixed-length representations.
    """

    def __init__(
        self,
        *,
        dim,
        depth=2,
        dim_context=None,
        num_latents=32,
        dim_head=64,
        heads=8,
        ff_mult=4,
        use_flash_attn=False,
    ):
        """Initialize Perceiver resampler.

        Args:
            dim (int): Model dimension.
            depth (int, optional): Number of transformer layers. Defaults to 2.
            dim_context (int, optional): Context dimension. Defaults to None.
            num_latents (int, optional): Number of learnable latent vectors. Defaults to 32.
            dim_head (int, optional): Attention head dimension. Defaults to 64.
            heads (int, optional): Number of attention heads. Defaults to 8.
            ff_mult (int, optional): Feed-forward expansion factor. Defaults to 4.
            use_flash_attn (bool, optional): Whether to use flash attention. Defaults to False.
        """
        super().__init__()
        dim_context = default(dim_context, dim)

        self.proj_context = nn.Linear(dim_context, dim) if dim_context != dim else nn.Identity()

        self.latents = nn.Parameter(torch.randn(num_latents, dim))
        nn.init.normal_(self.latents, std=0.02)

        self.layers = nn.ModuleList([])
        for _ in range(depth):
            self.layers.append(
                nn.ModuleList(
                    [
                        Attention(
                            dim=dim,
                            dim_head=dim_head,
                            heads=heads,
                            use_flash=use_flash_attn,
                            cross_attn_include_queries=True,
                        ),
                        FeedForward(dim=dim, mult=ff_mult),
                    ]
                )
            )

        self.norm = RMSNorm(dim)

    def forward(self, x, mask=None):
        """Process input sequence through Perceiver resampling.

        Args:
            x (torch.Tensor): Input tensor of shape [batch, seq_len, dim].
            mask (torch.Tensor, optional): Attention mask. Defaults to None.

        Returns:
            torch.Tensor: Processed tensor of shape [batch, num_latents, dim].
        """
        batch = x.shape[0]

        x = self.proj_context(x)

        latents = repeat(self.latents, "n d -> b n d", b=batch)

        for attn, ff in self.layers:
            latents = attn(latents, x, mask=mask) + latents
            latents = ff(latents) + latents

        return self.norm(latents)


class Attention(nn.Module):
    def __init__(
        self,
        dim,
        *,
        dim_context=None,
        causal=False,
        dim_head=64,
        heads=8,
        dropout=0.0,
        use_flash=False,
        cross_attn_include_queries=False,
    ):
        super().__init__()
        self.scale = dim_head**-0.5
        self.heads = heads
        self.cross_attn_include_queries = cross_attn_include_queries

        dim_inner = dim_head * heads
        dim_context = default(dim_context, dim)

        self.attend = Attend(causal=causal, dropout=dropout, use_flash=use_flash)
        self.to_q = nn.Linear(dim, dim_inner, bias=False)
        self.to_kv = nn.Linear(dim_context, dim_inner * 2, bias=False)
        self.to_out = nn.Linear(dim_inner, dim, bias=False)

    def forward(self, x, context=None, mask=None):
        h, has_context = self.heads, exists(context)

        context = default(context, x)

        if has_context and self.cross_attn_include_queries:
            context = torch.cat((x, context), dim=-2)

        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim=-1))
        q, k, v = map(lambda t: rearrange(t, "b n (h d) -> b h n d", h=h), (q, k, v))

        out = self.attend(q, k, v, mask=mask)

        out = rearrange(out, "b h n d -> b n (h d)")
        return self.to_out(out)


================================================================================
# File: auralis/models/xttsv2/components/tts/layers/xtts/latent_encoder.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/xttsv2/components/tts/layers/xtts/latent_encoder.py
================================================================================

# ported from: Originally ported from: https://github.com/neonbjb/tortoise-tts

import math

import torch
from torch import nn
from torch.nn import functional as F


class GroupNorm32(nn.GroupNorm):
    """GroupNorm with float32 conversion for improved numerical stability.
    
    This class extends PyTorch's GroupNorm to perform normalization in float32
    precision, regardless of the input tensor's dtype. This helps prevent 
    numerical instability issues when using lower precision dtypes.
    """

    def forward(self, x):
        """Forward pass with automatic float32 conversion.

        Args:
            x (torch.Tensor): Input tensor of any dtype.

        Returns:
            torch.Tensor: Normalized tensor converted back to input dtype.
        """
        return super().forward(x.float()).type(x.dtype)


def conv_nd(dims, *args, **kwargs):
    """N-dimensional convolution factory function.

    Args:
        dims (int): Number of dimensions (1, 2, or 3).
        *args: Arguments passed to the convolution constructor.
        **kwargs: Keyword arguments passed to the convolution constructor.

    Returns:
        nn.Module: Appropriate convolution module for the given dimensions.

    Raises:
        ValueError: If dimensions are not 1, 2, or 3.
    """
    if dims == 1:
        return nn.Conv1d(*args, **kwargs)
    elif dims == 2:
        return nn.Conv2d(*args, **kwargs)
    elif dims == 3:
        return nn.Conv3d(*args, **kwargs)
    raise ValueError(f"unsupported dimensions: {dims}")


def normalization(channels):
    """Create a GroupNorm32 normalization layer with adaptive group size.

    Automatically determines the optimal number of groups based on the number
    of channels, ensuring the number of channels is divisible by the group size.

    Args:
        channels (int): Number of input channels.

    Returns:
        GroupNorm32: Normalization layer with appropriate group size.
    """
    groups = 32
    if channels <= 16:
        groups = 8
    elif channels <= 64:
        groups = 16
    while channels % groups != 0:
        groups = int(groups / 2)
    assert groups > 2
    return GroupNorm32(groups, channels)


def zero_module(module):
    """Initialize all parameters of a module to zero.

    Args:
        module (nn.Module): PyTorch module to initialize.

    Returns:
        nn.Module: Module with zeroed parameters.
    """
    for p in module.parameters():
        p.detach().zero_()
    return module


class QKVAttention(nn.Module):
    """Multi-head QKV attention mechanism.
    
    Implements scaled dot-product attention with query, key, and value tensors
    combined in a single input tensor. Supports optional masking and bias.
    """

    def __init__(self, n_heads):
        """Initialize QKV attention.

        Args:
            n_heads (int): Number of attention heads.
        """
        super().__init__()
        self.n_heads = n_heads

    def forward(self, qkv, mask=None, qk_bias=0):
        """Apply QKV attention.

        Args:
            qkv (torch.Tensor): Input tensor of shape [N x (H * 3 * C) x T] containing
                concatenated queries, keys, and values.
            mask (torch.Tensor, optional): Attention mask. Defaults to None.
            qk_bias (float, optional): Bias added to attention scores. Defaults to 0.

        Returns:
            torch.Tensor: Output tensor of shape [N x (H * C) x T] after attention.
        """
        bs, width, length = qkv.shape
        assert width % (3 * self.n_heads) == 0
        ch = width // (3 * self.n_heads)
        q, k, v = qkv.reshape(bs * self.n_heads, ch * 3, length).split(ch, dim=1)
        scale = 1 / math.sqrt(math.sqrt(ch))
        weight = torch.einsum("bct,bcs->bts", q * scale, k * scale)  # More stable with f16 than dividing afterwards
        weight = weight + qk_bias
        if mask is not None:
            mask = mask.repeat(self.n_heads, 1, 1)
            weight[mask.logical_not()] = -torch.inf
        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)
        a = torch.einsum("bts,bcs->bct", weight, v)

        return a.reshape(bs, -1, length)


class AttentionBlock(nn.Module):
    """Self-attention block with spatial attention capabilities.
    
    This block allows different spatial positions to attend to each other through
    multi-head self-attention. It includes normalization, optional activation,
    and residual connections.
    """

    def __init__(
        self,
        channels,
        num_heads=1,
        num_head_channels=-1,
        out_channels=None,
        do_activation=False,
    ):
        """Initialize attention block.

        Args:
            channels (int): Number of input channels.
            num_heads (int, optional): Number of attention heads. Defaults to 1.
            num_head_channels (int, optional): Channels per head. If -1, divide channels
                by num_heads. Defaults to -1.
            out_channels (int, optional): Number of output channels. If None, same as
                input channels. Defaults to None.
            do_activation (bool, optional): Whether to apply SiLU activation after
                normalization. Defaults to False.
        """
        super().__init__()
        self.channels = channels
        out_channels = channels if out_channels is None else out_channels
        self.do_activation = do_activation
        if num_head_channels == -1:
            self.num_heads = num_heads
        else:
            assert (
                channels % num_head_channels == 0
            ), f"q,k,v channels {channels} is not divisible by num_head_channels {num_head_channels}"
            self.num_heads = channels // num_head_channels
        self.norm = normalization(channels)
        self.qkv = conv_nd(1, channels, out_channels * 3, 1)
        self.attention = QKVAttention(self.num_heads)

        self.x_proj = nn.Identity() if out_channels == channels else conv_nd(1, channels, out_channels, 1)
        self.proj_out = zero_module(conv_nd(1, out_channels, out_channels, 1))

    def forward(self, x, mask=None, qk_bias=0):
        """Forward pass of attention block.

        Args:
            x (torch.Tensor): Input tensor of shape [B x C x *spatial_dims].
            mask (torch.Tensor, optional): Attention mask. Defaults to None.
            qk_bias (float, optional): Bias added to attention scores. Defaults to 0.

        Returns:
            torch.Tensor: Output tensor with same shape as input.
        """
        b, c, *spatial = x.shape
        if mask is not None:
            if len(mask.shape) == 2:
                mask = mask.unsqueeze(0).repeat(x.shape[0], 1, 1)
            if mask.shape[1] != x.shape[-1]:
                mask = mask[:, : x.shape[-1], : x.shape[-1]]

        x = x.reshape(b, c, -1)
        x = self.norm(x)
        if self.do_activation:
            x = F.silu(x, inplace=True)
        qkv = self.qkv(x)
        h = self.attention(qkv, mask=mask, qk_bias=qk_bias)
        h = self.proj_out(h)
        xp = self.x_proj(x)
        return (xp + h).reshape(b, xp.shape[1], *spatial)


class ConditioningEncoder(nn.Module):
    """Encoder for conditioning signals using self-attention.
    
    This module encodes mel-spectrograms or similar conditioning signals into
    a latent space using a series of attention blocks. It first projects the
    input to the embedding dimension using a 1x1 convolution, then applies
    multiple attention blocks.
    """

    def __init__(
        self,
        spec_dim,
        embedding_dim,
        attn_blocks=6,
        num_attn_heads=4,
    ):
        """Initialize conditioning encoder.

        Args:
            spec_dim (int): Dimension of input spectrogram features.
            embedding_dim (int): Dimension of the embedding space.
            attn_blocks (int, optional): Number of attention blocks. Defaults to 6.
            num_attn_heads (int, optional): Number of attention heads per block.
                Defaults to 4.
        """
        super().__init__()
        attn = []
        self.init = nn.Conv1d(spec_dim, embedding_dim, kernel_size=1)
        for a in range(attn_blocks):
            attn.append(AttentionBlock(embedding_dim, num_attn_heads))
        self.attn = nn.Sequential(*attn)
        self.dim = embedding_dim

    def forward(self, x):
        """Encode input spectrogram into latent representation.

        Args:
            x (torch.Tensor): Input tensor of shape [batch_size, spec_dim, sequence_length].

        Returns:
            torch.Tensor: Encoded representation of shape [batch_size, embedding_dim, sequence_length].
        """
        h = self.init(x)
        h = self.attn(h)
        return h


================================================================================
# File: auralis/models/xttsv2/components/tts/layers/xtts/hifigan_decoder.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/xttsv2/components/tts/layers/xtts/hifigan_decoder.py
================================================================================

import torch
import torchaudio
from torch import nn
from torch.nn import Conv1d, ConvTranspose1d
from torch.nn import functional as F
from torch.nn.utils.parametrizations import weight_norm
from torch.nn.utils.parametrize import remove_parametrizations

from .......common.utilities import load_fsspec

LRELU_SLOPE = 0.1


def get_padding(k, d):
    """Calculate padding size for a convolutional layer.

    Args:
        k (int): Kernel size.
        d (int): Dilation rate.

    Returns:
        int: Required padding size.
    """
    return int((k * d - d) / 2)


class ResBlock1(torch.nn.Module):
    """Residual Block Type 1. It has 3 convolutional layers in each convolutional block.

    Network::

        x -> lrelu -> conv1_1 -> conv1_2 -> conv1_3 -> z -> lrelu -> conv2_1 -> conv2_2 -> conv2_3 -> o -> + -> o
        |--------------------------------------------------------------------------------------------------|


    Args:
        channels (int): number of hidden channels for the convolutional layers.
        kernel_size (int): size of the convolution filter in each layer.
        dilations (list): list of dilation value for each conv layer in a block.
    """

    def __init__(self, channels, kernel_size=3, dilation=(1, 3, 5)):
        super().__init__()
        self.convs1 = nn.ModuleList(
            [
                weight_norm(
                    Conv1d(
                        channels,
                        channels,
                        kernel_size,
                        1,
                        dilation=dilation[i],
                        padding=get_padding(kernel_size, dilation[i]),
                    )
                )
                for i in range(3)
            ]
        )

        self.convs2 = nn.ModuleList(
            [
                weight_norm(
                    Conv1d(
                        channels,
                        channels,
                        kernel_size,
                        1,
                        dilation=1,
                        padding=get_padding(kernel_size, 1),
                    )
                )
                for _ in range(3)
            ]
        )

    def forward(self, x):
        """
        Args:
            x (Tensor): input tensor.
        Returns:
            Tensor: output tensor.
        Shapes:
            x: [B, C, T]
        """
        for c1, c2 in zip(self.convs1, self.convs2):
            xt = F.leaky_relu(x, LRELU_SLOPE)
            xt = c1(xt)
            xt = F.leaky_relu(xt, LRELU_SLOPE)
            xt = c2(xt)
            x = xt + x
        return x

    def remove_weight_norm(self):
        for l in self.convs1:
            remove_parametrizations(l, "weight")
        for l in self.convs2:
            remove_parametrizations(l, "weight")


class ResBlock2(torch.nn.Module):
    """Residual Block Type 2. It has 1 convolutional layers in each convolutional block.

    Network::

        x -> lrelu -> conv1-> -> z -> lrelu -> conv2-> o -> + -> o
        |---------------------------------------------------|


    Args:
        channels (int): number of hidden channels for the convolutional layers.
        kernel_size (int): size of the convolution filter in each layer.
        dilations (list): list of dilation value for each conv layer in a block.
    """

    def __init__(self, channels, kernel_size=3, dilation=(1, 3)):
        super().__init__()
        self.convs = nn.ModuleList(
            [
                weight_norm(
                    Conv1d(
                        channels,
                        channels,
                        kernel_size,
                        1,
                        dilation=dilation[i],
                        padding=get_padding(kernel_size, dilation[i]),
                    )
                )
                for i in range(2)
            ]
        )

    def forward(self, x):
        for c in self.convs:
            xt = F.leaky_relu(x, LRELU_SLOPE)
            xt = c(xt)
            x = xt + x
        return x

    def remove_weight_norm(self):
        for l in self.convs:
            remove_parametrizations(l, "weight")


class HifiganGenerator(torch.nn.Module):
    def __init__(
        self,
        in_channels,
        out_channels,
        resblock_type,
        resblock_dilation_sizes,
        resblock_kernel_sizes,
        upsample_kernel_sizes,
        upsample_initial_channel,
        upsample_factors,
        inference_padding=5,
        cond_channels=0,
        conv_pre_weight_norm=True,
        conv_post_weight_norm=True,
        conv_post_bias=True,
        cond_in_each_up_layer=False,
    ):
        r"""HiFiGAN Generator with Multi-Receptive Field Fusion (MRF)

        Network:
            x -> lrelu -> upsampling_layer -> resblock1_k1x1 -> z1 -> + -> z_sum / #resblocks -> lrelu -> conv_post_7x1 -> tanh -> o
                                                 ..          -> zI ---|
                                              resblockN_kNx1 -> zN ---'

        Args:
            in_channels (int): number of input tensor channels.
            out_channels (int): number of output tensor channels.
            resblock_type (str): type of the `ResBlock`. '1' or '2'.
            resblock_dilation_sizes (List[List[int]]): list of dilation values in each layer of a `ResBlock`.
            resblock_kernel_sizes (List[int]): list of kernel sizes for each `ResBlock`.
            upsample_kernel_sizes (List[int]): list of kernel sizes for each transposed convolution.
            upsample_initial_channel (int): number of channels for the first upsampling layer. This is divided by 2
                for each consecutive upsampling layer.
            upsample_factors (List[int]): upsampling factors (stride) for each upsampling layer.
            inference_padding (int): constant padding applied to the input at inference time. Defaults to 5.
        """
        super().__init__()
        self.inference_padding = inference_padding
        self.num_kernels = len(resblock_kernel_sizes)
        self.num_upsamples = len(upsample_factors)
        self.cond_in_each_up_layer = cond_in_each_up_layer

        # initial upsampling layers
        self.conv_pre = weight_norm(Conv1d(in_channels, upsample_initial_channel, 7, 1, padding=3))
        resblock = ResBlock1 if resblock_type == "1" else ResBlock2
        # upsampling layers
        self.ups = nn.ModuleList()
        for i, (u, k) in enumerate(zip(upsample_factors, upsample_kernel_sizes)):
            self.ups.append(
                weight_norm(
                    ConvTranspose1d(
                        upsample_initial_channel // (2**i),
                        upsample_initial_channel // (2 ** (i + 1)),
                        k,
                        u,
                        padding=(k - u) // 2,
                    )
                )
            )
        # MRF blocks
        self.resblocks = nn.ModuleList()
        for i in range(len(self.ups)):
            ch = upsample_initial_channel // (2 ** (i + 1))
            for k, d in zip(resblock_kernel_sizes, resblock_dilation_sizes):
                self.resblocks.append(resblock(ch, k, d))
        # post convolution layer
        self.conv_post = weight_norm(Conv1d(ch, out_channels, 7, 1, padding=3, bias=conv_post_bias))
        if cond_channels > 0:
            self.cond_layer = nn.Conv1d(cond_channels, upsample_initial_channel, 1)

        if not conv_pre_weight_norm:
            remove_parametrizations(self.conv_pre, "weight")

        if not conv_post_weight_norm:
            remove_parametrizations(self.conv_post, "weight")

        if self.cond_in_each_up_layer:
            self.conds = nn.ModuleList()
            for i in range(len(self.ups)):
                ch = upsample_initial_channel // (2 ** (i + 1))
                self.conds.append(nn.Conv1d(cond_channels, ch, 1))

    def forward(self, x, g=None):
        """
        Args:
            x (Tensor): feature input tensor.
            g (Tensor): global conditioning input tensor.

        Returns:
            Tensor: output waveform.

        Shapes:
            x: [B, C, T]
            Tensor: [B, 1, T]
        """
        with torch.no_grad():
            with torch.amp.autocast('cuda'):
                x = self.conv_pre(x).unsqueeze(0)
                if hasattr(self, "cond_layer"):
                    x.add_(self.cond_layer(g))
                for i in range(self.num_upsamples):
                    x = F.leaky_relu(x, LRELU_SLOPE, inplace=True)
                    x = self.ups[i](x)

                    if self.cond_in_each_up_layer:
                        x.add_(self.conds[i](g))

                    z_sum = 0
                    for j in range(self.num_kernels):
                        z_sum += (self.resblocks[i * self.num_kernels + j](x)).float()
                    x = z_sum / self.num_kernels
                x = F.leaky_relu(x, inplace=True)
                x = self.conv_post(x)
                x = torch.tanh(x)
                return x

    @torch.no_grad()
    def inference(self, c):
        """
        Args:
            x (Tensor): conditioning input tensor.

        Returns:
            Tensor: output waveform.

        Shapes:
            x: [B, C, T]
            Tensor: [B, 1, T]
        """
        c = c.to(self.conv_pre.weight.device)
        c = torch.nn.functional.pad(c, (self.inference_padding, self.inference_padding), "replicate")
        return self.forward(c)

    def remove_weight_norm(self):
        print("Removing weight norm...")
        for l in self.ups:
            remove_parametrizations(l, "weight")
        for l in self.resblocks:
            l.remove_weight_norm()
        remove_parametrizations(self.conv_pre, "weight")
        remove_parametrizations(self.conv_post, "weight")

    def load_checkpoint(self, config, checkpoint_path, eval=False, cache=False):
        state = torch.load(checkpoint_path, map_location=torch.device("cpu"))
        self.load_state_dict(state["model"])
        if eval:
            self.eval()
            assert not self.training
            self.remove_weight_norm()

    def estimate_receptive_field(self):
        """
        Estimate the receptive field of the model based on its configuration.

        Steps:
        1. Start from the initial conv (conv_pre) with kernel=7 (no dilation):
           receptive_field = 7
        2. For each upsampling stage:
           - Multiply the current receptive field by the upsampling factor (since time length is scaled).
           - Add the receptive field contribution from the associated MRF blocks at this scale.

        The MRF block receptive field is calculated by summing the receptive fields of all resblocks at that scale.
        Each resblock adds its own receptive field based on the dilations and kernel sizes.

        Note: This is a heuristic estimation assuming that the resblocks are sequentially affecting the receptive field.
        """

        # Start from conv_pre: kernel_size=7, dilation=1
        # Receptive field increment = (7 - 1) * 1 = 6
        # Since we talk about total receptive field size, let's consider receptive_field as the number of frames.
        # Here we can say receptive_field = 7 (the number of frames covered by kernel=7)
        receptive_field = 7

        idx = 0
        for i, up_factor in enumerate(self.upsample_factors):
            # After upsampling, the receptive field scales
            receptive_field = receptive_field * up_factor

            # Now add the contribution of the MRF blocks at this scale
            # We have num_kernels blocks per scale
            scale_rf = 0
            for j in range(self.num_kernels):
                block = self.resblocks[idx]
                idx += 1
                scale_rf = max(scale_rf, block.receptive_field())  # Take max since they are parallel and merged

            # The MRF blocks process after upsampling, so we add that receptive field increment.
            # Since these blocks are in series at the same scale (averaged), we approximate by adding them.
            # Actually, they are parallel and then averaged. The effective RF should consider the largest block.
            # We'll consider just the largest one for a safer overestimate.
            receptive_field += scale_rf

        return receptive_field


class SELayer(nn.Module):
    """Squeeze-and-Excitation layer for channel-wise attention.
    
    This layer implements the Squeeze-and-Excitation mechanism that adaptively
    recalibrates channel-wise feature responses by explicitly modeling
    interdependencies between channels.
    """

    def __init__(self, channel, reduction=8):
        """Initialize SE layer.

        Args:
            channel (int): Number of input channels.
            reduction (int, optional): Channel reduction factor. Defaults to 8.
        """
        super(SELayer, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(channel, channel // reduction),
            nn.ReLU(inplace=True),
            nn.Linear(channel // reduction, channel),
            nn.Sigmoid(),
        )

    def forward(self, x):
        """Apply channel-wise attention.

        Args:
            x (torch.Tensor): Input tensor of shape [B, C, T].

        Returns:
            torch.Tensor: Channel-wise scaled tensor.
        """
        y = self.avg_pool(x).view(x.size(0), x.size(1))
        y = self.fc(y).view(x.size(0), x.size(1), 1, 1)
        return x * y


class SEBasicBlock(nn.Module):
    """Basic ResNet block with Squeeze-and-Excitation.
    
    This block combines residual connections with SE attention for improved
    feature extraction.
    """

    expansion = 1

    def __init__(self, inplanes, planes, stride=1, downsample=None, reduction=8):
        """Initialize SE-ResNet block.

        Args:
            inplanes (int): Number of input channels.
            planes (int): Number of output channels.
            stride (int, optional): Stride for convolution. Defaults to 1.
            downsample (nn.Module, optional): Downsampling layer. Defaults to None.
            reduction (int, optional): SE reduction factor. Defaults to 8.
        """
        super(SEBasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.se = SELayer(planes, reduction)
        self.downsample = downsample

    def forward(self, x):
        """Process input through SE-ResNet block.

        Args:
            x (torch.Tensor): Input tensor.

        Returns:
            torch.Tensor: Processed tensor.
        """
        residual = x

        x = self.conv1(x)
        x = self.relu(x)
        x = self.bn1(x)

        x = self.conv2(x)
        x = self.bn2(x)
        x = self.se(x)

        if self.downsample is not None:
            residual = self.downsample(residual)

        x += residual
        x = self.relu(x)
        return x


def set_init_dict(model_dict, checkpoint_state, c):
    # Partial initialization: if there is a mismatch with new and old layer, it is skipped.
    for k, v in checkpoint_state.items():
        if k not in model_dict:
            print(f" | > Layer missing in the model definition: {k}")
    pretrained_dict = {k: v for k, v in checkpoint_state.items() if k in model_dict}
    # 2. filter out different size layers
    pretrained_dict = {k: v for k, v in pretrained_dict.items() if v.numel() == model_dict[k].numel()}
    # 3. skip reinit layers
    if c.has("reinit_layers") and c.reinit_layers is not None:
        for reinit_layer_name in c.reinit_layers:
            pretrained_dict = {k: v for k, v in pretrained_dict.items() if reinit_layer_name not in k}
    # 4. overwrite entries in the existing state dict
    model_dict.update(pretrained_dict)
    print(f" | > {len(pretrained_dict)} / {len(model_dict)} layers are restored.")
    return model_dict


class PreEmphasis(nn.Module):
    """Pre-emphasis filter for audio processing.
    
    Applies pre-emphasis filtering to the input audio signal to enhance
    high-frequency components.
    """

    def __init__(self, coefficient=0.97):
        """Initialize pre-emphasis filter.

        Args:
            coefficient (float, optional): Pre-emphasis coefficient. Defaults to 0.97.
        """
        super().__init__()
        self.coefficient = coefficient
        self.register_buffer("filter", torch.tensor([-self.coefficient, 1.0], dtype=torch.float32).view(1, 1, -1))

    def forward(self, x):
        """Apply pre-emphasis filtering.

        Args:
            x (torch.Tensor): Input audio tensor.

        Returns:
            torch.Tensor: Pre-emphasized audio.
        """
        assert len(x.size()) == 2

        x = torch.nn.functional.pad(x.unsqueeze(1), (1, 0), "reflect")
        x = torch.nn.functional.conv1d(x, self.filter).squeeze(1)
        return x


class ResNetSpeakerEncoder(nn.Module):
    """ResNet-based speaker encoder for voice conversion.
    
    This module extracts speaker embeddings from audio using a modified ResNet
    architecture with optional attentive statistical pooling.
    """

    # pylint: disable=W0102
    def __init__(
        self,
        input_dim=64,
        proj_dim=512,
        layers=[3, 4, 6, 3],
        num_filters=[32, 64, 128, 256],
        encoder_type="ASP",
        log_input=False,
        use_torch_spec=False,
        audio_config=None,
    ):
        """Initialize speaker encoder.

        Args:
            input_dim (int, optional): Input feature dimension. Defaults to 64.
            proj_dim (int, optional): Projection dimension. Defaults to 512.
            layers (List[int], optional): Number of layers in each block. Defaults to [3,4,6,3].
            num_filters (List[int], optional): Number of filters in each block. Defaults to [32,64,128,256].
            encoder_type (str, optional): Type of encoder ("ASP" or "SAP"). Defaults to "ASP".
            log_input (bool, optional): Whether to apply log to input. Defaults to False.
            use_torch_spec (bool, optional): Whether to use torch spectrogram. Defaults to False.
            audio_config (dict, optional): Audio processing configuration. Defaults to None.
        """
        super(ResNetSpeakerEncoder, self).__init__()

        self.encoder_type = encoder_type
        self.input_dim = input_dim
        self.log_input = log_input
        self.use_torch_spec = use_torch_spec
        self.audio_config = audio_config
        self.proj_dim = proj_dim

        self.conv1 = nn.Conv2d(1, num_filters[0], kernel_size=3, stride=1, padding=1)
        self.relu = nn.ReLU(inplace=True)
        self.bn1 = nn.BatchNorm2d(num_filters[0])

        self.inplanes = num_filters[0]
        self.layer1 = self.create_layer(SEBasicBlock, num_filters[0], layers[0])
        self.layer2 = self.create_layer(SEBasicBlock, num_filters[1], layers[1], stride=(2, 2))
        self.layer3 = self.create_layer(SEBasicBlock, num_filters[2], layers[2], stride=(2, 2))
        self.layer4 = self.create_layer(SEBasicBlock, num_filters[3], layers[3], stride=(2, 2))

        self.instancenorm = nn.InstanceNorm1d(input_dim)

        if self.use_torch_spec:
            self.torch_spec = torch.nn.Sequential(
                PreEmphasis(audio_config["preemphasis"]),
                torchaudio.transforms.MelSpectrogram(
                    sample_rate=audio_config["sample_rate"],
                    n_fft=audio_config["fft_size"],
                    win_length=audio_config["win_length"],
                    hop_length=audio_config["hop_length"],
                    window_fn=torch.hamming_window,
                    n_mels=audio_config["num_mels"],
                ),
            )

        else:
            self.torch_spec = None

        outmap_size = int(self.input_dim / 8)

        self.attention = nn.Sequential(
            nn.Conv1d(num_filters[3] * outmap_size, 128, kernel_size=1),
            nn.ReLU(inplace=True),
            nn.BatchNorm1d(128),
            nn.Conv1d(128, num_filters[3] * outmap_size, kernel_size=1),
            nn.Softmax(dim=2),
        )

        if self.encoder_type == "SAP":
            out_dim = num_filters[3] * outmap_size
        elif self.encoder_type == "ASP":
            out_dim = num_filters[3] * outmap_size * 2
        else:
            raise ValueError("Undefined encoder")

        self.fc = nn.Linear(out_dim, proj_dim)

        self._init_layers()

    def _init_layers(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode="fan_out", nonlinearity="relu")
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def create_layer(self, block, planes, blocks, stride=1):
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(planes * block.expansion),
            )

        layers = [block(self.inplanes, planes, stride, downsample)]
        self.inplanes = planes * block.expansion
        layers.extend(block(self.inplanes, planes) for _ in range(1, blocks))

        return nn.Sequential(*layers)

    # pylint: disable=R0201
    def new_parameter(self, *size):
        out = nn.Parameter(torch.FloatTensor(*size))
        nn.init.xavier_normal_(out)
        return out

    def forward(self, x, l2_norm=False):
        """Extract speaker embeddings from input features.

        Args:
            x (torch.Tensor): Input features.
            l2_norm (bool, optional): Whether to apply L2 normalization. Defaults to False.

        Returns:
            torch.Tensor: Speaker embeddings.
        """
        x.squeeze_(1)
        # if you torch spec compute it otherwise use the mel spec computed by the AP
        if self.use_torch_spec:
            x = self.torch_spec(x)

        if self.log_input:
            x.add_(1e-6).log_()
        x = self.instancenorm(x).unsqueeze(1)

        x = self.conv1(x)
        x = self.relu(x)
        x = self.bn1(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        x = x.reshape(x.size(0), -1, x.size(-1))

        w = self.attention(x)

        if self.encoder_type == "SAP":
            x = torch.sum(x * w, dim=2)
        elif self.encoder_type == "ASP":
            mu = torch.sum(x * w, dim=2)
            sg = torch.sqrt((torch.sum((x ** 2) * w, dim=2) - mu ** 2).clamp(min=1e-5))
            x = torch.cat((mu, sg), 1)

        x = x.view(x.size()[0], -1)
        x = self.fc(x)

        if l2_norm:
            x = torch.nn.functional.normalize(x, p=2, dim=1)
        return x

    def load_checkpoint(
        self,
        checkpoint_path: str,
        eval: bool = False,
        use_cuda: bool = False,
        criterion=None,
        cache=False,
    ):
        state = load_fsspec(checkpoint_path, map_location=torch.device("cpu"), cache=cache)
        try:
            self.load_state_dict(state["model"])
            print(" > Model fully restored. ")
        except (KeyError, RuntimeError) as error:
            # If eval raise the error
            if eval:
                raise error

            print(" > Partial model initialization.")
            model_dict = self.state_dict()
            model_dict = set_init_dict(model_dict, state["model"])
            self.load_state_dict(model_dict)
            del model_dict

        # load the criterion for restore_path
        if criterion is not None and "criterion" in state:
            try:
                criterion.load_state_dict(state["criterion"])
            except (KeyError, RuntimeError) as error:
                print(" > Criterion load ignored because of:", error)

        if use_cuda:
            self.cuda()
            if criterion is not None:
                criterion = criterion.cuda()

        if eval:
            self.eval()
            assert not self.training

        if not eval:
            return criterion, state["step"]
        return criterion


class HifiDecoder(torch.nn.Module):
    """HiFi-GAN based decoder for high-quality speech synthesis.
    
    This module converts mel-spectrograms or other acoustic features into
    high-fidelity waveforms using a HiFi-GAN architecture with optional
    speaker conditioning.
    """

    def __init__(
        self,
        input_sample_rate=22050,
        output_sample_rate=24000,
        output_hop_length=256,
        ar_mel_length_compression=1024,
        decoder_input_dim=1024,
        resblock_type_decoder="1",
        resblock_dilation_sizes_decoder=[[1, 3, 5], [1, 3, 5], [1, 3, 5]],
        resblock_kernel_sizes_decoder=[3, 7, 11],
        upsample_rates_decoder=[8, 8, 2, 2],
        upsample_initial_channel_decoder=512,
        upsample_kernel_sizes_decoder=[16, 16, 4, 4],
        d_vector_dim=512,
        cond_d_vector_in_each_upsampling_layer=True,
        speaker_encoder_audio_config={
            "fft_size": 512,
            "win_length": 400,
            "hop_length": 160,
            "sample_rate": 16000,
            "preemphasis": 0.97,
            "num_mels": 64,
        },
    ):
        """Initialize HiFi decoder.

        Args:
            input_sample_rate (int, optional): Input sampling rate. Defaults to 22050.
            output_sample_rate (int, optional): Output sampling rate. Defaults to 24000.
            output_hop_length (int, optional): Output hop length. Defaults to 256.
            ar_mel_length_compression (int, optional): Autoregressive compression factor. Defaults to 1024.
            decoder_input_dim (int, optional): Input dimension for decoder. Defaults to 1024.
            resblock_type_decoder (str, optional): Type of residual blocks. Defaults to "1".
            resblock_dilation_sizes_decoder (List[List[int]], optional): Dilation sizes for residual blocks.
            resblock_kernel_sizes_decoder (List[int], optional): Kernel sizes for residual blocks.
            upsample_rates_decoder (List[int], optional): Upsampling rates.
            upsample_initial_channel_decoder (int, optional): Initial number of channels.
            upsample_kernel_sizes_decoder (List[int], optional): Kernel sizes for upsampling.
            d_vector_dim (int, optional): Speaker embedding dimension. Defaults to 512.
            cond_d_vector_in_each_upsampling_layer (bool, optional): Whether to condition each layer.
            speaker_encoder_audio_config (dict, optional): Speaker encoder configuration.
        """
        super().__init__()
        self.input_sample_rate = input_sample_rate
        self.output_sample_rate = output_sample_rate
        self.output_hop_length = output_hop_length
        self.ar_mel_length_compression = ar_mel_length_compression
        self.speaker_encoder_audio_config = speaker_encoder_audio_config
        self.waveform_decoder = HifiganGenerator(
            decoder_input_dim,
            1,
            resblock_type_decoder,
            resblock_dilation_sizes_decoder,
            resblock_kernel_sizes_decoder,
            upsample_kernel_sizes_decoder,
            upsample_initial_channel_decoder,
            upsample_rates_decoder,
            inference_padding=0,
            cond_channels=d_vector_dim,
            conv_pre_weight_norm=False,
            conv_post_weight_norm=False,
            conv_post_bias=False,
            cond_in_each_up_layer=cond_d_vector_in_each_upsampling_layer,
        )
        self.speaker_encoder = ResNetSpeakerEncoder(
            input_dim=64,
            proj_dim=512,
            log_input=True,
            use_torch_spec=True,
            audio_config=speaker_encoder_audio_config,
        )

    @property
    def device(self):
        return next(self.parameters()).device

    def forward(self, latents, g=None):
        """Generate waveform from latent features.

        Args:
            latents (torch.Tensor): Input latent features.
            g (torch.Tensor, optional): Speaker embedding. Defaults to None.

        Returns:
            torch.Tensor: Generated waveform.
        """

        z = torch.nn.functional.interpolate(
            latents.transpose(1, 2),
            scale_factor=self.ar_mel_length_compression / self.output_hop_length,
            mode="linear",
            align_corners=False,
        ).squeeze(1)
        # upsample to the right sr
        if self.output_sample_rate != self.input_sample_rate:
            z = torch.nn.functional.interpolate(
                z,
                scale_factor=self.output_sample_rate / self.input_sample_rate,
                mode="linear",
                align_corners=False,
            ).squeeze(0)
        o = self.waveform_decoder(z, g=g)
        return o

    @torch.no_grad()
    def inference(self, c, g):
        """Generate waveform in inference mode.

        Args:
            c (torch.Tensor): Input features.
            g (torch.Tensor): Speaker embedding.

        Returns:
            torch.Tensor: Generated waveform.
        """
        return self.forward(c, g=g)

    def load_checkpoint(self, checkpoint_path, eval=False):  # pylint: disable=unused-argument, redefined-builtin
        """Load model checkpoint.

        Args:
            checkpoint_path (str): Path to checkpoint file.
            eval (bool, optional): Whether to set model to eval mode. Defaults to False.

        Returns:
            dict: Loaded checkpoint state.
        """
        state = load_fsspec(checkpoint_path, map_location=torch.device("cpu"))
        # remove unused keys
        state = state["model"]
        for key in list(state.keys()):
            if "waveform_decoder." not in key and "speaker_encoder." not in key:
                del state[key]

        self.load_state_dict(state)
        if eval:
            self.eval()
            assert not self.training
            self.waveform_decoder.remove_weight_norm()


================================================================================
# File: auralis/models/xttsv2/components/vllm/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/xttsv2/components/vllm/__init__.py
================================================================================



================================================================================
# File: auralis/models/xttsv2/components/vllm/hijack.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/xttsv2/components/vllm/hijack.py
================================================================================

from typing import List, Optional

import torch
from vllm import SamplingParams

from auralis.models.xttsv2.components.vllm.hidden_state_collector import HiddenStatesCollector


class ExtendedSamplingParams(SamplingParams, kw_only=True):
    """Extended sampling parameters for VLLM with additional fields.
    
    This class extends VLLM's SamplingParams to include additional required fields
    for hidden state collection and request tracking, while maintaining compatibility
    with the base class's functionality.

    Attributes:
        hidden_state_collector (Optional[HiddenStatesCollector]): Collector for model's
            hidden states during generation.
        request_id (Optional[str]): Unique identifier for the generation request.
    """
    hidden_state_collector: Optional[HiddenStatesCollector] = None
    request_id: Optional[str] = None


class LogitsRepetitionPenalizer:
    """Logits processor for preventing repetitive text generation.
    
    This class implements a repetition penalty mechanism that modifies token logits
    based on their previous occurrences in the generated text. It helps prevent
    the model from getting stuck in repetitive patterns.
    """

    def __init__(self, repetition_penalty: float):
        """Initialize repetition penalizer.

        Args:
            repetition_penalty (float): Penalty factor for repeated tokens.
                Values > 1.0 decrease probability of repetition,
                Values < 1.0 increase probability of repetition,
                Value = 1.0 applies no penalty.

        Raises:
            ValueError: If repetition_penalty is negative.
        """
        if repetition_penalty < 0:
            raise ValueError("Repetition penalty must be non-negative")
        self.repetition_penalty = repetition_penalty

    def __call__(self, prompt_token_ids: List[int], token_ids: List[int], logits: torch.Tensor) -> torch.Tensor:
        """Apply repetition penalty to logits.

        This method modifies the logits of tokens that have appeared in either the
        prompt or the generated sequence. For repeated tokens:
        - Positive logits are divided by the penalty
        - Negative logits are multiplied by the penalty
        This effectively reduces the probability of generating repeated tokens.

        Args:
            prompt_token_ids (List[int]): Token IDs from the input prompt.
            token_ids (List[int]): Token IDs from the generated sequence.
            logits (torch.Tensor): Raw logits from the model.

        Returns:
            torch.Tensor: Modified logits with repetition penalty applied.
        """
        # If no repetition penalty or no tokens to check, return original logits
        if self.repetition_penalty == 1.0 or (not token_ids and not prompt_token_ids):
            return logits

        # Create a mask for the repeated tokens
        repeated_tokens = torch.tensor(prompt_token_ids + token_ids,
                                       device=logits.device,
                                       dtype=torch.long)

        # Get logits of repeated tokens
        repeated_logits = logits[repeated_tokens]

        # Apply penalty: divide positive logits by penalty, multiply negative logits by penalty
        repeated_logits = torch.where(
            repeated_logits > 0,
            repeated_logits / self.repetition_penalty,
            repeated_logits * self.repetition_penalty
        )

        # Update only the logits for repeated tokens
        logits[repeated_tokens] = repeated_logits

        return logits



================================================================================
# File: auralis/models/xttsv2/components/vllm/hidden_state_collector.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/models/xttsv2/components/vllm/hidden_state_collector.py
================================================================================

import threading
from typing import Optional, Dict, List, Callable
import torch
from queue import Queue
from concurrent.futures import ThreadPoolExecutor

from auralis.common.logging.logger import setup_logger


class SyncCollectorWrapper:
    """Thread-safe wrapper for synchronous hidden state collection.
    
    This wrapper provides a synchronous interface for collecting hidden states
    while maintaining thread safety. It stores a request ID and collection function,
    allowing for simplified collection calls that don't require explicit request IDs.
    """

    def __init__(self, collector_fn: Callable[[torch.Tensor, str], None], request_id: str):
        """Initialize synchronous collector wrapper.

        Args:
            collector_fn (Callable[[torch.Tensor, str], None]): Function to collect
                hidden states with request ID.
            request_id (str): Unique identifier for the collection request.
        """
        self.collector_fn = collector_fn
        self.request_id = request_id

    def __call__(self, hidden_states: Optional[torch.Tensor], request_id: Optional[str] = None):
        """Collect hidden states synchronously.

        Args:
            hidden_states (Optional[torch.Tensor]): Hidden states to collect.
            request_id (Optional[str], optional): Request identifier. If None,
                uses the stored request_id. Defaults to None.
        """
        self.collector_fn(hidden_states, request_id or self.request_id)

class HiddenStatesCollector:
    """Thread-safe collector for model hidden states.
    
    This class manages the collection and retrieval of model hidden states during
    generation, with support for multiple concurrent requests. It provides thread-safe
    operations and timeout-based retrieval.

    The collector maintains separate queues and synchronization primitives for each
    request, allowing for parallel collection of hidden states from different
    generation processes.
    """

    def __init__(self):
        """Initialize hidden states collector.
        
        Sets up thread-safe data structures for collecting and managing hidden states,
        including locks, events, and output storage for multiple requests.
        """
        self.outputs: Dict[str, List[torch.Tensor]] = {}
        self.collection_ready: Dict[str, threading.Event] = {}
        self.collection_complete: Dict[str, threading.Event] = {}
        self.locks: Dict[str, threading.Lock] = {}
        self.global_lock = threading.Lock()
        self.logger = setup_logger(__file__)
        self.states_count: Dict[str, int] = {}
        self.expected_states: Dict[str, int] = {}
        self.notifications: Dict[str, Queue] = {}
        self.executor = ThreadPoolExecutor(max_workers=4)

    def initialize_request(self, request_id: str):
        """Initialize collection resources for a new request.

        Sets up all necessary synchronization primitives and storage for a new
        collection request. This method is thread-safe and idempotent.

        Args:
            request_id (str): Unique identifier for the request.
        """
        with self.global_lock:
            if request_id not in self.locks:
                self.locks[request_id] = threading.Lock()
                self.collection_ready[request_id] = threading.Event()
                self.collection_complete[request_id] = threading.Event()
                self.outputs[request_id] = []
                self.states_count[request_id] = 0
                self.expected_states[request_id] = 1
                self.notifications[request_id] = Queue()
                self.collection_ready[request_id].set()
                self.logger.debug(f"Initialized collector for request {request_id}")

    def sync_collect(self, hidden_states: Optional[torch.Tensor], request_id: str):
        """Synchronously collect hidden states for a request.

        This method is called by VLLM to collect hidden states during generation.
        It handles the thread-safe storage of states and signals completion when
        all expected states are collected.

        Args:
            hidden_states (Optional[torch.Tensor]): Hidden states to collect.
            request_id (str): Request identifier.

        Raises:
            Exception: If there's an error during collection.
        """
        if request_id not in self.collection_ready:
            self.logger.error(f"Collector not initialized for request {request_id}")
            # Initialize on demand if needed
            self.initialize_request(request_id)
            return

        try:
            with self.locks[request_id]:
                if hidden_states is not None:
                    self.outputs[request_id].append(hidden_states.clone())
                    self.states_count[request_id] += 1
                    self.logger.debug(f"Collected state {self.states_count[request_id]} for request {request_id}")

                    if self.states_count[request_id] >= self.expected_states[request_id]:
                        self.collection_complete[request_id].set()
                        self.notifications[request_id].put(True)
                else:
                    self.logger.warning(f"Received None hidden states for request {request_id}")
        except Exception as e:
            self.logger.error(f"Error collecting hidden states: {e}")
            raise

    async def get_hidden_states(self, request_id: str, timeout: float = 3.0) -> Optional[torch.Tensor]:
        """Retrieve collected hidden states for a request.

        This method waits for all hidden states to be collected or until timeout,
        then concatenates and returns the collected states.

        Args:
            request_id (str): Request identifier.
            timeout (float, optional): Maximum time to wait in seconds. Defaults to 3.0.

        Returns:
            Optional[torch.Tensor]: Concatenated hidden states or None if timeout or error.

        Raises:
            ValueError: If no hidden states were collected.
        """
        try:
            if request_id not in self.collection_ready:
                self.logger.error(f"Request {request_id} was never initialized")
                return None

            # Wait for completion using threading.Event
            if not self.collection_complete[request_id].wait(timeout):
                return None

            with self.locks[request_id]:
                outputs = self.outputs.get(request_id, [])
                if not outputs:
                    self.logger.critical(f"No hidden states found for request {request_id}") # most likely due to wrong profiling data dimensions
                    raise ValueError(f"No hidden states found for request {request_id}, "
                                     f"this should not happen, please open an issue on github")

                try:
                    result = torch.cat(outputs, dim=0)
                    self._cleanup_request(request_id)
                    return result
                except Exception as e:
                    self.logger.error(f"Error processing hidden states: {e}")
                    raise

        except Exception as e:
            self.logger.error(f"Error retrieving hidden states: {e}")
            return None

    def _cleanup_request(self, request_id: str):
        """Clean up resources associated with a completed request.

        This method removes all data structures and synchronization primitives
        associated with a request to prevent memory leaks.

        Args:
            request_id (str): Request identifier to clean up.
        """
        with self.global_lock:
            self.outputs.pop(request_id, None)
            self.collection_ready.pop(request_id, None)
            self.collection_complete.pop(request_id, None)
            self.locks.pop(request_id, None)
            self.states_count.pop(request_id, None)
            self.expected_states.pop(request_id, None)
            self.notifications.pop(request_id, None)
            self.logger.debug(f"Cleaned up request {request_id}")

    def bind_to_request(self, request_id: str) -> SyncCollectorWrapper:
        """Create a synchronous collector wrapper for a request.

        This method initializes collection resources and returns a wrapper that
        simplifies the collection process for VLLM callbacks.

        Args:
            request_id (str): Request identifier.

        Returns:
            SyncCollectorWrapper: Thread-safe wrapper for collecting hidden states.
        """
        # Synchronous initialization
        self.initialize_request(request_id)
        # Pass request_id to wrapper so it's available even if VLLM passes None
        return SyncCollectorWrapper(
            collector_fn=lambda hs, rid: self.sync_collect(hs, rid),
            request_id=request_id
        )

================================================================================
# File: auralis/core/tts.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/core/tts.py
================================================================================

import asyncio
import json
import logging
import os
import time
import uuid
from concurrent.futures import ThreadPoolExecutor
from functools import partial
from typing import AsyncGenerator, Optional, Dict, Union, Generator, List, Any

from huggingface_hub import hf_hub_download

from auralis.common.logging.logger import setup_logger, set_vllm_logging_level
from auralis.common.definitions.output import TTSOutput
from auralis.common.definitions.requests import TTSRequest
from auralis.common.metrics.performance import track_generation
from auralis.common.scheduling.two_phase_scheduler import TwoPhaseScheduler
from auralis.core.async_lru_cache import AsyncLRUCache
from auralis.models.base import BaseAsyncTTSEngine, AudioOutputGenerator


class TTS:
    """A high-performance text-to-speech engine optimized for inference speed.

    This class provides an interface for both synchronous and asynchronous speech generation,
    with support for streaming output and parallel processing of multiple requests.
    """

    def __init__(self, scheduler_max_concurrency: int = 10, vllm_logging_level=logging.DEBUG):
        """Initialize the TTS engine.

        Args:
            scheduler_max_concurrency (int): Maximum number of concurrent requests to process.
            vllm_logging_level: Logging level for the VLLM backend.
        """
        set_vllm_logging_level(vllm_logging_level)

        self.scheduler: Optional[TwoPhaseScheduler] = TwoPhaseScheduler(scheduler_max_concurrency)
        self.tts_engine: Optional[BaseAsyncTTSEngine] = None
        self.concurrency = scheduler_max_concurrency
        self.max_vllm_memory: Optional[int] = None
        self.logger = setup_logger(__file__)
        self.loop = None

    def _ensure_event_loop(self):
        """Ensures that an event loop exists and is set."""

        if not self.loop:
            try:
                self.loop = asyncio.get_running_loop()
            except RuntimeError:
                self.loop = asyncio.new_event_loop()
                asyncio.set_event_loop(self.loop)

    def from_pretrained(self, model_name_or_path: str, **kwargs):
        """Load a pretrained model from local path or Hugging Face Hub.
           **THIS METHOD IS SYNCHRONOUS**

        Args:
            model_name_or_path (str): Local path or Hugging Face model identifier.
            **kwargs: Additional arguments passed to the model's from_pretrained method.

        Returns:
            TTS: The TTS instance with loaded model.

        Raises:
            ValueError: If the model cannot be loaded from the specified path.
        """
        from auralis.models.registry import MODEL_REGISTRY

        # Ensure an event loop exists for potential async operations within from_pretrained
        self._ensure_event_loop()

        try:
            with open(os.path.join(model_name_or_path, 'config.json'), 'r') as f:
                config = json.load(f)
        except FileNotFoundError:
            try:
                config_path = hf_hub_download(repo_id=model_name_or_path, filename='config.json')
                with open(config_path, 'r') as f:
                    config = json.load(f)
            except Exception as e:
                raise ValueError(f"Could not load model from {model_name_or_path} neither locally or online: {e}")

        # Run potential async operations within from_pretrained in the event loop
        async def _load_model():
            return MODEL_REGISTRY[config['model_type']].from_pretrained(model_name_or_path, **kwargs)

        self.tts_engine = self.loop.run_until_complete(_load_model()) # to start form the correct loop

        return self

    async def _phase_1_prepare_context(self, input_request: TTSRequest):
        """Phase 1: Prepare the generation context (text to tokens, conditioning).
           This happens sequentially.

        Args:
            input_request (TTSRequest): The TTS request to process.

        Returns:
            dict: Dictionary containing parallel inputs and the original request.
        """
        conditioning_config = self.tts_engine.conditioning_config
        input_request.start_time = time.time()

        audio_token_generators, speaker_embeddings, gpt_like_decoder_conditioning = None, None, None

        if conditioning_config.speaker_embeddings and conditioning_config.gpt_like_decoder_conditioning:
            (audio_token_generators, requests_ids,
             speaker_embeddings,
             gpt_like_decoder_conditioning) = await self.tts_engine.get_generation_context(input_request)
        elif conditioning_config.speaker_embeddings:
            (audio_token_generators, requests_ids,
             speaker_embeddings) = await self.tts_engine.get_generation_context(input_request)
        elif conditioning_config.gpt_like_decoder_conditioning:
            (audio_token_generators, requests_ids,
             gpt_like_decoder_conditioning) = await self.tts_engine.get_generation_context(input_request)
        else:
            audio_token_generators, requests_ids = await self.tts_engine.get_generation_context(input_request)

        parallel_inputs = [
            {
                'generator': gen,
                'speaker_embedding': speaker_embeddings[i] if
                speaker_embeddings is not None and isinstance(speaker_embeddings, list) else
                speaker_embeddings if speaker_embeddings is not None else
                None,
                'multimodal_data': gpt_like_decoder_conditioning[i] if
                gpt_like_decoder_conditioning is not None and isinstance(gpt_like_decoder_conditioning, list) else
                gpt_like_decoder_conditioning if gpt_like_decoder_conditioning is not None else
                None,
                'request': input_request,
            }
            for i, gen in enumerate(audio_token_generators)
        ]
        input_request.generators_count = len(parallel_inputs)
        input_request.sequence_buffers = {i: [] for i in range(input_request.generators_count)}
        input_request.completed_generators = 0

        return {
            'parallel_inputs': parallel_inputs,
            'request': input_request
        }

    async def _phase_2_process_tokens(self, gen_input: Dict) -> AsyncGenerator[Any, None]:
        """Phase 2: Process the audio tokens to produce the hidden states.
           This happens in parallel
        Args:
           gen_input (Dict): Dictionary containing the request and generated tokens
        Returns:
            AudioOutputGenerator: Generator yielding audio chunks.
        """
        async with self.scheduler.semaphore:
            try:
                async for item in self.tts_engine.process_tokens_to_speech( # type: ignore
                        generator=gen_input['generator'],
                        speaker_embeddings=gen_input['speaker_embedding'],
                        multimodal_data=gen_input['multimodal_data'],
                        request=gen_input['request']
                ):

                    yield item
            except Exception as e:
                raise e

    async def _phase_3_collect_and_yield(self, gen_input: Dict) -> AsyncGenerator[Any, None]:
        """Third and Final Phase: collect output from the previous phase and yield audio
           This happens in parallel
           Args:
               gen_input (Dict): Dictionary containing the request and generated tokens
           Returns:
                AsyncGenerator: generator yielding audio chunks
        """
        async for item in self._phase_2_process_tokens(gen_input):
            yield item

    async def _process_request(self, request: TTSRequest):
        """Process a single TTS request through all three phases.

        Args:
            request (TTSRequest): The TTS request to process.

        Yields:
            AsyncGenerator[TTSOutput, None]: Asynchronous generator for audio output.
        """

        try:
            # Phase 1: Prepare the generation context (text to tokens, conditioning)
            phase_1_result = await asyncio.wait_for(self._phase_1_prepare_context(request), timeout=self.scheduler.request_timeout)

            # Phase 2 & 3: Process audio tokens and generate waveforms in parallel
            async for item in self.scheduler.run(
                    inputs = phase_1_result,
                    request_id = request.request_id,
                    processing_fn = self._phase_3_collect_and_yield
                ):
                   yield item
        except Exception as e:
            self.logger.error(f"Error processing request: {e}")
            raise

    @track_generation
    async def generate_speech_async(self, request: TTSRequest) -> Union[AsyncGenerator[TTSOutput, None], TTSOutput]:
        """Generate speech asynchronously from text.

        Args:
            request (TTSRequest): The TTS request to process.

        Returns:
            Union[AsyncGenerator[TTSOutput, None], TTSOutput]: Audio output, either streamed or complete.

        Raises:
            RuntimeError: If instance was not created for async generation.
        """
        self._ensure_event_loop()

        async def process_chunks():
            chunks = []
            try:
                async for chunk in self._process_request(request):
                    if request.stream:
                        yield chunk
                    chunks.append(chunk)
            except Exception as e:
                self.logger.error(f"Error during speech generation: {e}")
                raise

            if not request.stream:
                yield TTSOutput.combine_outputs(chunks)

        if request.stream:
            return process_chunks()
        else:
            async for result in process_chunks():
                return result

    @staticmethod
    def split_requests(request: TTSRequest, max_length: int = 100000) -> List[TTSRequest]:
        """Split a long text request into multiple smaller requests.

        Args:
            request (TTSRequest): The original TTS request.
            max_length (int): Maximum length of text per request.

        Returns:
            List[TTSRequest]: List of split requests.
        """
        if len(request.text) <= max_length:
            return [request]

        text_chunks = [request.text[i:i + max_length]
                       for i in range(0, len(request.text), max_length)]

        return [
            (copy := request.copy(), setattr(copy, 'text', chunk), setattr(copy, 'request_id', uuid.uuid4().hex))[0]
            for chunk in text_chunks
        ]

    async def _process_multiple_requests(self, requests: List[TTSRequest], results: Optional[List] = None) -> Optional[
        TTSOutput]:
        """Process multiple TTS requests in parallel.

        Args:
            requests (List[TTSRequest]): List of requests to process.
            results (Optional[List]): Optional list to store results for streaming.

        Returns:
            Optional[TTSOutput]: Combined audio output if not streaming, None otherwise.
        """
        output_queues = [asyncio.Queue() for _ in requests] if results is not None else None

        async def process_subrequest(idx, sub_request, queue: Optional[asyncio.Queue] = None):
            chunks = []
            async for chunk in self._process_request(sub_request):
                chunks.append(chunk)
                if queue is not None:
                    await queue.put(chunk)

            if queue is not None:
                await queue.put(None)
            return chunks

        tasks = [
            asyncio.create_task(
                process_subrequest(
                    idx,
                    sub_request,
                    output_queues[idx] if output_queues else None
                )
            )
            for idx, sub_request in enumerate(requests)
        ]

        if results is not None:
            for idx, queue in enumerate(output_queues):
                while True:
                    chunk = await queue.get()
                    if chunk is None:
                        break
                    results[idx].append(chunk)
            return None
        else:
            all_chunks = await asyncio.gather(*tasks)
            complete_audio = [chunk for chunks in all_chunks for chunk in chunks]
            return TTSOutput.combine_outputs(complete_audio)

    def generate_speech(self, request: TTSRequest) -> Union[Generator[TTSOutput, None, None], TTSOutput]:
        """Generate speech synchronously from text.

        Args:
            request (TTSRequest): The TTS request to process.

        Returns:
            Union[Generator[TTSOutput, None, None], TTSOutput]: Audio output, either streamed or complete.

        Raises:
            RuntimeError: If instance was created for async generation.
        """
        self._ensure_event_loop()
        requests = self.split_requests(request)

        if request.stream:
            # Streaming case
            def streaming_wrapper():
                for sub_request in requests:
                    # For streaming, execute the async gen
                    async def process_stream():
                        try:
                            async for chunk in self._process_request(sub_request):
                                yield chunk
                        except Exception as e:
                            self.logger.error(f"Error during streaming: {e}")
                            raise

                    # Execute the async gen
                    generator = process_stream()
                    try:
                        while True:
                            chunk = self.loop.run_until_complete(anext(generator))
                            yield chunk
                    except StopAsyncIteration:
                        pass

            return streaming_wrapper()
        else:
            # Non streaming
            return self.loop.run_until_complete(self._process_multiple_requests(requests))

    async def shutdown(self):
        """Shuts down the TTS engine and scheduler."""
        if self.scheduler:
            await self.scheduler.shutdown()
        if self.tts_engine and hasattr(self.tts_engine, 'shutdown'):
            await self.tts_engine.shutdown()

================================================================================
# File: auralis/core/__init__.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/core/__init__.py
================================================================================



================================================================================
# File: auralis/core/async_lru_cache.py
# Path: /home/marco/PycharmProjects/betterVoiceCraft/Auralis/src/auralis/core/async_lru_cache.py
================================================================================

import asyncio
from collections import OrderedDict
from functools import wraps
from typing import TypeVar, Callable, ParamSpec, Awaitable, Any, List

P = ParamSpec('P')
R = TypeVar('R')


class AsyncLRUCache:
    def __init__(self, max_size: int = 128):
        self.max_size = max_size
        self.cache: OrderedDict[int, Any] = OrderedDict()
        self.lock = asyncio.Lock()

    def __call__(self, func: Callable[P, Awaitable[R]]) -> Callable[P, Awaitable[R]]:
        @wraps(func)
        async def wrapper(*args: P.args, **kwargs: P.kwargs) -> R:
            # Create a unique key
            key = hash(str(args) + str(sorted(kwargs.items())))

            async with self.lock:
                # check if the key is in the cache
                if key in self.cache:
                    # Bring it to the end
                    self.cache.move_to_end(key)
                    return self.cache[key]

                # if the cache is full, remove the oldest element
                if len(self.cache) >= self.max_size:
                    self.cache.popitem(last=False)

            # if the key is not in the cache, call the function
            result = await func(*args, **kwargs)

            async with self.lock:
                self.cache[key] = result

            return result

        return wrapper